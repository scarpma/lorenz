{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0428 01:55:55.000815 46913073061376 training.py:2197] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "W0428 01:55:56.636697 46913073061376 training.py:2197] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.999912] [G loss: 0.999820]\n",
      "1 [D loss: 0.999919] [G loss: 0.999811]\n",
      "2 [D loss: 0.999920] [G loss: 0.999808]\n",
      "3 [D loss: 0.999925] [G loss: 0.999800]\n",
      "4 [D loss: 0.999916] [G loss: 0.999798]\n",
      "5 [D loss: 0.999919] [G loss: 0.999794]\n",
      "6 [D loss: 0.999920] [G loss: 0.999796]\n",
      "7 [D loss: 0.999923] [G loss: 0.999800]\n",
      "8 [D loss: 0.999919] [G loss: 0.999799]\n",
      "9 [D loss: 0.999923] [G loss: 0.999803]\n",
      "10 [D loss: 0.999925] [G loss: 0.999810]\n",
      "11 [D loss: 0.999922] [G loss: 0.999806]\n",
      "12 [D loss: 0.999924] [G loss: 0.999813]\n",
      "13 [D loss: 0.999933] [G loss: 0.999821]\n",
      "14 [D loss: 0.999930] [G loss: 0.999826]\n",
      "15 [D loss: 0.999930] [G loss: 0.999833]\n",
      "16 [D loss: 0.999930] [G loss: 0.999839]\n",
      "17 [D loss: 0.999936] [G loss: 0.999846]\n",
      "18 [D loss: 0.999936] [G loss: 0.999846]\n",
      "19 [D loss: 0.999928] [G loss: 0.999851]\n",
      "20 [D loss: 0.999940] [G loss: 0.999856]\n",
      "21 [D loss: 0.999946] [G loss: 0.999867]\n",
      "22 [D loss: 0.999939] [G loss: 0.999867]\n",
      "23 [D loss: 0.999941] [G loss: 0.999868]\n",
      "24 [D loss: 0.999950] [G loss: 0.999876]\n",
      "25 [D loss: 0.999952] [G loss: 0.999877]\n",
      "26 [D loss: 0.999951] [G loss: 0.999884]\n",
      "27 [D loss: 0.999952] [G loss: 0.999889]\n",
      "28 [D loss: 0.999957] [G loss: 0.999896]\n",
      "29 [D loss: 0.999952] [G loss: 0.999898]\n",
      "30 [D loss: 0.999957] [G loss: 0.999900]\n",
      "31 [D loss: 0.999957] [G loss: 0.999900]\n",
      "32 [D loss: 0.999965] [G loss: 0.999903]\n",
      "33 [D loss: 0.999960] [G loss: 0.999907]\n",
      "34 [D loss: 0.999961] [G loss: 0.999911]\n",
      "35 [D loss: 0.999961] [G loss: 0.999912]\n",
      "36 [D loss: 0.999964] [G loss: 0.999917]\n",
      "37 [D loss: 0.999961] [G loss: 0.999922]\n",
      "38 [D loss: 0.999963] [G loss: 0.999926]\n",
      "39 [D loss: 0.999970] [G loss: 0.999924]\n",
      "40 [D loss: 0.999974] [G loss: 0.999926]\n",
      "41 [D loss: 0.999971] [G loss: 0.999929]\n",
      "42 [D loss: 0.999970] [G loss: 0.999927]\n",
      "43 [D loss: 0.999978] [G loss: 0.999928]\n",
      "44 [D loss: 0.999982] [G loss: 0.999926]\n",
      "45 [D loss: 0.999988] [G loss: 0.999931]\n",
      "46 [D loss: 0.999995] [G loss: 0.999931]\n",
      "47 [D loss: 1.000003] [G loss: 0.999937]\n",
      "48 [D loss: 1.000008] [G loss: 0.999937]\n",
      "49 [D loss: 1.000029] [G loss: 0.999937]\n",
      "50 [D loss: 1.000046] [G loss: 0.999945]\n",
      "51 [D loss: 1.000060] [G loss: 0.999948]\n",
      "52 [D loss: 1.000078] [G loss: 0.999949]\n",
      "53 [D loss: 1.000120] [G loss: 0.999952]\n",
      "54 [D loss: 1.000152] [G loss: 0.999955]\n",
      "55 [D loss: 1.000194] [G loss: 0.999959]\n",
      "56 [D loss: 1.000213] [G loss: 0.999964]\n",
      "57 [D loss: 1.000263] [G loss: 0.999965]\n",
      "58 [D loss: 1.000316] [G loss: 0.999968]\n",
      "59 [D loss: 1.000366] [G loss: 0.999976]\n",
      "60 [D loss: 1.000443] [G loss: 0.999980]\n",
      "61 [D loss: 1.000512] [G loss: 0.999985]\n",
      "62 [D loss: 1.000581] [G loss: 0.999994]\n",
      "63 [D loss: 1.000668] [G loss: 1.000006]\n",
      "64 [D loss: 1.000718] [G loss: 1.000010]\n",
      "65 [D loss: 1.000851] [G loss: 1.000029]\n",
      "66 [D loss: 1.000973] [G loss: 1.000047]\n",
      "67 [D loss: 1.001103] [G loss: 1.000061]\n",
      "68 [D loss: 1.001150] [G loss: 1.000079]\n",
      "69 [D loss: 1.001352] [G loss: 1.000093]\n",
      "70 [D loss: 1.001596] [G loss: 1.000110]\n",
      "71 [D loss: 1.001716] [G loss: 1.000135]\n",
      "72 [D loss: 1.001984] [G loss: 1.000158]\n",
      "73 [D loss: 1.002095] [G loss: 1.000191]\n",
      "74 [D loss: 1.002250] [G loss: 1.000222]\n",
      "75 [D loss: 1.002490] [G loss: 1.000245]\n",
      "76 [D loss: 1.002688] [G loss: 1.000285]\n",
      "77 [D loss: 1.002914] [G loss: 1.000306]\n",
      "78 [D loss: 1.003115] [G loss: 1.000334]\n",
      "79 [D loss: 1.003232] [G loss: 1.000382]\n",
      "80 [D loss: 1.003441] [G loss: 1.000409]\n",
      "81 [D loss: 1.003758] [G loss: 1.000448]\n",
      "82 [D loss: 1.004005] [G loss: 1.000497]\n",
      "83 [D loss: 1.004377] [G loss: 1.000531]\n",
      "84 [D loss: 1.004658] [G loss: 1.000570]\n",
      "85 [D loss: 1.004743] [G loss: 1.000608]\n",
      "86 [D loss: 1.004730] [G loss: 1.000658]\n",
      "87 [D loss: 1.004734] [G loss: 1.000684]\n",
      "88 [D loss: 1.005038] [G loss: 1.000738]\n",
      "89 [D loss: 1.005682] [G loss: 1.000759]\n",
      "90 [D loss: 1.005487] [G loss: 1.000813]\n",
      "91 [D loss: 1.005938] [G loss: 1.000859]\n",
      "92 [D loss: 1.005998] [G loss: 1.000906]\n",
      "93 [D loss: 1.006161] [G loss: 1.000948]\n",
      "94 [D loss: 1.006560] [G loss: 1.000976]\n",
      "95 [D loss: 1.006650] [G loss: 1.001026]\n",
      "96 [D loss: 1.007379] [G loss: 1.001094]\n",
      "97 [D loss: 1.007178] [G loss: 1.001128]\n",
      "98 [D loss: 1.007071] [G loss: 1.001194]\n",
      "99 [D loss: 1.007268] [G loss: 1.001243]\n",
      "100 [D loss: 1.007594] [G loss: 1.001298]\n",
      "101 [D loss: 1.007741] [G loss: 1.001336]\n",
      "102 [D loss: 1.008048] [G loss: 1.001388]\n",
      "103 [D loss: 1.008164] [G loss: 1.001452]\n",
      "104 [D loss: 1.007573] [G loss: 1.001481]\n",
      "105 [D loss: 1.007866] [G loss: 1.001516]\n",
      "106 [D loss: 1.008611] [G loss: 1.001538]\n",
      "107 [D loss: 1.008831] [G loss: 1.001575]\n",
      "108 [D loss: 1.008300] [G loss: 1.001655]\n",
      "109 [D loss: 1.009441] [G loss: 1.001661]\n",
      "110 [D loss: 1.008788] [G loss: 1.001705]\n",
      "111 [D loss: 1.008105] [G loss: 1.001724]\n",
      "112 [D loss: 1.009089] [G loss: 1.001747]\n",
      "113 [D loss: 1.009850] [G loss: 1.001785]\n",
      "114 [D loss: 1.009970] [G loss: 1.001816]\n",
      "115 [D loss: 1.009747] [G loss: 1.001857]\n",
      "116 [D loss: 1.009694] [G loss: 1.001880]\n",
      "117 [D loss: 1.008817] [G loss: 1.001908]\n",
      "118 [D loss: 1.009599] [G loss: 1.001926]\n",
      "119 [D loss: 1.009950] [G loss: 1.001975]\n",
      "120 [D loss: 1.010400] [G loss: 1.001987]\n",
      "121 [D loss: 1.010577] [G loss: 1.002052]\n",
      "122 [D loss: 1.009886] [G loss: 1.002066]\n",
      "123 [D loss: 1.010907] [G loss: 1.002098]\n",
      "124 [D loss: 1.009905] [G loss: 1.002098]\n",
      "125 [D loss: 1.010883] [G loss: 1.002160]\n",
      "126 [D loss: 1.010139] [G loss: 1.002166]\n",
      "127 [D loss: 1.011505] [G loss: 1.002207]\n",
      "128 [D loss: 1.011137] [G loss: 1.002250]\n",
      "129 [D loss: 1.010810] [G loss: 1.002246]\n",
      "130 [D loss: 1.011647] [G loss: 1.002274]\n",
      "131 [D loss: 1.012284] [G loss: 1.002296]\n",
      "132 [D loss: 1.012176] [G loss: 1.002277]\n",
      "133 [D loss: 1.012475] [G loss: 1.002370]\n",
      "134 [D loss: 1.011537] [G loss: 1.002388]\n",
      "135 [D loss: 1.011817] [G loss: 1.002389]\n",
      "136 [D loss: 1.011035] [G loss: 1.002356]\n",
      "137 [D loss: 1.012566] [G loss: 1.002373]\n",
      "138 [D loss: 1.012363] [G loss: 1.002396]\n",
      "139 [D loss: 1.012694] [G loss: 1.002467]\n",
      "140 [D loss: 1.012340] [G loss: 1.002491]\n",
      "141 [D loss: 1.012914] [G loss: 1.002502]\n",
      "142 [D loss: 1.011459] [G loss: 1.002480]\n",
      "143 [D loss: 1.012528] [G loss: 1.002505]\n",
      "144 [D loss: 1.012529] [G loss: 1.002551]\n",
      "145 [D loss: 1.012109] [G loss: 1.002567]\n",
      "146 [D loss: 1.013089] [G loss: 1.002552]\n",
      "147 [D loss: 1.013479] [G loss: 1.002619]\n",
      "148 [D loss: 1.013097] [G loss: 1.002632]\n",
      "149 [D loss: 1.012987] [G loss: 1.002616]\n",
      "150 [D loss: 1.012674] [G loss: 1.002605]\n",
      "151 [D loss: 1.013599] [G loss: 1.002649]\n",
      "152 [D loss: 1.013592] [G loss: 1.002608]\n",
      "153 [D loss: 1.012704] [G loss: 1.002601]\n",
      "154 [D loss: 1.013952] [G loss: 1.002576]\n",
      "155 [D loss: 1.012882] [G loss: 1.002597]\n",
      "156 [D loss: 1.014165] [G loss: 1.002542]\n",
      "157 [D loss: 1.013296] [G loss: 1.002614]\n",
      "158 [D loss: 1.013900] [G loss: 1.002571]\n",
      "159 [D loss: 1.013269] [G loss: 1.002581]\n",
      "160 [D loss: 1.013594] [G loss: 1.002566]\n",
      "161 [D loss: 1.014395] [G loss: 1.002582]\n",
      "162 [D loss: 1.013603] [G loss: 1.002605]\n",
      "163 [D loss: 1.014122] [G loss: 1.002549]\n",
      "164 [D loss: 1.014944] [G loss: 1.002576]\n",
      "165 [D loss: 1.013776] [G loss: 1.002533]\n",
      "166 [D loss: 1.013680] [G loss: 1.002557]\n",
      "167 [D loss: 1.014370] [G loss: 1.002607]\n",
      "168 [D loss: 1.014131] [G loss: 1.002555]\n",
      "169 [D loss: 1.014746] [G loss: 1.002565]\n",
      "170 [D loss: 1.014867] [G loss: 1.002579]\n",
      "171 [D loss: 1.014063] [G loss: 1.002505]\n",
      "172 [D loss: 1.016250] [G loss: 1.002544]\n",
      "173 [D loss: 1.015169] [G loss: 1.002536]\n",
      "174 [D loss: 1.015121] [G loss: 1.002465]\n",
      "175 [D loss: 1.014867] [G loss: 1.002528]\n",
      "176 [D loss: 1.015363] [G loss: 1.002466]\n",
      "177 [D loss: 1.015509] [G loss: 1.002466]\n",
      "178 [D loss: 1.016068] [G loss: 1.002457]\n",
      "179 [D loss: 1.014520] [G loss: 1.002522]\n",
      "180 [D loss: 1.015546] [G loss: 1.002460]\n",
      "181 [D loss: 1.015136] [G loss: 1.002427]\n",
      "182 [D loss: 1.015528] [G loss: 1.002437]\n",
      "183 [D loss: 1.015513] [G loss: 1.002457]\n",
      "184 [D loss: 1.016487] [G loss: 1.002460]\n",
      "185 [D loss: 1.015010] [G loss: 1.002416]\n",
      "186 [D loss: 1.016044] [G loss: 1.002375]\n",
      "187 [D loss: 1.015993] [G loss: 1.002317]\n",
      "188 [D loss: 1.015360] [G loss: 1.002363]\n",
      "189 [D loss: 1.015256] [G loss: 1.002342]\n",
      "190 [D loss: 1.016000] [G loss: 1.002350]\n",
      "191 [D loss: 1.016050] [G loss: 1.002347]\n",
      "192 [D loss: 1.016661] [G loss: 1.002344]\n",
      "193 [D loss: 1.016117] [G loss: 1.002284]\n",
      "194 [D loss: 1.015631] [G loss: 1.002298]\n",
      "195 [D loss: 1.015631] [G loss: 1.002342]\n",
      "196 [D loss: 1.014883] [G loss: 1.002327]\n",
      "197 [D loss: 1.016382] [G loss: 1.002303]\n",
      "198 [D loss: 1.016964] [G loss: 1.002305]\n",
      "199 [D loss: 1.017519] [G loss: 1.002308]\n",
      "200 [D loss: 1.016132] [G loss: 1.002240]\n",
      "201 [D loss: 1.015590] [G loss: 1.002260]\n",
      "202 [D loss: 1.016959] [G loss: 1.002242]\n",
      "203 [D loss: 1.017083] [G loss: 1.002306]\n",
      "204 [D loss: 1.015885] [G loss: 1.002234]\n",
      "205 [D loss: 1.016865] [G loss: 1.002208]\n",
      "206 [D loss: 1.017307] [G loss: 1.002233]\n",
      "207 [D loss: 1.017415] [G loss: 1.002185]\n",
      "208 [D loss: 1.016268] [G loss: 1.002191]\n",
      "209 [D loss: 1.017288] [G loss: 1.002161]\n",
      "210 [D loss: 1.016871] [G loss: 1.002193]\n",
      "211 [D loss: 1.017729] [G loss: 1.002222]\n",
      "212 [D loss: 1.017316] [G loss: 1.002144]\n",
      "213 [D loss: 1.015859] [G loss: 1.002137]\n",
      "214 [D loss: 1.016961] [G loss: 1.002142]\n",
      "215 [D loss: 1.016922] [G loss: 1.002088]\n",
      "216 [D loss: 1.016957] [G loss: 1.002159]\n",
      "217 [D loss: 1.017964] [G loss: 1.002064]\n",
      "218 [D loss: 1.017999] [G loss: 1.002084]\n",
      "219 [D loss: 1.017300] [G loss: 1.002023]\n",
      "220 [D loss: 1.016787] [G loss: 1.002072]\n",
      "221 [D loss: 1.018861] [G loss: 1.002007]\n",
      "222 [D loss: 1.018073] [G loss: 1.001987]\n",
      "223 [D loss: 1.016954] [G loss: 1.001953]\n",
      "224 [D loss: 1.018118] [G loss: 1.002009]\n",
      "225 [D loss: 1.016525] [G loss: 1.001991]\n",
      "226 [D loss: 1.017253] [G loss: 1.001944]\n",
      "227 [D loss: 1.016870] [G loss: 1.001909]\n",
      "228 [D loss: 1.016034] [G loss: 1.002008]\n",
      "229 [D loss: 1.017641] [G loss: 1.001942]\n",
      "230 [D loss: 1.018917] [G loss: 1.001949]\n",
      "231 [D loss: 1.016594] [G loss: 1.001914]\n",
      "232 [D loss: 1.018393] [G loss: 1.001800]\n",
      "233 [D loss: 1.018122] [G loss: 1.001915]\n",
      "234 [D loss: 1.016588] [G loss: 1.001878]\n",
      "235 [D loss: 1.016327] [G loss: 1.001795]\n",
      "236 [D loss: 1.018396] [G loss: 1.001788]\n",
      "237 [D loss: 1.018231] [G loss: 1.001837]\n",
      "238 [D loss: 1.017044] [G loss: 1.001763]\n",
      "239 [D loss: 1.016942] [G loss: 1.001782]\n",
      "240 [D loss: 1.017897] [G loss: 1.001769]\n",
      "241 [D loss: 1.019442] [G loss: 1.001740]\n",
      "242 [D loss: 1.018710] [G loss: 1.001745]\n",
      "243 [D loss: 1.017913] [G loss: 1.001730]\n",
      "244 [D loss: 1.018849] [G loss: 1.001702]\n",
      "245 [D loss: 1.017971] [G loss: 1.001717]\n",
      "246 [D loss: 1.019192] [G loss: 1.001699]\n",
      "247 [D loss: 1.018689] [G loss: 1.001675]\n",
      "248 [D loss: 1.017804] [G loss: 1.001673]\n",
      "249 [D loss: 1.017891] [G loss: 1.001616]\n",
      "250 [D loss: 1.016663] [G loss: 1.001608]\n",
      "251 [D loss: 1.019443] [G loss: 1.001608]\n",
      "252 [D loss: 1.020125] [G loss: 1.001560]\n",
      "253 [D loss: 1.018807] [G loss: 1.001577]\n",
      "254 [D loss: 1.017956] [G loss: 1.001575]\n",
      "255 [D loss: 1.020613] [G loss: 1.001534]\n",
      "256 [D loss: 1.019779] [G loss: 1.001600]\n",
      "257 [D loss: 1.018032] [G loss: 1.001645]\n",
      "258 [D loss: 1.018206] [G loss: 1.001638]\n",
      "259 [D loss: 1.018858] [G loss: 1.001528]\n",
      "260 [D loss: 1.019047] [G loss: 1.001495]\n",
      "261 [D loss: 1.017922] [G loss: 1.001532]\n",
      "262 [D loss: 1.016746] [G loss: 1.001530]\n",
      "263 [D loss: 1.019314] [G loss: 1.001596]\n",
      "264 [D loss: 1.018427] [G loss: 1.001547]\n",
      "265 [D loss: 1.018984] [G loss: 1.001549]\n",
      "266 [D loss: 1.018422] [G loss: 1.001504]\n",
      "267 [D loss: 1.020260] [G loss: 1.001457]\n",
      "268 [D loss: 1.021037] [G loss: 1.001431]\n",
      "269 [D loss: 1.018843] [G loss: 1.001447]\n",
      "270 [D loss: 1.019144] [G loss: 1.001540]\n",
      "271 [D loss: 1.018673] [G loss: 1.001359]\n",
      "272 [D loss: 1.020265] [G loss: 1.001386]\n",
      "273 [D loss: 1.017953] [G loss: 1.001414]\n",
      "274 [D loss: 1.019163] [G loss: 1.001452]\n",
      "275 [D loss: 1.019244] [G loss: 1.001462]\n",
      "276 [D loss: 1.018955] [G loss: 1.001395]\n",
      "277 [D loss: 1.019393] [G loss: 1.001418]\n",
      "278 [D loss: 1.019840] [G loss: 1.001412]\n",
      "279 [D loss: 1.019091] [G loss: 1.001364]\n",
      "280 [D loss: 1.019599] [G loss: 1.001413]\n",
      "281 [D loss: 1.019760] [G loss: 1.001372]\n",
      "282 [D loss: 1.019198] [G loss: 1.001390]\n",
      "283 [D loss: 1.019536] [G loss: 1.001374]\n",
      "284 [D loss: 1.019123] [G loss: 1.001289]\n",
      "285 [D loss: 1.021978] [G loss: 1.001336]\n",
      "286 [D loss: 1.021595] [G loss: 1.001344]\n",
      "287 [D loss: 1.018853] [G loss: 1.001310]\n",
      "288 [D loss: 1.020106] [G loss: 1.001273]\n",
      "289 [D loss: 1.016743] [G loss: 1.001375]\n",
      "290 [D loss: 1.020013] [G loss: 1.001253]\n",
      "291 [D loss: 1.019149] [G loss: 1.001352]\n",
      "292 [D loss: 1.020359] [G loss: 1.001326]\n",
      "293 [D loss: 1.021347] [G loss: 1.001317]\n",
      "294 [D loss: 1.018669] [G loss: 1.001322]\n",
      "295 [D loss: 1.020876] [G loss: 1.001335]\n",
      "296 [D loss: 1.020796] [G loss: 1.001219]\n",
      "297 [D loss: 1.019185] [G loss: 1.001311]\n",
      "298 [D loss: 1.021148] [G loss: 1.001338]\n",
      "299 [D loss: 1.018607] [G loss: 1.001223]\n",
      "300 [D loss: 1.019327] [G loss: 1.001194]\n",
      "301 [D loss: 1.019829] [G loss: 1.001202]\n",
      "302 [D loss: 1.021257] [G loss: 1.001185]\n",
      "303 [D loss: 1.019654] [G loss: 1.001223]\n",
      "304 [D loss: 1.019253] [G loss: 1.001205]\n",
      "305 [D loss: 1.020398] [G loss: 1.001234]\n",
      "306 [D loss: 1.020531] [G loss: 1.001279]\n",
      "307 [D loss: 1.021592] [G loss: 1.001246]\n",
      "308 [D loss: 1.021094] [G loss: 1.001194]\n",
      "309 [D loss: 1.021374] [G loss: 1.001302]\n",
      "310 [D loss: 1.021215] [G loss: 1.001129]\n",
      "311 [D loss: 1.020401] [G loss: 1.001185]\n",
      "312 [D loss: 1.019579] [G loss: 1.001137]\n",
      "313 [D loss: 1.020052] [G loss: 1.001098]\n",
      "314 [D loss: 1.019205] [G loss: 1.001079]\n",
      "315 [D loss: 1.019827] [G loss: 1.001173]\n",
      "316 [D loss: 1.021055] [G loss: 1.001119]\n",
      "317 [D loss: 1.020362] [G loss: 1.001228]\n",
      "318 [D loss: 1.020401] [G loss: 1.001097]\n",
      "319 [D loss: 1.020207] [G loss: 1.001106]\n",
      "320 [D loss: 1.021461] [G loss: 1.001124]\n",
      "321 [D loss: 1.020114] [G loss: 1.001085]\n",
      "322 [D loss: 1.019214] [G loss: 1.001198]\n",
      "323 [D loss: 1.019564] [G loss: 1.000997]\n",
      "324 [D loss: 1.019405] [G loss: 1.001206]\n",
      "325 [D loss: 1.021514] [G loss: 1.001057]\n",
      "326 [D loss: 1.022626] [G loss: 1.001122]\n",
      "327 [D loss: 1.020505] [G loss: 1.001005]\n",
      "328 [D loss: 1.019270] [G loss: 1.001025]\n",
      "329 [D loss: 1.019209] [G loss: 1.000947]\n",
      "330 [D loss: 1.019694] [G loss: 1.001032]\n",
      "331 [D loss: 1.019994] [G loss: 1.001036]\n",
      "332 [D loss: 1.022777] [G loss: 1.001073]\n",
      "333 [D loss: 1.020302] [G loss: 1.000988]\n",
      "334 [D loss: 1.021565] [G loss: 1.001017]\n",
      "335 [D loss: 1.019588] [G loss: 1.001137]\n",
      "336 [D loss: 1.018449] [G loss: 1.001098]\n",
      "337 [D loss: 1.020686] [G loss: 1.000937]\n",
      "338 [D loss: 1.019488] [G loss: 1.000979]\n",
      "339 [D loss: 1.021869] [G loss: 1.000983]\n",
      "340 [D loss: 1.022183] [G loss: 1.000962]\n",
      "341 [D loss: 1.021606] [G loss: 1.001021]\n",
      "342 [D loss: 1.020762] [G loss: 1.000979]\n",
      "343 [D loss: 1.018040] [G loss: 1.000951]\n",
      "344 [D loss: 1.021737] [G loss: 1.000926]\n",
      "345 [D loss: 1.020380] [G loss: 1.000891]\n",
      "346 [D loss: 1.019208] [G loss: 1.000940]\n",
      "347 [D loss: 1.022480] [G loss: 1.000933]\n",
      "348 [D loss: 1.020397] [G loss: 1.000816]\n",
      "349 [D loss: 1.021756] [G loss: 1.000895]\n",
      "350 [D loss: 1.021854] [G loss: 1.000850]\n",
      "351 [D loss: 1.022331] [G loss: 1.000888]\n",
      "352 [D loss: 1.021490] [G loss: 1.000765]\n",
      "353 [D loss: 1.021622] [G loss: 1.000764]\n",
      "354 [D loss: 1.022234] [G loss: 1.000873]\n",
      "355 [D loss: 1.021822] [G loss: 1.000849]\n",
      "356 [D loss: 1.018402] [G loss: 1.000828]\n",
      "357 [D loss: 1.021058] [G loss: 1.000852]\n",
      "358 [D loss: 1.021525] [G loss: 1.000835]\n",
      "359 [D loss: 1.021040] [G loss: 1.000775]\n",
      "360 [D loss: 1.022414] [G loss: 1.000805]\n",
      "361 [D loss: 1.021773] [G loss: 1.000832]\n",
      "362 [D loss: 1.022246] [G loss: 1.000734]\n",
      "363 [D loss: 1.020268] [G loss: 1.000854]\n",
      "364 [D loss: 1.021267] [G loss: 1.000770]\n",
      "365 [D loss: 1.022550] [G loss: 1.000839]\n",
      "366 [D loss: 1.019847] [G loss: 1.000769]\n",
      "367 [D loss: 1.021405] [G loss: 1.000718]\n",
      "368 [D loss: 1.022678] [G loss: 1.000750]\n",
      "369 [D loss: 1.021328] [G loss: 1.000747]\n",
      "370 [D loss: 1.022749] [G loss: 1.000792]\n",
      "371 [D loss: 1.022924] [G loss: 1.000738]\n",
      "372 [D loss: 1.023205] [G loss: 1.000800]\n",
      "373 [D loss: 1.021053] [G loss: 1.000813]\n",
      "374 [D loss: 1.022602] [G loss: 1.000716]\n",
      "375 [D loss: 1.022402] [G loss: 1.000702]\n",
      "376 [D loss: 1.022482] [G loss: 1.000794]\n",
      "377 [D loss: 1.021940] [G loss: 1.000706]\n",
      "378 [D loss: 1.021731] [G loss: 1.000623]\n",
      "379 [D loss: 1.020625] [G loss: 1.000683]\n",
      "380 [D loss: 1.021690] [G loss: 1.000781]\n",
      "381 [D loss: 1.020201] [G loss: 1.000716]\n",
      "382 [D loss: 1.022822] [G loss: 1.000632]\n",
      "383 [D loss: 1.021215] [G loss: 1.000604]\n",
      "384 [D loss: 1.020740] [G loss: 1.000641]\n",
      "385 [D loss: 1.023263] [G loss: 1.000707]\n",
      "386 [D loss: 1.021483] [G loss: 1.000621]\n",
      "387 [D loss: 1.019541] [G loss: 1.000621]\n",
      "388 [D loss: 1.023352] [G loss: 1.000633]\n",
      "389 [D loss: 1.021851] [G loss: 1.000658]\n",
      "390 [D loss: 1.021714] [G loss: 1.000633]\n",
      "391 [D loss: 1.022517] [G loss: 1.000542]\n",
      "392 [D loss: 1.019625] [G loss: 1.000581]\n",
      "393 [D loss: 1.023014] [G loss: 1.000571]\n",
      "394 [D loss: 1.021923] [G loss: 1.000601]\n",
      "395 [D loss: 1.019801] [G loss: 1.000555]\n",
      "396 [D loss: 1.019751] [G loss: 1.000547]\n",
      "397 [D loss: 1.021455] [G loss: 1.000579]\n",
      "398 [D loss: 1.021051] [G loss: 1.000472]\n",
      "399 [D loss: 1.023505] [G loss: 1.000529]\n",
      "400 [D loss: 1.021523] [G loss: 1.000525]\n",
      "401 [D loss: 1.020016] [G loss: 1.000498]\n",
      "402 [D loss: 1.022093] [G loss: 1.000579]\n",
      "403 [D loss: 1.023323] [G loss: 1.000563]\n",
      "404 [D loss: 1.020855] [G loss: 1.000540]\n",
      "405 [D loss: 1.022269] [G loss: 1.000601]\n",
      "406 [D loss: 1.023138] [G loss: 1.000558]\n",
      "407 [D loss: 1.021433] [G loss: 1.000472]\n",
      "408 [D loss: 1.020582] [G loss: 1.000616]\n",
      "409 [D loss: 1.024231] [G loss: 1.000527]\n",
      "410 [D loss: 1.019706] [G loss: 1.000484]\n",
      "411 [D loss: 1.024452] [G loss: 1.000432]\n",
      "412 [D loss: 1.019705] [G loss: 1.000539]\n",
      "413 [D loss: 1.023831] [G loss: 1.000545]\n",
      "414 [D loss: 1.022163] [G loss: 1.000484]\n",
      "415 [D loss: 1.022099] [G loss: 1.000511]\n",
      "416 [D loss: 1.021709] [G loss: 1.000477]\n",
      "417 [D loss: 1.021883] [G loss: 1.000534]\n",
      "418 [D loss: 1.024412] [G loss: 1.000544]\n",
      "419 [D loss: 1.022122] [G loss: 1.000510]\n",
      "420 [D loss: 1.021602] [G loss: 1.000509]\n",
      "421 [D loss: 1.022346] [G loss: 1.000518]\n",
      "422 [D loss: 1.022330] [G loss: 1.000543]\n",
      "423 [D loss: 1.022143] [G loss: 1.000515]\n",
      "424 [D loss: 1.022881] [G loss: 1.000524]\n",
      "425 [D loss: 1.020548] [G loss: 1.000504]\n",
      "426 [D loss: 1.024302] [G loss: 1.000526]\n",
      "427 [D loss: 1.023252] [G loss: 1.000497]\n",
      "428 [D loss: 1.023195] [G loss: 1.000464]\n",
      "429 [D loss: 1.021962] [G loss: 1.000486]\n",
      "430 [D loss: 1.021687] [G loss: 1.000520]\n",
      "431 [D loss: 1.021045] [G loss: 1.000455]\n",
      "432 [D loss: 1.023160] [G loss: 1.000555]\n",
      "433 [D loss: 1.024024] [G loss: 1.000572]\n",
      "434 [D loss: 1.025284] [G loss: 1.000469]\n",
      "435 [D loss: 1.022409] [G loss: 1.000449]\n",
      "436 [D loss: 1.025791] [G loss: 1.000488]\n",
      "437 [D loss: 1.022856] [G loss: 1.000427]\n",
      "438 [D loss: 1.022601] [G loss: 1.000456]\n",
      "439 [D loss: 1.022671] [G loss: 1.000388]\n",
      "440 [D loss: 1.022393] [G loss: 1.000383]\n",
      "441 [D loss: 1.021995] [G loss: 1.000389]\n",
      "442 [D loss: 1.023155] [G loss: 1.000457]\n",
      "443 [D loss: 1.022032] [G loss: 1.000418]\n",
      "444 [D loss: 1.022944] [G loss: 1.000469]\n",
      "445 [D loss: 1.023511] [G loss: 1.000438]\n",
      "446 [D loss: 1.024628] [G loss: 1.000297]\n",
      "447 [D loss: 1.024929] [G loss: 1.000474]\n",
      "448 [D loss: 1.021349] [G loss: 1.000341]\n",
      "449 [D loss: 1.022307] [G loss: 1.000364]\n",
      "450 [D loss: 1.023043] [G loss: 1.000357]\n",
      "451 [D loss: 1.024494] [G loss: 1.000386]\n",
      "452 [D loss: 1.021637] [G loss: 1.000438]\n",
      "453 [D loss: 1.023936] [G loss: 1.000291]\n",
      "454 [D loss: 1.024407] [G loss: 1.000406]\n",
      "455 [D loss: 1.022821] [G loss: 1.000319]\n",
      "456 [D loss: 1.022441] [G loss: 1.000362]\n",
      "457 [D loss: 1.022452] [G loss: 1.000431]\n",
      "458 [D loss: 1.023646] [G loss: 1.000375]\n",
      "459 [D loss: 1.021718] [G loss: 1.000262]\n",
      "460 [D loss: 1.022018] [G loss: 1.000323]\n",
      "461 [D loss: 1.022803] [G loss: 1.000354]\n",
      "462 [D loss: 1.022024] [G loss: 1.000346]\n",
      "463 [D loss: 1.021895] [G loss: 1.000265]\n",
      "464 [D loss: 1.022963] [G loss: 1.000322]\n",
      "465 [D loss: 1.022906] [G loss: 1.000396]\n",
      "466 [D loss: 1.021783] [G loss: 1.000333]\n",
      "467 [D loss: 1.023192] [G loss: 1.000314]\n",
      "468 [D loss: 1.025516] [G loss: 1.000277]\n",
      "469 [D loss: 1.021646] [G loss: 1.000319]\n",
      "470 [D loss: 1.023977] [G loss: 1.000262]\n",
      "471 [D loss: 1.022141] [G loss: 1.000286]\n",
      "472 [D loss: 1.021933] [G loss: 1.000209]\n",
      "473 [D loss: 1.023123] [G loss: 1.000303]\n",
      "474 [D loss: 1.022074] [G loss: 1.000329]\n",
      "475 [D loss: 1.022185] [G loss: 1.000134]\n",
      "476 [D loss: 1.021941] [G loss: 1.000175]\n",
      "477 [D loss: 1.022776] [G loss: 1.000271]\n",
      "478 [D loss: 1.024150] [G loss: 1.000231]\n",
      "479 [D loss: 1.023710] [G loss: 1.000136]\n",
      "480 [D loss: 1.022204] [G loss: 1.000216]\n",
      "481 [D loss: 1.022704] [G loss: 1.000180]\n",
      "482 [D loss: 1.022734] [G loss: 1.000133]\n",
      "483 [D loss: 1.023951] [G loss: 1.000209]\n",
      "484 [D loss: 1.024683] [G loss: 1.000134]\n",
      "485 [D loss: 1.020987] [G loss: 1.000204]\n",
      "486 [D loss: 1.021829] [G loss: 1.000236]\n",
      "487 [D loss: 1.022710] [G loss: 1.000141]\n",
      "488 [D loss: 1.022513] [G loss: 1.000307]\n",
      "489 [D loss: 1.023513] [G loss: 1.000112]\n",
      "490 [D loss: 1.023739] [G loss: 1.000216]\n",
      "491 [D loss: 1.022623] [G loss: 1.000139]\n",
      "492 [D loss: 1.025025] [G loss: 1.000117]\n",
      "493 [D loss: 1.024759] [G loss: 1.000184]\n",
      "494 [D loss: 1.024219] [G loss: 1.000227]\n",
      "495 [D loss: 1.025242] [G loss: 1.000072]\n",
      "496 [D loss: 1.026113] [G loss: 1.000159]\n",
      "497 [D loss: 1.023648] [G loss: 1.000092]\n",
      "498 [D loss: 1.021896] [G loss: 1.000066]\n",
      "499 [D loss: 1.024587] [G loss: 1.000087]\n",
      "500 [D loss: 1.025041] [G loss: 1.000084]\n",
      "501 [D loss: 1.021520] [G loss: 1.000141]\n",
      "502 [D loss: 1.020542] [G loss: 1.000115]\n",
      "503 [D loss: 1.025398] [G loss: 1.000151]\n",
      "504 [D loss: 1.026315] [G loss: 1.000170]\n",
      "505 [D loss: 1.023440] [G loss: 1.000083]\n",
      "506 [D loss: 1.022507] [G loss: 1.000136]\n",
      "507 [D loss: 1.021881] [G loss: 1.000158]\n",
      "508 [D loss: 1.023218] [G loss: 1.000191]\n",
      "509 [D loss: 1.024865] [G loss: 1.000230]\n",
      "510 [D loss: 1.021428] [G loss: 1.000060]\n",
      "511 [D loss: 1.022636] [G loss: 1.000017]\n",
      "512 [D loss: 1.021873] [G loss: 1.000071]\n",
      "513 [D loss: 1.022805] [G loss: 1.000078]\n",
      "514 [D loss: 1.023244] [G loss: 1.000112]\n",
      "515 [D loss: 1.021826] [G loss: 1.000124]\n",
      "516 [D loss: 1.023629] [G loss: 1.000110]\n",
      "517 [D loss: 1.023898] [G loss: 1.000102]\n",
      "518 [D loss: 1.023802] [G loss: 1.000152]\n",
      "519 [D loss: 1.021450] [G loss: 1.000145]\n",
      "520 [D loss: 1.020970] [G loss: 1.000054]\n",
      "521 [D loss: 1.025140] [G loss: 1.000092]\n",
      "522 [D loss: 1.023960] [G loss: 1.000185]\n",
      "523 [D loss: 1.023862] [G loss: 1.000108]\n",
      "524 [D loss: 1.024375] [G loss: 1.000196]\n",
      "525 [D loss: 1.021775] [G loss: 1.000058]\n",
      "526 [D loss: 1.024231] [G loss: 0.999978]\n",
      "527 [D loss: 1.023584] [G loss: 1.000197]\n",
      "528 [D loss: 1.023535] [G loss: 1.000145]\n",
      "529 [D loss: 1.024287] [G loss: 1.000136]\n",
      "530 [D loss: 1.021956] [G loss: 1.000017]\n",
      "531 [D loss: 1.025191] [G loss: 1.000050]\n",
      "532 [D loss: 1.023988] [G loss: 1.000051]\n",
      "533 [D loss: 1.022735] [G loss: 1.000007]\n",
      "534 [D loss: 1.024091] [G loss: 1.000142]\n",
      "535 [D loss: 1.018332] [G loss: 1.000031]\n",
      "536 [D loss: 1.025070] [G loss: 1.000024]\n",
      "537 [D loss: 1.024072] [G loss: 1.000052]\n",
      "538 [D loss: 1.022553] [G loss: 1.000146]\n",
      "539 [D loss: 1.024808] [G loss: 1.000050]\n",
      "540 [D loss: 1.021392] [G loss: 1.000093]\n",
      "541 [D loss: 1.023808] [G loss: 1.000216]\n",
      "542 [D loss: 1.020186] [G loss: 1.000105]\n",
      "543 [D loss: 1.024386] [G loss: 1.000136]\n",
      "544 [D loss: 1.022162] [G loss: 1.000133]\n",
      "545 [D loss: 1.024385] [G loss: 1.000148]\n",
      "546 [D loss: 1.024356] [G loss: 1.000058]\n",
      "547 [D loss: 1.023656] [G loss: 1.000043]\n",
      "548 [D loss: 1.023678] [G loss: 1.000095]\n",
      "549 [D loss: 1.025443] [G loss: 1.000107]\n",
      "550 [D loss: 1.023583] [G loss: 1.000113]\n",
      "551 [D loss: 1.024104] [G loss: 1.000027]\n",
      "552 [D loss: 1.022129] [G loss: 1.000098]\n",
      "553 [D loss: 1.022394] [G loss: 1.000106]\n",
      "554 [D loss: 1.022813] [G loss: 1.000150]\n",
      "555 [D loss: 1.023352] [G loss: 1.000083]\n",
      "556 [D loss: 1.025499] [G loss: 1.000049]\n",
      "557 [D loss: 1.024151] [G loss: 1.000104]\n",
      "558 [D loss: 1.023541] [G loss: 1.000115]\n",
      "559 [D loss: 1.022230] [G loss: 1.000017]\n",
      "560 [D loss: 1.026356] [G loss: 1.000074]\n",
      "561 [D loss: 1.025306] [G loss: 1.000091]\n",
      "562 [D loss: 1.022092] [G loss: 1.000055]\n",
      "563 [D loss: 1.023011] [G loss: 1.000113]\n",
      "564 [D loss: 1.023452] [G loss: 1.000112]\n",
      "565 [D loss: 1.022747] [G loss: 1.000070]\n",
      "566 [D loss: 1.022323] [G loss: 1.000040]\n",
      "567 [D loss: 1.025083] [G loss: 1.000044]\n",
      "568 [D loss: 1.024883] [G loss: 1.000094]\n",
      "569 [D loss: 1.022992] [G loss: 1.000191]\n",
      "570 [D loss: 1.020975] [G loss: 1.000083]\n",
      "571 [D loss: 1.021923] [G loss: 1.000118]\n",
      "572 [D loss: 1.022547] [G loss: 1.000159]\n",
      "573 [D loss: 1.022738] [G loss: 1.000141]\n",
      "574 [D loss: 1.021930] [G loss: 1.000116]\n",
      "575 [D loss: 1.022112] [G loss: 1.000045]\n",
      "576 [D loss: 1.024177] [G loss: 1.000224]\n",
      "577 [D loss: 1.023839] [G loss: 1.000109]\n",
      "578 [D loss: 1.021061] [G loss: 1.000118]\n",
      "579 [D loss: 1.021772] [G loss: 1.000075]\n",
      "580 [D loss: 1.022579] [G loss: 1.000154]\n",
      "581 [D loss: 1.025327] [G loss: 1.000041]\n",
      "582 [D loss: 1.024169] [G loss: 1.000074]\n",
      "583 [D loss: 1.023467] [G loss: 1.000123]\n",
      "584 [D loss: 1.023965] [G loss: 1.000155]\n",
      "585 [D loss: 1.022255] [G loss: 1.000140]\n",
      "586 [D loss: 1.021801] [G loss: 0.999980]\n",
      "587 [D loss: 1.023500] [G loss: 1.000143]\n",
      "588 [D loss: 1.021669] [G loss: 1.000131]\n",
      "589 [D loss: 1.022838] [G loss: 1.000067]\n",
      "590 [D loss: 1.024089] [G loss: 1.000117]\n",
      "591 [D loss: 1.023445] [G loss: 1.000110]\n",
      "592 [D loss: 1.022628] [G loss: 1.000012]\n",
      "593 [D loss: 1.023536] [G loss: 1.000086]\n",
      "594 [D loss: 1.022540] [G loss: 1.000181]\n",
      "595 [D loss: 1.024573] [G loss: 1.000212]\n",
      "596 [D loss: 1.025235] [G loss: 1.000142]\n",
      "597 [D loss: 1.022390] [G loss: 1.000134]\n",
      "598 [D loss: 1.023612] [G loss: 1.000168]\n",
      "599 [D loss: 1.022257] [G loss: 1.000090]\n",
      "600 [D loss: 1.021074] [G loss: 1.000119]\n",
      "601 [D loss: 1.024937] [G loss: 1.000092]\n",
      "602 [D loss: 1.023917] [G loss: 1.000110]\n",
      "603 [D loss: 1.021878] [G loss: 1.000199]\n",
      "604 [D loss: 1.023595] [G loss: 1.000171]\n",
      "605 [D loss: 1.021859] [G loss: 1.000168]\n",
      "606 [D loss: 1.021144] [G loss: 1.000026]\n",
      "607 [D loss: 1.023886] [G loss: 1.000099]\n",
      "608 [D loss: 1.020750] [G loss: 1.000107]\n",
      "609 [D loss: 1.021268] [G loss: 1.000154]\n",
      "610 [D loss: 1.023357] [G loss: 1.000233]\n",
      "611 [D loss: 1.021451] [G loss: 1.000177]\n",
      "612 [D loss: 1.020722] [G loss: 1.000082]\n",
      "613 [D loss: 1.024677] [G loss: 1.000114]\n",
      "614 [D loss: 1.026141] [G loss: 1.000138]\n",
      "615 [D loss: 1.023330] [G loss: 1.000153]\n",
      "616 [D loss: 1.022055] [G loss: 1.000101]\n",
      "617 [D loss: 1.023332] [G loss: 1.000110]\n",
      "618 [D loss: 1.021214] [G loss: 1.000065]\n",
      "619 [D loss: 1.021419] [G loss: 1.000085]\n",
      "620 [D loss: 1.024781] [G loss: 1.000135]\n",
      "621 [D loss: 1.022320] [G loss: 1.000101]\n",
      "622 [D loss: 1.025826] [G loss: 1.000155]\n",
      "623 [D loss: 1.023299] [G loss: 1.000159]\n",
      "624 [D loss: 1.024037] [G loss: 1.000054]\n",
      "625 [D loss: 1.023893] [G loss: 1.000170]\n",
      "626 [D loss: 1.023629] [G loss: 1.000126]\n",
      "627 [D loss: 1.025755] [G loss: 1.000153]\n",
      "628 [D loss: 1.022688] [G loss: 1.000068]\n",
      "629 [D loss: 1.023286] [G loss: 1.000183]\n",
      "630 [D loss: 1.024177] [G loss: 1.000088]\n",
      "631 [D loss: 1.026338] [G loss: 1.000141]\n",
      "632 [D loss: 1.024302] [G loss: 1.000205]\n",
      "633 [D loss: 1.021810] [G loss: 1.000175]\n",
      "634 [D loss: 1.023804] [G loss: 1.000149]\n",
      "635 [D loss: 1.026204] [G loss: 1.000192]\n",
      "636 [D loss: 1.025213] [G loss: 1.000140]\n",
      "637 [D loss: 1.021740] [G loss: 1.000086]\n",
      "638 [D loss: 1.025861] [G loss: 1.000124]\n",
      "639 [D loss: 1.023814] [G loss: 1.000178]\n",
      "640 [D loss: 1.027066] [G loss: 1.000126]\n",
      "641 [D loss: 1.022271] [G loss: 1.000167]\n",
      "642 [D loss: 1.023485] [G loss: 1.000200]\n",
      "643 [D loss: 1.022957] [G loss: 1.000205]\n",
      "644 [D loss: 1.025325] [G loss: 1.000181]\n",
      "645 [D loss: 1.024259] [G loss: 1.000168]\n",
      "646 [D loss: 1.024592] [G loss: 1.000205]\n",
      "647 [D loss: 1.027053] [G loss: 1.000188]\n",
      "648 [D loss: 1.022468] [G loss: 1.000156]\n",
      "649 [D loss: 1.024038] [G loss: 1.000118]\n",
      "650 [D loss: 1.023896] [G loss: 1.000178]\n",
      "651 [D loss: 1.025669] [G loss: 1.000144]\n",
      "652 [D loss: 1.024789] [G loss: 1.000102]\n",
      "653 [D loss: 1.027444] [G loss: 1.000116]\n",
      "654 [D loss: 1.022375] [G loss: 1.000185]\n",
      "655 [D loss: 1.024471] [G loss: 1.000160]\n",
      "656 [D loss: 1.025053] [G loss: 1.000150]\n",
      "657 [D loss: 1.026346] [G loss: 1.000155]\n",
      "658 [D loss: 1.023284] [G loss: 1.000131]\n",
      "659 [D loss: 1.021854] [G loss: 1.000206]\n",
      "660 [D loss: 1.023153] [G loss: 1.000070]\n",
      "661 [D loss: 1.021967] [G loss: 1.000139]\n",
      "662 [D loss: 1.024647] [G loss: 1.000090]\n",
      "663 [D loss: 1.024449] [G loss: 1.000067]\n",
      "664 [D loss: 1.023713] [G loss: 1.000228]\n",
      "665 [D loss: 1.023330] [G loss: 1.000070]\n",
      "666 [D loss: 1.023821] [G loss: 1.000105]\n",
      "667 [D loss: 1.024746] [G loss: 1.000147]\n",
      "668 [D loss: 1.022842] [G loss: 1.000072]\n",
      "669 [D loss: 1.025223] [G loss: 1.000255]\n",
      "670 [D loss: 1.024913] [G loss: 1.000247]\n",
      "671 [D loss: 1.021614] [G loss: 1.000086]\n",
      "672 [D loss: 1.024774] [G loss: 1.000200]\n",
      "673 [D loss: 1.021928] [G loss: 1.000146]\n",
      "674 [D loss: 1.023895] [G loss: 1.000192]\n",
      "675 [D loss: 1.023492] [G loss: 1.000193]\n",
      "676 [D loss: 1.024391] [G loss: 1.000174]\n",
      "677 [D loss: 1.024427] [G loss: 1.000187]\n",
      "678 [D loss: 1.024656] [G loss: 1.000091]\n",
      "679 [D loss: 1.024309] [G loss: 1.000081]\n",
      "680 [D loss: 1.023220] [G loss: 1.000200]\n",
      "681 [D loss: 1.023555] [G loss: 1.000148]\n",
      "682 [D loss: 1.024154] [G loss: 1.000097]\n",
      "683 [D loss: 1.024859] [G loss: 1.000174]\n",
      "684 [D loss: 1.020468] [G loss: 1.000207]\n",
      "685 [D loss: 1.023507] [G loss: 1.000253]\n",
      "686 [D loss: 1.022315] [G loss: 1.000205]\n",
      "687 [D loss: 1.023274] [G loss: 1.000208]\n",
      "688 [D loss: 1.025473] [G loss: 1.000236]\n",
      "689 [D loss: 1.023614] [G loss: 1.000132]\n",
      "690 [D loss: 1.023238] [G loss: 1.000257]\n",
      "691 [D loss: 1.025754] [G loss: 1.000216]\n",
      "692 [D loss: 1.023285] [G loss: 1.000183]\n",
      "693 [D loss: 1.021701] [G loss: 1.000218]\n",
      "694 [D loss: 1.023368] [G loss: 1.000200]\n",
      "695 [D loss: 1.023523] [G loss: 1.000242]\n",
      "696 [D loss: 1.023677] [G loss: 1.000196]\n",
      "697 [D loss: 1.024125] [G loss: 1.000114]\n",
      "698 [D loss: 1.022752] [G loss: 1.000224]\n",
      "699 [D loss: 1.023658] [G loss: 1.000220]\n",
      "700 [D loss: 1.022011] [G loss: 1.000190]\n",
      "701 [D loss: 1.024479] [G loss: 1.000215]\n",
      "702 [D loss: 1.021972] [G loss: 1.000216]\n",
      "703 [D loss: 1.026062] [G loss: 1.000235]\n",
      "704 [D loss: 1.022675] [G loss: 1.000214]\n",
      "705 [D loss: 1.023333] [G loss: 1.000320]\n",
      "706 [D loss: 1.025936] [G loss: 1.000220]\n",
      "707 [D loss: 1.025447] [G loss: 1.000269]\n",
      "708 [D loss: 1.024115] [G loss: 1.000249]\n",
      "709 [D loss: 1.023198] [G loss: 1.000255]\n",
      "710 [D loss: 1.025283] [G loss: 1.000149]\n",
      "711 [D loss: 1.023962] [G loss: 1.000198]\n",
      "712 [D loss: 1.023467] [G loss: 1.000224]\n",
      "713 [D loss: 1.023746] [G loss: 1.000268]\n",
      "714 [D loss: 1.024651] [G loss: 1.000373]\n",
      "715 [D loss: 1.024139] [G loss: 1.000223]\n",
      "716 [D loss: 1.024636] [G loss: 1.000180]\n",
      "717 [D loss: 1.023184] [G loss: 1.000195]\n",
      "718 [D loss: 1.025262] [G loss: 1.000214]\n",
      "719 [D loss: 1.022684] [G loss: 1.000294]\n",
      "720 [D loss: 1.023005] [G loss: 1.000230]\n",
      "721 [D loss: 1.022967] [G loss: 1.000216]\n",
      "722 [D loss: 1.022615] [G loss: 1.000173]\n",
      "723 [D loss: 1.023462] [G loss: 1.000254]\n",
      "724 [D loss: 1.025671] [G loss: 1.000166]\n",
      "725 [D loss: 1.022733] [G loss: 1.000206]\n",
      "726 [D loss: 1.024134] [G loss: 1.000202]\n",
      "727 [D loss: 1.024820] [G loss: 1.000168]\n",
      "728 [D loss: 1.024086] [G loss: 1.000211]\n",
      "729 [D loss: 1.026372] [G loss: 1.000251]\n",
      "730 [D loss: 1.020006] [G loss: 1.000258]\n",
      "731 [D loss: 1.023371] [G loss: 1.000255]\n",
      "732 [D loss: 1.026332] [G loss: 1.000202]\n",
      "733 [D loss: 1.024867] [G loss: 1.000199]\n",
      "734 [D loss: 1.025534] [G loss: 1.000296]\n",
      "735 [D loss: 1.023216] [G loss: 1.000194]\n",
      "736 [D loss: 1.025790] [G loss: 1.000316]\n",
      "737 [D loss: 1.023432] [G loss: 1.000169]\n",
      "738 [D loss: 1.022174] [G loss: 1.000194]\n",
      "739 [D loss: 1.026388] [G loss: 1.000282]\n",
      "740 [D loss: 1.023305] [G loss: 1.000261]\n",
      "741 [D loss: 1.025257] [G loss: 1.000211]\n",
      "742 [D loss: 1.022742] [G loss: 1.000293]\n",
      "743 [D loss: 1.025889] [G loss: 1.000173]\n",
      "744 [D loss: 1.024142] [G loss: 1.000260]\n",
      "745 [D loss: 1.023584] [G loss: 1.000335]\n",
      "746 [D loss: 1.022723] [G loss: 1.000290]\n",
      "747 [D loss: 1.024382] [G loss: 1.000306]\n",
      "748 [D loss: 1.023255] [G loss: 1.000330]\n",
      "749 [D loss: 1.021408] [G loss: 1.000270]\n",
      "750 [D loss: 1.023199] [G loss: 1.000359]\n",
      "751 [D loss: 1.023978] [G loss: 1.000286]\n",
      "752 [D loss: 1.026391] [G loss: 1.000295]\n",
      "753 [D loss: 1.023077] [G loss: 1.000323]\n",
      "754 [D loss: 1.023062] [G loss: 1.000250]\n",
      "755 [D loss: 1.026810] [G loss: 1.000248]\n",
      "756 [D loss: 1.022762] [G loss: 1.000279]\n",
      "757 [D loss: 1.025238] [G loss: 1.000401]\n",
      "758 [D loss: 1.025825] [G loss: 1.000270]\n",
      "759 [D loss: 1.022404] [G loss: 1.000252]\n",
      "760 [D loss: 1.025078] [G loss: 1.000362]\n",
      "761 [D loss: 1.023151] [G loss: 1.000240]\n",
      "762 [D loss: 1.026220] [G loss: 1.000250]\n",
      "763 [D loss: 1.024806] [G loss: 1.000314]\n",
      "764 [D loss: 1.024174] [G loss: 1.000307]\n",
      "765 [D loss: 1.023045] [G loss: 1.000339]\n",
      "766 [D loss: 1.022894] [G loss: 1.000289]\n",
      "767 [D loss: 1.023482] [G loss: 1.000309]\n",
      "768 [D loss: 1.023778] [G loss: 1.000405]\n",
      "769 [D loss: 1.025301] [G loss: 1.000368]\n",
      "770 [D loss: 1.026859] [G loss: 1.000247]\n",
      "771 [D loss: 1.022658] [G loss: 1.000283]\n",
      "772 [D loss: 1.022212] [G loss: 1.000328]\n",
      "773 [D loss: 1.023868] [G loss: 1.000325]\n",
      "774 [D loss: 1.024559] [G loss: 1.000346]\n",
      "775 [D loss: 1.023641] [G loss: 1.000271]\n",
      "776 [D loss: 1.023754] [G loss: 1.000314]\n",
      "777 [D loss: 1.023624] [G loss: 1.000387]\n",
      "778 [D loss: 1.022892] [G loss: 1.000287]\n",
      "779 [D loss: 1.024254] [G loss: 1.000364]\n",
      "780 [D loss: 1.026557] [G loss: 1.000354]\n",
      "781 [D loss: 1.024125] [G loss: 1.000356]\n",
      "782 [D loss: 1.025915] [G loss: 1.000343]\n",
      "783 [D loss: 1.024309] [G loss: 1.000297]\n",
      "784 [D loss: 1.025553] [G loss: 1.000406]\n",
      "785 [D loss: 1.026554] [G loss: 1.000318]\n",
      "786 [D loss: 1.025127] [G loss: 1.000326]\n",
      "787 [D loss: 1.023969] [G loss: 1.000351]\n",
      "788 [D loss: 1.025106] [G loss: 1.000304]\n",
      "789 [D loss: 1.022743] [G loss: 1.000372]\n",
      "790 [D loss: 1.024605] [G loss: 1.000333]\n",
      "791 [D loss: 1.027612] [G loss: 1.000450]\n",
      "792 [D loss: 1.023602] [G loss: 1.000396]\n",
      "793 [D loss: 1.025125] [G loss: 1.000331]\n",
      "794 [D loss: 1.023229] [G loss: 1.000360]\n",
      "795 [D loss: 1.024173] [G loss: 1.000435]\n",
      "796 [D loss: 1.024564] [G loss: 1.000343]\n",
      "797 [D loss: 1.019335] [G loss: 1.000331]\n",
      "798 [D loss: 1.026000] [G loss: 1.000336]\n",
      "799 [D loss: 1.024946] [G loss: 1.000479]\n",
      "800 [D loss: 1.024850] [G loss: 1.000402]\n",
      "801 [D loss: 1.023644] [G loss: 1.000406]\n",
      "802 [D loss: 1.024855] [G loss: 1.000343]\n",
      "803 [D loss: 1.025136] [G loss: 1.000562]\n",
      "804 [D loss: 1.024361] [G loss: 1.000432]\n",
      "805 [D loss: 1.024046] [G loss: 1.000351]\n",
      "806 [D loss: 1.025402] [G loss: 1.000411]\n",
      "807 [D loss: 1.024200] [G loss: 1.000394]\n",
      "808 [D loss: 1.026318] [G loss: 1.000442]\n",
      "809 [D loss: 1.025665] [G loss: 1.000350]\n",
      "810 [D loss: 1.027410] [G loss: 1.000501]\n",
      "811 [D loss: 1.023847] [G loss: 1.000456]\n",
      "812 [D loss: 1.025169] [G loss: 1.000409]\n",
      "813 [D loss: 1.025701] [G loss: 1.000372]\n",
      "814 [D loss: 1.023491] [G loss: 1.000450]\n",
      "815 [D loss: 1.024988] [G loss: 1.000431]\n",
      "816 [D loss: 1.024942] [G loss: 1.000465]\n",
      "817 [D loss: 1.025061] [G loss: 1.000505]\n",
      "818 [D loss: 1.026989] [G loss: 1.000365]\n",
      "819 [D loss: 1.024414] [G loss: 1.000508]\n",
      "820 [D loss: 1.024404] [G loss: 1.000461]\n",
      "821 [D loss: 1.026083] [G loss: 1.000473]\n",
      "822 [D loss: 1.022634] [G loss: 1.000416]\n",
      "823 [D loss: 1.025152] [G loss: 1.000578]\n",
      "824 [D loss: 1.023903] [G loss: 1.000515]\n",
      "825 [D loss: 1.026371] [G loss: 1.000545]\n",
      "826 [D loss: 1.028145] [G loss: 1.000534]\n",
      "827 [D loss: 1.027356] [G loss: 1.000506]\n",
      "828 [D loss: 1.025911] [G loss: 1.000462]\n",
      "829 [D loss: 1.023590] [G loss: 1.000531]\n",
      "830 [D loss: 1.024868] [G loss: 1.000498]\n",
      "831 [D loss: 1.025696] [G loss: 1.000441]\n",
      "832 [D loss: 1.024677] [G loss: 1.000477]\n",
      "833 [D loss: 1.025169] [G loss: 1.000520]\n",
      "834 [D loss: 1.023900] [G loss: 1.000418]\n",
      "835 [D loss: 1.022571] [G loss: 1.000471]\n",
      "836 [D loss: 1.024373] [G loss: 1.000565]\n",
      "837 [D loss: 1.024851] [G loss: 1.000419]\n",
      "838 [D loss: 1.022794] [G loss: 1.000352]\n",
      "839 [D loss: 1.025947] [G loss: 1.000499]\n",
      "840 [D loss: 1.024968] [G loss: 1.000548]\n",
      "841 [D loss: 1.025527] [G loss: 1.000489]\n",
      "842 [D loss: 1.024004] [G loss: 1.000535]\n",
      "843 [D loss: 1.025289] [G loss: 1.000483]\n",
      "844 [D loss: 1.024325] [G loss: 1.000478]\n",
      "845 [D loss: 1.024534] [G loss: 1.000510]\n",
      "846 [D loss: 1.026034] [G loss: 1.000567]\n",
      "847 [D loss: 1.024367] [G loss: 1.000590]\n",
      "848 [D loss: 1.024174] [G loss: 1.000486]\n",
      "849 [D loss: 1.024830] [G loss: 1.000488]\n",
      "850 [D loss: 1.026990] [G loss: 1.000453]\n",
      "851 [D loss: 1.027848] [G loss: 1.000526]\n",
      "852 [D loss: 1.024363] [G loss: 1.000463]\n",
      "853 [D loss: 1.022857] [G loss: 1.000457]\n",
      "854 [D loss: 1.023565] [G loss: 1.000585]\n",
      "855 [D loss: 1.027951] [G loss: 1.000530]\n",
      "856 [D loss: 1.023373] [G loss: 1.000411]\n",
      "857 [D loss: 1.025034] [G loss: 1.000527]\n",
      "858 [D loss: 1.023112] [G loss: 1.000499]\n",
      "859 [D loss: 1.026393] [G loss: 1.000518]\n",
      "860 [D loss: 1.025682] [G loss: 1.000491]\n",
      "861 [D loss: 1.024268] [G loss: 1.000444]\n",
      "862 [D loss: 1.023650] [G loss: 1.000467]\n",
      "863 [D loss: 1.025102] [G loss: 1.000435]\n",
      "864 [D loss: 1.025540] [G loss: 1.000544]\n",
      "865 [D loss: 1.024518] [G loss: 1.000414]\n",
      "866 [D loss: 1.022241] [G loss: 1.000452]\n",
      "867 [D loss: 1.023245] [G loss: 1.000562]\n",
      "868 [D loss: 1.024530] [G loss: 1.000481]\n",
      "869 [D loss: 1.027300] [G loss: 1.000564]\n",
      "870 [D loss: 1.024860] [G loss: 1.000533]\n",
      "871 [D loss: 1.024466] [G loss: 1.000475]\n",
      "872 [D loss: 1.024692] [G loss: 1.000425]\n",
      "873 [D loss: 1.021939] [G loss: 1.000500]\n",
      "874 [D loss: 1.025438] [G loss: 1.000608]\n",
      "875 [D loss: 1.025475] [G loss: 1.000485]\n",
      "876 [D loss: 1.024704] [G loss: 1.000471]\n",
      "877 [D loss: 1.024272] [G loss: 1.000497]\n",
      "878 [D loss: 1.024262] [G loss: 1.000506]\n",
      "879 [D loss: 1.023824] [G loss: 1.000538]\n",
      "880 [D loss: 1.026311] [G loss: 1.000494]\n",
      "881 [D loss: 1.024409] [G loss: 1.000487]\n",
      "882 [D loss: 1.026688] [G loss: 1.000537]\n",
      "883 [D loss: 1.026094] [G loss: 1.000455]\n",
      "884 [D loss: 1.023794] [G loss: 1.000525]\n",
      "885 [D loss: 1.024376] [G loss: 1.000509]\n",
      "886 [D loss: 1.024635] [G loss: 1.000531]\n",
      "887 [D loss: 1.025932] [G loss: 1.000545]\n",
      "888 [D loss: 1.022823] [G loss: 1.000474]\n",
      "889 [D loss: 1.024002] [G loss: 1.000414]\n",
      "890 [D loss: 1.024293] [G loss: 1.000515]\n",
      "891 [D loss: 1.024222] [G loss: 1.000542]\n",
      "892 [D loss: 1.023946] [G loss: 1.000501]\n",
      "893 [D loss: 1.025186] [G loss: 1.000447]\n",
      "894 [D loss: 1.025223] [G loss: 1.000432]\n",
      "895 [D loss: 1.027243] [G loss: 1.000497]\n",
      "896 [D loss: 1.023697] [G loss: 1.000436]\n",
      "897 [D loss: 1.025357] [G loss: 1.000392]\n",
      "898 [D loss: 1.025396] [G loss: 1.000458]\n",
      "899 [D loss: 1.026102] [G loss: 1.000455]\n",
      "900 [D loss: 1.024768] [G loss: 1.000392]\n",
      "901 [D loss: 1.025948] [G loss: 1.000380]\n",
      "902 [D loss: 1.023417] [G loss: 1.000363]\n",
      "903 [D loss: 1.025531] [G loss: 1.000423]\n",
      "904 [D loss: 1.024466] [G loss: 1.000444]\n",
      "905 [D loss: 1.023675] [G loss: 1.000521]\n",
      "906 [D loss: 1.021408] [G loss: 1.000389]\n",
      "907 [D loss: 1.024437] [G loss: 1.000422]\n",
      "908 [D loss: 1.026042] [G loss: 1.000368]\n",
      "909 [D loss: 1.024075] [G loss: 1.000542]\n",
      "910 [D loss: 1.025306] [G loss: 1.000526]\n",
      "911 [D loss: 1.025736] [G loss: 1.000494]\n",
      "912 [D loss: 1.025348] [G loss: 1.000501]\n",
      "913 [D loss: 1.025610] [G loss: 1.000424]\n",
      "914 [D loss: 1.028683] [G loss: 1.000421]\n",
      "915 [D loss: 1.024140] [G loss: 1.000543]\n",
      "916 [D loss: 1.025900] [G loss: 1.000497]\n",
      "917 [D loss: 1.027034] [G loss: 1.000427]\n",
      "918 [D loss: 1.025031] [G loss: 1.000473]\n",
      "919 [D loss: 1.025426] [G loss: 1.000405]\n",
      "920 [D loss: 1.023642] [G loss: 1.000496]\n",
      "921 [D loss: 1.028021] [G loss: 1.000327]\n",
      "922 [D loss: 1.024745] [G loss: 1.000385]\n",
      "923 [D loss: 1.023321] [G loss: 1.000313]\n",
      "924 [D loss: 1.024921] [G loss: 1.000442]\n",
      "925 [D loss: 1.026107] [G loss: 1.000324]\n",
      "926 [D loss: 1.027449] [G loss: 1.000449]\n",
      "927 [D loss: 1.025512] [G loss: 1.000432]\n",
      "928 [D loss: 1.025437] [G loss: 1.000416]\n",
      "929 [D loss: 1.024903] [G loss: 1.000506]\n",
      "930 [D loss: 1.023126] [G loss: 1.000456]\n",
      "931 [D loss: 1.023973] [G loss: 1.000405]\n",
      "932 [D loss: 1.021844] [G loss: 1.000340]\n",
      "933 [D loss: 1.024949] [G loss: 1.000407]\n",
      "934 [D loss: 1.023306] [G loss: 1.000468]\n",
      "935 [D loss: 1.023863] [G loss: 1.000455]\n",
      "936 [D loss: 1.024641] [G loss: 1.000428]\n",
      "937 [D loss: 1.024603] [G loss: 1.000463]\n",
      "938 [D loss: 1.025392] [G loss: 1.000369]\n",
      "939 [D loss: 1.024214] [G loss: 1.000401]\n",
      "940 [D loss: 1.025001] [G loss: 1.000475]\n",
      "941 [D loss: 1.025656] [G loss: 1.000359]\n",
      "942 [D loss: 1.022919] [G loss: 1.000348]\n",
      "943 [D loss: 1.025672] [G loss: 1.000340]\n",
      "944 [D loss: 1.024952] [G loss: 1.000354]\n",
      "945 [D loss: 1.026009] [G loss: 1.000427]\n",
      "946 [D loss: 1.025434] [G loss: 1.000372]\n",
      "947 [D loss: 1.022288] [G loss: 1.000338]\n",
      "948 [D loss: 1.026302] [G loss: 1.000395]\n",
      "949 [D loss: 1.026642] [G loss: 1.000366]\n",
      "950 [D loss: 1.023808] [G loss: 1.000417]\n",
      "951 [D loss: 1.023665] [G loss: 1.000426]\n",
      "952 [D loss: 1.024969] [G loss: 1.000445]\n",
      "953 [D loss: 1.024912] [G loss: 1.000392]\n",
      "954 [D loss: 1.024261] [G loss: 1.000361]\n",
      "955 [D loss: 1.024586] [G loss: 1.000275]\n",
      "956 [D loss: 1.026337] [G loss: 1.000291]\n",
      "957 [D loss: 1.026533] [G loss: 1.000332]\n",
      "958 [D loss: 1.025968] [G loss: 1.000417]\n",
      "959 [D loss: 1.025873] [G loss: 1.000268]\n",
      "960 [D loss: 1.022848] [G loss: 1.000374]\n",
      "961 [D loss: 1.026451] [G loss: 1.000391]\n",
      "962 [D loss: 1.025040] [G loss: 1.000395]\n",
      "963 [D loss: 1.026372] [G loss: 1.000394]\n",
      "964 [D loss: 1.026014] [G loss: 1.000277]\n",
      "965 [D loss: 1.024715] [G loss: 1.000342]\n",
      "966 [D loss: 1.024893] [G loss: 1.000425]\n",
      "967 [D loss: 1.026717] [G loss: 1.000267]\n",
      "968 [D loss: 1.026002] [G loss: 1.000299]\n",
      "969 [D loss: 1.025740] [G loss: 1.000306]\n",
      "970 [D loss: 1.024043] [G loss: 1.000324]\n",
      "971 [D loss: 1.024885] [G loss: 1.000374]\n",
      "972 [D loss: 1.026857] [G loss: 1.000374]\n",
      "973 [D loss: 1.025338] [G loss: 1.000285]\n",
      "974 [D loss: 1.026171] [G loss: 1.000335]\n",
      "975 [D loss: 1.024346] [G loss: 1.000244]\n",
      "976 [D loss: 1.024745] [G loss: 1.000264]\n",
      "977 [D loss: 1.024151] [G loss: 1.000336]\n",
      "978 [D loss: 1.024468] [G loss: 1.000299]\n",
      "979 [D loss: 1.025509] [G loss: 1.000370]\n",
      "980 [D loss: 1.024118] [G loss: 1.000327]\n",
      "981 [D loss: 1.024671] [G loss: 1.000274]\n",
      "982 [D loss: 1.025859] [G loss: 1.000342]\n",
      "983 [D loss: 1.028569] [G loss: 1.000281]\n",
      "984 [D loss: 1.025608] [G loss: 1.000269]\n",
      "985 [D loss: 1.023782] [G loss: 1.000234]\n",
      "986 [D loss: 1.025792] [G loss: 1.000290]\n",
      "987 [D loss: 1.024761] [G loss: 1.000292]\n",
      "988 [D loss: 1.024780] [G loss: 1.000302]\n",
      "989 [D loss: 1.025031] [G loss: 1.000427]\n",
      "990 [D loss: 1.026933] [G loss: 1.000272]\n",
      "991 [D loss: 1.024366] [G loss: 1.000321]\n",
      "992 [D loss: 1.025626] [G loss: 1.000349]\n",
      "993 [D loss: 1.028700] [G loss: 1.000299]\n",
      "994 [D loss: 1.023577] [G loss: 1.000433]\n",
      "995 [D loss: 1.026830] [G loss: 1.000221]\n",
      "996 [D loss: 1.026139] [G loss: 1.000354]\n",
      "997 [D loss: 1.025018] [G loss: 1.000297]\n",
      "998 [D loss: 1.026195] [G loss: 1.000282]\n",
      "999 [D loss: 1.023576] [G loss: 1.000278]\n",
      "1000 [D loss: 1.023334] [G loss: 1.000405]\n",
      "1001 [D loss: 1.022938] [G loss: 1.000242]\n",
      "1002 [D loss: 1.025686] [G loss: 1.000288]\n",
      "1003 [D loss: 1.023411] [G loss: 1.000303]\n",
      "1004 [D loss: 1.023059] [G loss: 1.000327]\n",
      "1005 [D loss: 1.027056] [G loss: 1.000273]\n",
      "1006 [D loss: 1.024578] [G loss: 1.000189]\n",
      "1007 [D loss: 1.025582] [G loss: 1.000297]\n",
      "1008 [D loss: 1.023545] [G loss: 1.000260]\n",
      "1009 [D loss: 1.025742] [G loss: 1.000316]\n",
      "1010 [D loss: 1.026046] [G loss: 1.000238]\n",
      "1011 [D loss: 1.025315] [G loss: 1.000313]\n",
      "1012 [D loss: 1.026311] [G loss: 1.000229]\n",
      "1013 [D loss: 1.023129] [G loss: 1.000224]\n",
      "1014 [D loss: 1.025298] [G loss: 1.000258]\n",
      "1015 [D loss: 1.023671] [G loss: 1.000326]\n",
      "1016 [D loss: 1.024875] [G loss: 1.000290]\n",
      "1017 [D loss: 1.025211] [G loss: 1.000308]\n",
      "1018 [D loss: 1.021637] [G loss: 1.000298]\n",
      "1019 [D loss: 1.022137] [G loss: 1.000289]\n",
      "1020 [D loss: 1.025387] [G loss: 1.000291]\n",
      "1021 [D loss: 1.024452] [G loss: 1.000258]\n",
      "1022 [D loss: 1.024254] [G loss: 1.000203]\n",
      "1023 [D loss: 1.027414] [G loss: 1.000219]\n",
      "1024 [D loss: 1.025087] [G loss: 1.000273]\n",
      "1025 [D loss: 1.024679] [G loss: 1.000287]\n",
      "1026 [D loss: 1.023007] [G loss: 1.000159]\n",
      "1027 [D loss: 1.026359] [G loss: 1.000276]\n",
      "1028 [D loss: 1.022005] [G loss: 1.000283]\n",
      "1029 [D loss: 1.023831] [G loss: 1.000382]\n",
      "1030 [D loss: 1.026556] [G loss: 1.000252]\n",
      "1031 [D loss: 1.026356] [G loss: 1.000221]\n",
      "1032 [D loss: 1.023536] [G loss: 1.000246]\n",
      "1033 [D loss: 1.022759] [G loss: 1.000234]\n",
      "1034 [D loss: 1.024382] [G loss: 1.000182]\n",
      "1035 [D loss: 1.027290] [G loss: 1.000410]\n",
      "1036 [D loss: 1.025128] [G loss: 1.000252]\n",
      "1037 [D loss: 1.024879] [G loss: 1.000229]\n",
      "1038 [D loss: 1.026392] [G loss: 1.000321]\n",
      "1039 [D loss: 1.021088] [G loss: 1.000242]\n",
      "1040 [D loss: 1.024813] [G loss: 1.000204]\n",
      "1041 [D loss: 1.027182] [G loss: 1.000218]\n",
      "1042 [D loss: 1.024906] [G loss: 1.000260]\n",
      "1043 [D loss: 1.024249] [G loss: 1.000189]\n",
      "1044 [D loss: 1.025982] [G loss: 1.000218]\n",
      "1045 [D loss: 1.023559] [G loss: 1.000345]\n",
      "1046 [D loss: 1.024607] [G loss: 1.000177]\n",
      "1047 [D loss: 1.024961] [G loss: 1.000246]\n",
      "1048 [D loss: 1.026095] [G loss: 1.000226]\n",
      "1049 [D loss: 1.024064] [G loss: 1.000239]\n",
      "1050 [D loss: 1.025516] [G loss: 1.000163]\n",
      "1051 [D loss: 1.025257] [G loss: 1.000214]\n",
      "1052 [D loss: 1.024568] [G loss: 1.000223]\n",
      "1053 [D loss: 1.022717] [G loss: 1.000213]\n",
      "1054 [D loss: 1.022917] [G loss: 1.000287]\n",
      "1055 [D loss: 1.024301] [G loss: 1.000188]\n",
      "1056 [D loss: 1.025591] [G loss: 1.000171]\n",
      "1057 [D loss: 1.024116] [G loss: 1.000221]\n",
      "1058 [D loss: 1.025184] [G loss: 1.000238]\n",
      "1059 [D loss: 1.026797] [G loss: 1.000221]\n",
      "1060 [D loss: 1.025713] [G loss: 1.000157]\n",
      "1061 [D loss: 1.025491] [G loss: 1.000267]\n",
      "1062 [D loss: 1.023318] [G loss: 1.000289]\n",
      "1063 [D loss: 1.029469] [G loss: 1.000201]\n",
      "1064 [D loss: 1.024166] [G loss: 1.000129]\n",
      "1065 [D loss: 1.025804] [G loss: 1.000263]\n",
      "1066 [D loss: 1.024771] [G loss: 1.000334]\n",
      "1067 [D loss: 1.025464] [G loss: 1.000218]\n",
      "1068 [D loss: 1.024579] [G loss: 1.000162]\n",
      "1069 [D loss: 1.025444] [G loss: 1.000141]\n",
      "1070 [D loss: 1.024241] [G loss: 1.000126]\n",
      "1071 [D loss: 1.026450] [G loss: 1.000200]\n",
      "1072 [D loss: 1.027075] [G loss: 1.000205]\n",
      "1073 [D loss: 1.027492] [G loss: 1.000262]\n",
      "1074 [D loss: 1.026117] [G loss: 1.000097]\n",
      "1075 [D loss: 1.026857] [G loss: 1.000130]\n",
      "1076 [D loss: 1.024563] [G loss: 1.000177]\n",
      "1077 [D loss: 1.025443] [G loss: 1.000185]\n",
      "1078 [D loss: 1.027670] [G loss: 1.000157]\n",
      "1079 [D loss: 1.025266] [G loss: 1.000181]\n",
      "1080 [D loss: 1.022870] [G loss: 1.000120]\n",
      "1081 [D loss: 1.027332] [G loss: 1.000293]\n",
      "1082 [D loss: 1.026373] [G loss: 1.000180]\n",
      "1083 [D loss: 1.026643] [G loss: 1.000281]\n",
      "1084 [D loss: 1.025393] [G loss: 1.000108]\n",
      "1085 [D loss: 1.026232] [G loss: 1.000188]\n",
      "1086 [D loss: 1.026538] [G loss: 1.000198]\n",
      "1087 [D loss: 1.022043] [G loss: 1.000273]\n",
      "1088 [D loss: 1.025658] [G loss: 1.000246]\n",
      "1089 [D loss: 1.027351] [G loss: 1.000237]\n",
      "1090 [D loss: 1.022480] [G loss: 1.000217]\n",
      "1091 [D loss: 1.023176] [G loss: 1.000232]\n",
      "1092 [D loss: 1.027839] [G loss: 1.000278]\n",
      "1093 [D loss: 1.024112] [G loss: 1.000169]\n",
      "1094 [D loss: 1.024067] [G loss: 1.000263]\n",
      "1095 [D loss: 1.026536] [G loss: 1.000209]\n",
      "1096 [D loss: 1.027349] [G loss: 1.000252]\n",
      "1097 [D loss: 1.023979] [G loss: 1.000223]\n",
      "1098 [D loss: 1.024771] [G loss: 1.000163]\n",
      "1099 [D loss: 1.025262] [G loss: 1.000092]\n",
      "1100 [D loss: 1.026282] [G loss: 1.000113]\n",
      "1101 [D loss: 1.025163] [G loss: 1.000252]\n",
      "1102 [D loss: 1.025622] [G loss: 1.000249]\n",
      "1103 [D loss: 1.026876] [G loss: 1.000226]\n",
      "1104 [D loss: 1.028073] [G loss: 1.000239]\n",
      "1105 [D loss: 1.027318] [G loss: 1.000172]\n",
      "1106 [D loss: 1.026071] [G loss: 1.000201]\n",
      "1107 [D loss: 1.025691] [G loss: 1.000186]\n",
      "1108 [D loss: 1.026529] [G loss: 1.000225]\n",
      "1109 [D loss: 1.024272] [G loss: 1.000188]\n",
      "1110 [D loss: 1.023112] [G loss: 1.000093]\n",
      "1111 [D loss: 1.024685] [G loss: 1.000174]\n",
      "1112 [D loss: 1.020984] [G loss: 1.000190]\n",
      "1113 [D loss: 1.027484] [G loss: 1.000096]\n",
      "1114 [D loss: 1.024267] [G loss: 1.000196]\n",
      "1115 [D loss: 1.028115] [G loss: 1.000230]\n",
      "1116 [D loss: 1.026408] [G loss: 1.000183]\n",
      "1117 [D loss: 1.024590] [G loss: 1.000153]\n",
      "1118 [D loss: 1.027473] [G loss: 1.000169]\n",
      "1119 [D loss: 1.025583] [G loss: 1.000191]\n",
      "1120 [D loss: 1.023260] [G loss: 1.000138]\n",
      "1121 [D loss: 1.024609] [G loss: 1.000182]\n",
      "1122 [D loss: 1.025360] [G loss: 1.000148]\n",
      "1123 [D loss: 1.025769] [G loss: 1.000152]\n",
      "1124 [D loss: 1.023613] [G loss: 1.000168]\n",
      "1125 [D loss: 1.028165] [G loss: 1.000251]\n",
      "1126 [D loss: 1.023214] [G loss: 1.000208]\n",
      "1127 [D loss: 1.025275] [G loss: 1.000168]\n",
      "1128 [D loss: 1.024188] [G loss: 1.000163]\n",
      "1129 [D loss: 1.026318] [G loss: 1.000219]\n",
      "1130 [D loss: 1.024304] [G loss: 1.000214]\n",
      "1131 [D loss: 1.025365] [G loss: 1.000208]\n",
      "1132 [D loss: 1.024535] [G loss: 1.000208]\n",
      "1133 [D loss: 1.027028] [G loss: 1.000348]\n",
      "1134 [D loss: 1.024879] [G loss: 1.000134]\n",
      "1135 [D loss: 1.027250] [G loss: 1.000119]\n",
      "1136 [D loss: 1.022970] [G loss: 1.000152]\n",
      "1137 [D loss: 1.024266] [G loss: 1.000328]\n",
      "1138 [D loss: 1.026990] [G loss: 1.000229]\n",
      "1139 [D loss: 1.025757] [G loss: 1.000200]\n",
      "1140 [D loss: 1.026024] [G loss: 1.000150]\n",
      "1141 [D loss: 1.023205] [G loss: 1.000256]\n",
      "1142 [D loss: 1.021199] [G loss: 1.000144]\n",
      "1143 [D loss: 1.025656] [G loss: 1.000259]\n",
      "1144 [D loss: 1.024940] [G loss: 1.000296]\n",
      "1145 [D loss: 1.026048] [G loss: 1.000212]\n",
      "1146 [D loss: 1.027048] [G loss: 1.000103]\n",
      "1147 [D loss: 1.023990] [G loss: 1.000267]\n",
      "1148 [D loss: 1.025471] [G loss: 1.000262]\n",
      "1149 [D loss: 1.026482] [G loss: 1.000103]\n",
      "1150 [D loss: 1.025115] [G loss: 1.000285]\n",
      "1151 [D loss: 1.028065] [G loss: 1.000192]\n",
      "1152 [D loss: 1.026393] [G loss: 1.000224]\n",
      "1153 [D loss: 1.026990] [G loss: 1.000167]\n",
      "1154 [D loss: 1.026372] [G loss: 1.000211]\n",
      "1155 [D loss: 1.027878] [G loss: 1.000261]\n",
      "1156 [D loss: 1.026383] [G loss: 1.000209]\n",
      "1157 [D loss: 1.027144] [G loss: 1.000315]\n",
      "1158 [D loss: 1.025593] [G loss: 1.000258]\n",
      "1159 [D loss: 1.026518] [G loss: 1.000236]\n",
      "1160 [D loss: 1.025103] [G loss: 1.000272]\n",
      "1161 [D loss: 1.026124] [G loss: 1.000228]\n",
      "1162 [D loss: 1.029019] [G loss: 1.000203]\n",
      "1163 [D loss: 1.025224] [G loss: 1.000222]\n",
      "1164 [D loss: 1.023522] [G loss: 1.000200]\n",
      "1165 [D loss: 1.024404] [G loss: 1.000260]\n",
      "1166 [D loss: 1.023804] [G loss: 1.000255]\n",
      "1167 [D loss: 1.023859] [G loss: 1.000186]\n",
      "1168 [D loss: 1.026369] [G loss: 1.000257]\n",
      "1169 [D loss: 1.026611] [G loss: 1.000399]\n",
      "1170 [D loss: 1.024767] [G loss: 1.000284]\n",
      "1171 [D loss: 1.022628] [G loss: 1.000158]\n",
      "1172 [D loss: 1.026532] [G loss: 1.000183]\n",
      "1173 [D loss: 1.025883] [G loss: 1.000218]\n",
      "1174 [D loss: 1.026940] [G loss: 1.000223]\n",
      "1175 [D loss: 1.026046] [G loss: 1.000181]\n",
      "1176 [D loss: 1.025239] [G loss: 1.000201]\n",
      "1177 [D loss: 1.023581] [G loss: 1.000262]\n",
      "1178 [D loss: 1.026209] [G loss: 1.000294]\n",
      "1179 [D loss: 1.026720] [G loss: 1.000304]\n",
      "1180 [D loss: 1.024867] [G loss: 1.000232]\n",
      "1181 [D loss: 1.025739] [G loss: 1.000198]\n",
      "1182 [D loss: 1.023106] [G loss: 1.000333]\n",
      "1183 [D loss: 1.026557] [G loss: 1.000247]\n",
      "1184 [D loss: 1.027411] [G loss: 1.000198]\n",
      "1185 [D loss: 1.023163] [G loss: 1.000246]\n",
      "1186 [D loss: 1.026501] [G loss: 1.000232]\n",
      "1187 [D loss: 1.026960] [G loss: 1.000246]\n",
      "1188 [D loss: 1.027605] [G loss: 1.000361]\n",
      "1189 [D loss: 1.025324] [G loss: 1.000167]\n",
      "1190 [D loss: 1.025362] [G loss: 1.000219]\n",
      "1191 [D loss: 1.027926] [G loss: 1.000214]\n",
      "1192 [D loss: 1.027279] [G loss: 1.000245]\n",
      "1193 [D loss: 1.026021] [G loss: 1.000397]\n",
      "1194 [D loss: 1.023303] [G loss: 1.000246]\n",
      "1195 [D loss: 1.023551] [G loss: 1.000350]\n",
      "1196 [D loss: 1.021976] [G loss: 1.000355]\n",
      "1197 [D loss: 1.025107] [G loss: 1.000289]\n",
      "1198 [D loss: 1.023207] [G loss: 1.000265]\n",
      "1199 [D loss: 1.028389] [G loss: 1.000312]\n",
      "1200 [D loss: 1.024308] [G loss: 1.000303]\n",
      "1201 [D loss: 1.023159] [G loss: 1.000302]\n",
      "1202 [D loss: 1.025266] [G loss: 1.000223]\n",
      "1203 [D loss: 1.022867] [G loss: 1.000359]\n",
      "1204 [D loss: 1.025761] [G loss: 1.000314]\n",
      "1205 [D loss: 1.024973] [G loss: 1.000255]\n",
      "1206 [D loss: 1.026112] [G loss: 1.000355]\n",
      "1207 [D loss: 1.027253] [G loss: 1.000254]\n",
      "1208 [D loss: 1.025153] [G loss: 1.000338]\n",
      "1209 [D loss: 1.023136] [G loss: 1.000290]\n",
      "1210 [D loss: 1.026801] [G loss: 1.000222]\n",
      "1211 [D loss: 1.025577] [G loss: 1.000275]\n",
      "1212 [D loss: 1.023129] [G loss: 1.000368]\n",
      "1213 [D loss: 1.026861] [G loss: 1.000251]\n",
      "1214 [D loss: 1.025459] [G loss: 1.000234]\n",
      "1215 [D loss: 1.029247] [G loss: 1.000286]\n",
      "1216 [D loss: 1.026426] [G loss: 1.000181]\n",
      "1217 [D loss: 1.025541] [G loss: 1.000292]\n",
      "1218 [D loss: 1.024323] [G loss: 1.000184]\n",
      "1219 [D loss: 1.024607] [G loss: 1.000332]\n",
      "1220 [D loss: 1.025967] [G loss: 1.000385]\n",
      "1221 [D loss: 1.025727] [G loss: 1.000337]\n",
      "1222 [D loss: 1.023762] [G loss: 1.000196]\n",
      "1223 [D loss: 1.026098] [G loss: 1.000381]\n",
      "1224 [D loss: 1.027415] [G loss: 1.000345]\n",
      "1225 [D loss: 1.024337] [G loss: 1.000336]\n",
      "1226 [D loss: 1.023642] [G loss: 1.000271]\n",
      "1227 [D loss: 1.027002] [G loss: 1.000308]\n",
      "1228 [D loss: 1.027101] [G loss: 1.000312]\n",
      "1229 [D loss: 1.027092] [G loss: 1.000218]\n",
      "1230 [D loss: 1.023837] [G loss: 1.000279]\n",
      "1231 [D loss: 1.027473] [G loss: 1.000319]\n",
      "1232 [D loss: 1.026482] [G loss: 1.000327]\n",
      "1233 [D loss: 1.025729] [G loss: 1.000367]\n",
      "1234 [D loss: 1.023048] [G loss: 1.000350]\n",
      "1235 [D loss: 1.025591] [G loss: 1.000304]\n",
      "1236 [D loss: 1.021640] [G loss: 1.000246]\n",
      "1237 [D loss: 1.025993] [G loss: 1.000247]\n",
      "1238 [D loss: 1.025057] [G loss: 1.000331]\n",
      "1239 [D loss: 1.026227] [G loss: 1.000238]\n",
      "1240 [D loss: 1.024078] [G loss: 1.000297]\n",
      "1241 [D loss: 1.026397] [G loss: 1.000278]\n",
      "1242 [D loss: 1.026297] [G loss: 1.000325]\n",
      "1243 [D loss: 1.024963] [G loss: 1.000227]\n",
      "1244 [D loss: 1.027895] [G loss: 1.000337]\n",
      "1245 [D loss: 1.027528] [G loss: 1.000365]\n",
      "1246 [D loss: 1.025122] [G loss: 1.000308]\n",
      "1247 [D loss: 1.027398] [G loss: 1.000299]\n",
      "1248 [D loss: 1.026990] [G loss: 1.000364]\n",
      "1249 [D loss: 1.026726] [G loss: 1.000267]\n",
      "1250 [D loss: 1.025046] [G loss: 1.000372]\n",
      "1251 [D loss: 1.026764] [G loss: 1.000344]\n",
      "1252 [D loss: 1.026714] [G loss: 1.000417]\n",
      "1253 [D loss: 1.022909] [G loss: 1.000295]\n",
      "1254 [D loss: 1.023676] [G loss: 1.000277]\n",
      "1255 [D loss: 1.025158] [G loss: 1.000258]\n",
      "1256 [D loss: 1.026113] [G loss: 1.000278]\n",
      "1257 [D loss: 1.025566] [G loss: 1.000300]\n",
      "1258 [D loss: 1.027008] [G loss: 1.000334]\n",
      "1259 [D loss: 1.026573] [G loss: 1.000413]\n",
      "1260 [D loss: 1.025961] [G loss: 1.000237]\n",
      "1261 [D loss: 1.027633] [G loss: 1.000207]\n",
      "1262 [D loss: 1.026186] [G loss: 1.000197]\n",
      "1263 [D loss: 1.026100] [G loss: 1.000289]\n",
      "1264 [D loss: 1.027610] [G loss: 1.000222]\n",
      "1265 [D loss: 1.027267] [G loss: 1.000212]\n",
      "1266 [D loss: 1.025629] [G loss: 1.000357]\n",
      "1267 [D loss: 1.024757] [G loss: 1.000288]\n",
      "1268 [D loss: 1.025928] [G loss: 1.000245]\n",
      "1269 [D loss: 1.024670] [G loss: 1.000175]\n",
      "1270 [D loss: 1.023580] [G loss: 1.000292]\n",
      "1271 [D loss: 1.028250] [G loss: 1.000258]\n",
      "1272 [D loss: 1.026503] [G loss: 1.000318]\n",
      "1273 [D loss: 1.026024] [G loss: 1.000246]\n",
      "1274 [D loss: 1.026175] [G loss: 1.000164]\n",
      "1275 [D loss: 1.024489] [G loss: 1.000354]\n",
      "1276 [D loss: 1.025123] [G loss: 1.000263]\n",
      "1277 [D loss: 1.027163] [G loss: 1.000230]\n",
      "1278 [D loss: 1.023094] [G loss: 1.000225]\n",
      "1279 [D loss: 1.025689] [G loss: 1.000268]\n",
      "1280 [D loss: 1.025112] [G loss: 1.000241]\n",
      "1281 [D loss: 1.026647] [G loss: 1.000161]\n",
      "1282 [D loss: 1.026619] [G loss: 1.000258]\n",
      "1283 [D loss: 1.027162] [G loss: 1.000246]\n",
      "1284 [D loss: 1.024484] [G loss: 1.000224]\n",
      "1285 [D loss: 1.028382] [G loss: 1.000234]\n",
      "1286 [D loss: 1.024516] [G loss: 1.000235]\n",
      "1287 [D loss: 1.025245] [G loss: 1.000198]\n",
      "1288 [D loss: 1.026773] [G loss: 1.000357]\n",
      "1289 [D loss: 1.027899] [G loss: 1.000345]\n",
      "1290 [D loss: 1.027946] [G loss: 1.000234]\n",
      "1291 [D loss: 1.025868] [G loss: 1.000352]\n",
      "1292 [D loss: 1.027557] [G loss: 1.000184]\n",
      "1293 [D loss: 1.025015] [G loss: 1.000273]\n",
      "1294 [D loss: 1.022756] [G loss: 1.000277]\n",
      "1295 [D loss: 1.025957] [G loss: 1.000302]\n",
      "1296 [D loss: 1.023598] [G loss: 1.000108]\n",
      "1297 [D loss: 1.027586] [G loss: 1.000243]\n",
      "1298 [D loss: 1.028947] [G loss: 1.000263]\n",
      "1299 [D loss: 1.022525] [G loss: 1.000196]\n",
      "1300 [D loss: 1.025114] [G loss: 1.000234]\n",
      "1301 [D loss: 1.026286] [G loss: 1.000395]\n",
      "1302 [D loss: 1.028300] [G loss: 1.000288]\n",
      "1303 [D loss: 1.026193] [G loss: 1.000234]\n",
      "1304 [D loss: 1.027604] [G loss: 1.000237]\n",
      "1305 [D loss: 1.027436] [G loss: 1.000202]\n",
      "1306 [D loss: 1.026304] [G loss: 1.000124]\n",
      "1307 [D loss: 1.025155] [G loss: 1.000260]\n",
      "1308 [D loss: 1.025239] [G loss: 1.000190]\n",
      "1309 [D loss: 1.025935] [G loss: 1.000358]\n",
      "1310 [D loss: 1.025137] [G loss: 1.000265]\n",
      "1311 [D loss: 1.024705] [G loss: 1.000236]\n",
      "1312 [D loss: 1.025939] [G loss: 1.000318]\n",
      "1313 [D loss: 1.026256] [G loss: 1.000246]\n",
      "1314 [D loss: 1.026451] [G loss: 1.000222]\n",
      "1315 [D loss: 1.024044] [G loss: 1.000332]\n",
      "1316 [D loss: 1.026640] [G loss: 1.000269]\n",
      "1317 [D loss: 1.024118] [G loss: 1.000306]\n",
      "1318 [D loss: 1.027482] [G loss: 1.000251]\n",
      "1319 [D loss: 1.028509] [G loss: 1.000260]\n",
      "1320 [D loss: 1.027572] [G loss: 1.000235]\n",
      "1321 [D loss: 1.024229] [G loss: 1.000238]\n",
      "1322 [D loss: 1.025736] [G loss: 1.000202]\n",
      "1323 [D loss: 1.028549] [G loss: 1.000255]\n",
      "1324 [D loss: 1.026037] [G loss: 1.000202]\n",
      "1325 [D loss: 1.025556] [G loss: 1.000362]\n",
      "1326 [D loss: 1.025086] [G loss: 1.000307]\n",
      "1327 [D loss: 1.021628] [G loss: 1.000287]\n",
      "1328 [D loss: 1.022572] [G loss: 1.000334]\n",
      "1329 [D loss: 1.026032] [G loss: 1.000315]\n",
      "1330 [D loss: 1.023518] [G loss: 1.000253]\n",
      "1331 [D loss: 1.025333] [G loss: 1.000325]\n",
      "1332 [D loss: 1.024081] [G loss: 1.000243]\n",
      "1333 [D loss: 1.026664] [G loss: 1.000326]\n",
      "1334 [D loss: 1.028232] [G loss: 1.000365]\n",
      "1335 [D loss: 1.024966] [G loss: 1.000222]\n",
      "1336 [D loss: 1.029166] [G loss: 1.000294]\n",
      "1337 [D loss: 1.027688] [G loss: 1.000150]\n",
      "1338 [D loss: 1.025024] [G loss: 1.000167]\n",
      "1339 [D loss: 1.026420] [G loss: 1.000219]\n",
      "1340 [D loss: 1.026202] [G loss: 1.000183]\n",
      "1341 [D loss: 1.025979] [G loss: 1.000236]\n",
      "1342 [D loss: 1.024524] [G loss: 1.000212]\n",
      "1343 [D loss: 1.025985] [G loss: 1.000219]\n",
      "1344 [D loss: 1.024245] [G loss: 1.000284]\n",
      "1345 [D loss: 1.026840] [G loss: 1.000238]\n",
      "1346 [D loss: 1.026584] [G loss: 1.000300]\n",
      "1347 [D loss: 1.024921] [G loss: 1.000271]\n",
      "1348 [D loss: 1.025332] [G loss: 1.000279]\n",
      "1349 [D loss: 1.026414] [G loss: 1.000225]\n",
      "1350 [D loss: 1.025176] [G loss: 1.000135]\n",
      "1351 [D loss: 1.026647] [G loss: 1.000251]\n",
      "1352 [D loss: 1.027996] [G loss: 1.000211]\n",
      "1353 [D loss: 1.022116] [G loss: 1.000262]\n",
      "1354 [D loss: 1.027143] [G loss: 1.000300]\n",
      "1355 [D loss: 1.027180] [G loss: 1.000204]\n",
      "1356 [D loss: 1.024395] [G loss: 1.000298]\n",
      "1357 [D loss: 1.025009] [G loss: 1.000240]\n",
      "1358 [D loss: 1.024667] [G loss: 1.000324]\n",
      "1359 [D loss: 1.026830] [G loss: 1.000190]\n",
      "1360 [D loss: 1.027777] [G loss: 1.000290]\n",
      "1361 [D loss: 1.024598] [G loss: 1.000245]\n",
      "1362 [D loss: 1.025797] [G loss: 1.000338]\n",
      "1363 [D loss: 1.024683] [G loss: 1.000250]\n",
      "1364 [D loss: 1.024451] [G loss: 1.000185]\n",
      "1365 [D loss: 1.027413] [G loss: 1.000212]\n",
      "1366 [D loss: 1.024485] [G loss: 1.000295]\n",
      "1367 [D loss: 1.026271] [G loss: 1.000298]\n",
      "1368 [D loss: 1.025379] [G loss: 1.000314]\n",
      "1369 [D loss: 1.027041] [G loss: 1.000212]\n",
      "1370 [D loss: 1.027446] [G loss: 1.000239]\n",
      "1371 [D loss: 1.028035] [G loss: 1.000184]\n",
      "1372 [D loss: 1.026748] [G loss: 1.000235]\n",
      "1373 [D loss: 1.026590] [G loss: 1.000269]\n",
      "1374 [D loss: 1.025158] [G loss: 1.000246]\n",
      "1375 [D loss: 1.026901] [G loss: 1.000292]\n",
      "1376 [D loss: 1.025265] [G loss: 1.000200]\n",
      "1377 [D loss: 1.024640] [G loss: 1.000271]\n",
      "1378 [D loss: 1.025586] [G loss: 1.000027]\n",
      "1379 [D loss: 1.028518] [G loss: 1.000176]\n",
      "1380 [D loss: 1.026724] [G loss: 1.000217]\n",
      "1381 [D loss: 1.025442] [G loss: 1.000281]\n",
      "1382 [D loss: 1.029041] [G loss: 1.000268]\n",
      "1383 [D loss: 1.023843] [G loss: 1.000305]\n",
      "1384 [D loss: 1.024198] [G loss: 1.000207]\n",
      "1385 [D loss: 1.027156] [G loss: 1.000223]\n",
      "1386 [D loss: 1.026464] [G loss: 1.000119]\n",
      "1387 [D loss: 1.026473] [G loss: 1.000166]\n",
      "1388 [D loss: 1.023574] [G loss: 1.000192]\n",
      "1389 [D loss: 1.026280] [G loss: 1.000234]\n",
      "1390 [D loss: 1.023363] [G loss: 1.000227]\n",
      "1391 [D loss: 1.025036] [G loss: 1.000246]\n",
      "1392 [D loss: 1.024058] [G loss: 1.000228]\n",
      "1393 [D loss: 1.025781] [G loss: 1.000211]\n",
      "1394 [D loss: 1.024108] [G loss: 1.000171]\n",
      "1395 [D loss: 1.026087] [G loss: 1.000205]\n",
      "1396 [D loss: 1.025511] [G loss: 1.000202]\n",
      "1397 [D loss: 1.027047] [G loss: 1.000289]\n",
      "1398 [D loss: 1.028485] [G loss: 1.000226]\n",
      "1399 [D loss: 1.027138] [G loss: 1.000292]\n",
      "1400 [D loss: 1.026385] [G loss: 1.000210]\n",
      "1401 [D loss: 1.027330] [G loss: 1.000263]\n",
      "1402 [D loss: 1.028087] [G loss: 1.000186]\n",
      "1403 [D loss: 1.028308] [G loss: 1.000258]\n",
      "1404 [D loss: 1.025662] [G loss: 1.000162]\n",
      "1405 [D loss: 1.026094] [G loss: 1.000213]\n",
      "1406 [D loss: 1.025042] [G loss: 1.000287]\n",
      "1407 [D loss: 1.025892] [G loss: 1.000201]\n",
      "1408 [D loss: 1.026482] [G loss: 1.000314]\n",
      "1409 [D loss: 1.025061] [G loss: 1.000167]\n",
      "1410 [D loss: 1.027413] [G loss: 1.000181]\n",
      "1411 [D loss: 1.025036] [G loss: 1.000192]\n",
      "1412 [D loss: 1.023563] [G loss: 1.000223]\n",
      "1413 [D loss: 1.024671] [G loss: 1.000122]\n",
      "1414 [D loss: 1.026692] [G loss: 1.000290]\n",
      "1415 [D loss: 1.026275] [G loss: 1.000286]\n",
      "1416 [D loss: 1.026730] [G loss: 1.000203]\n",
      "1417 [D loss: 1.026204] [G loss: 1.000234]\n",
      "1418 [D loss: 1.026356] [G loss: 1.000122]\n",
      "1419 [D loss: 1.025719] [G loss: 1.000158]\n",
      "1420 [D loss: 1.022404] [G loss: 1.000147]\n",
      "1421 [D loss: 1.024970] [G loss: 1.000270]\n",
      "1422 [D loss: 1.025278] [G loss: 1.000204]\n",
      "1423 [D loss: 1.024248] [G loss: 1.000270]\n",
      "1424 [D loss: 1.023521] [G loss: 1.000240]\n",
      "1425 [D loss: 1.027704] [G loss: 1.000247]\n",
      "1426 [D loss: 1.025066] [G loss: 1.000208]\n",
      "1427 [D loss: 1.023458] [G loss: 1.000206]\n",
      "1428 [D loss: 1.027804] [G loss: 1.000218]\n",
      "1429 [D loss: 1.022147] [G loss: 1.000237]\n",
      "1430 [D loss: 1.026860] [G loss: 1.000179]\n",
      "1431 [D loss: 1.029062] [G loss: 1.000151]\n",
      "1432 [D loss: 1.028487] [G loss: 1.000087]\n",
      "1433 [D loss: 1.027452] [G loss: 1.000278]\n",
      "1434 [D loss: 1.026316] [G loss: 1.000232]\n",
      "1435 [D loss: 1.024039] [G loss: 1.000160]\n",
      "1436 [D loss: 1.025458] [G loss: 1.000165]\n",
      "1437 [D loss: 1.022899] [G loss: 1.000296]\n",
      "1438 [D loss: 1.025472] [G loss: 1.000255]\n",
      "1439 [D loss: 1.028243] [G loss: 1.000211]\n",
      "1440 [D loss: 1.023938] [G loss: 1.000143]\n",
      "1441 [D loss: 1.026096] [G loss: 1.000142]\n",
      "1442 [D loss: 1.027603] [G loss: 1.000094]\n",
      "1443 [D loss: 1.027228] [G loss: 1.000236]\n",
      "1444 [D loss: 1.027545] [G loss: 1.000138]\n",
      "1445 [D loss: 1.025976] [G loss: 1.000091]\n",
      "1446 [D loss: 1.023378] [G loss: 1.000245]\n",
      "1447 [D loss: 1.024180] [G loss: 1.000178]\n",
      "1448 [D loss: 1.025970] [G loss: 1.000159]\n",
      "1449 [D loss: 1.024946] [G loss: 1.000156]\n",
      "1450 [D loss: 1.027685] [G loss: 1.000154]\n",
      "1451 [D loss: 1.027831] [G loss: 1.000213]\n",
      "1452 [D loss: 1.023623] [G loss: 1.000062]\n",
      "1453 [D loss: 1.025366] [G loss: 1.000157]\n",
      "1454 [D loss: 1.024896] [G loss: 1.000116]\n",
      "1455 [D loss: 1.026202] [G loss: 1.000177]\n",
      "1456 [D loss: 1.027081] [G loss: 1.000084]\n",
      "1457 [D loss: 1.024911] [G loss: 1.000152]\n",
      "1458 [D loss: 1.025042] [G loss: 1.000152]\n",
      "1459 [D loss: 1.024497] [G loss: 1.000175]\n",
      "1460 [D loss: 1.024679] [G loss: 1.000129]\n",
      "1461 [D loss: 1.026668] [G loss: 1.000114]\n",
      "1462 [D loss: 1.025292] [G loss: 1.000189]\n",
      "1463 [D loss: 1.024146] [G loss: 1.000177]\n",
      "1464 [D loss: 1.025025] [G loss: 1.000250]\n",
      "1465 [D loss: 1.026651] [G loss: 1.000228]\n",
      "1466 [D loss: 1.026863] [G loss: 1.000106]\n",
      "1467 [D loss: 1.027301] [G loss: 1.000197]\n",
      "1468 [D loss: 1.026880] [G loss: 1.000186]\n",
      "1469 [D loss: 1.023559] [G loss: 1.000237]\n",
      "1470 [D loss: 1.022900] [G loss: 1.000225]\n",
      "1471 [D loss: 1.024831] [G loss: 1.000190]\n",
      "1472 [D loss: 1.023650] [G loss: 1.000207]\n",
      "1473 [D loss: 1.025102] [G loss: 1.000209]\n",
      "1474 [D loss: 1.025419] [G loss: 1.000208]\n",
      "1475 [D loss: 1.025295] [G loss: 1.000211]\n",
      "1476 [D loss: 1.028870] [G loss: 1.000172]\n",
      "1477 [D loss: 1.025536] [G loss: 1.000180]\n",
      "1478 [D loss: 1.025369] [G loss: 1.000208]\n",
      "1479 [D loss: 1.025646] [G loss: 1.000147]\n",
      "1480 [D loss: 1.024679] [G loss: 1.000207]\n",
      "1481 [D loss: 1.026359] [G loss: 1.000127]\n",
      "1482 [D loss: 1.024794] [G loss: 1.000179]\n",
      "1483 [D loss: 1.026515] [G loss: 1.000047]\n",
      "1484 [D loss: 1.029346] [G loss: 1.000062]\n",
      "1485 [D loss: 1.025788] [G loss: 1.000251]\n",
      "1486 [D loss: 1.027220] [G loss: 1.000167]\n",
      "1487 [D loss: 1.024822] [G loss: 1.000135]\n",
      "1488 [D loss: 1.024214] [G loss: 1.000171]\n",
      "1489 [D loss: 1.027503] [G loss: 1.000157]\n",
      "1490 [D loss: 1.026454] [G loss: 1.000233]\n",
      "1491 [D loss: 1.025005] [G loss: 1.000197]\n",
      "1492 [D loss: 1.026187] [G loss: 1.000153]\n",
      "1493 [D loss: 1.024228] [G loss: 1.000246]\n",
      "1494 [D loss: 1.027557] [G loss: 1.000249]\n",
      "1495 [D loss: 1.026361] [G loss: 1.000187]\n",
      "1496 [D loss: 1.024871] [G loss: 1.000179]\n",
      "1497 [D loss: 1.026319] [G loss: 1.000121]\n",
      "1498 [D loss: 1.024454] [G loss: 1.000082]\n",
      "1499 [D loss: 1.023452] [G loss: 1.000316]\n",
      "1500 [D loss: 1.025855] [G loss: 1.000144]\n",
      "1501 [D loss: 1.025420] [G loss: 1.000187]\n",
      "1502 [D loss: 1.028758] [G loss: 1.000222]\n",
      "1503 [D loss: 1.026481] [G loss: 1.000196]\n",
      "1504 [D loss: 1.023759] [G loss: 1.000235]\n",
      "1505 [D loss: 1.023136] [G loss: 1.000203]\n",
      "1506 [D loss: 1.026173] [G loss: 1.000234]\n",
      "1507 [D loss: 1.025575] [G loss: 1.000295]\n",
      "1508 [D loss: 1.025319] [G loss: 1.000175]\n",
      "1509 [D loss: 1.024864] [G loss: 1.000227]\n",
      "1510 [D loss: 1.026171] [G loss: 1.000198]\n",
      "1511 [D loss: 1.026264] [G loss: 1.000164]\n",
      "1512 [D loss: 1.021699] [G loss: 1.000239]\n",
      "1513 [D loss: 1.025954] [G loss: 1.000171]\n",
      "1514 [D loss: 1.027344] [G loss: 1.000136]\n",
      "1515 [D loss: 1.023730] [G loss: 1.000152]\n",
      "1516 [D loss: 1.026092] [G loss: 1.000118]\n",
      "1517 [D loss: 1.025997] [G loss: 1.000243]\n",
      "1518 [D loss: 1.028746] [G loss: 1.000228]\n",
      "1519 [D loss: 1.024759] [G loss: 1.000135]\n",
      "1520 [D loss: 1.025687] [G loss: 1.000167]\n",
      "1521 [D loss: 1.028080] [G loss: 1.000231]\n",
      "1522 [D loss: 1.020632] [G loss: 1.000228]\n",
      "1523 [D loss: 1.027724] [G loss: 1.000244]\n",
      "1524 [D loss: 1.025265] [G loss: 1.000246]\n",
      "1525 [D loss: 1.024227] [G loss: 1.000225]\n",
      "1526 [D loss: 1.026640] [G loss: 1.000107]\n",
      "1527 [D loss: 1.025811] [G loss: 1.000128]\n",
      "1528 [D loss: 1.024706] [G loss: 1.000212]\n",
      "1529 [D loss: 1.025696] [G loss: 1.000270]\n",
      "1530 [D loss: 1.025536] [G loss: 1.000248]\n",
      "1531 [D loss: 1.025475] [G loss: 1.000269]\n",
      "1532 [D loss: 1.023276] [G loss: 1.000188]\n",
      "1533 [D loss: 1.028059] [G loss: 1.000146]\n",
      "1534 [D loss: 1.024714] [G loss: 1.000274]\n",
      "1535 [D loss: 1.026169] [G loss: 1.000192]\n",
      "1536 [D loss: 1.025921] [G loss: 1.000144]\n",
      "1537 [D loss: 1.027910] [G loss: 1.000188]\n",
      "1538 [D loss: 1.026089] [G loss: 1.000171]\n",
      "1539 [D loss: 1.023075] [G loss: 1.000283]\n",
      "1540 [D loss: 1.025002] [G loss: 1.000229]\n",
      "1541 [D loss: 1.023842] [G loss: 1.000147]\n",
      "1542 [D loss: 1.028059] [G loss: 1.000258]\n",
      "1543 [D loss: 1.026319] [G loss: 1.000350]\n",
      "1544 [D loss: 1.026315] [G loss: 1.000198]\n",
      "1545 [D loss: 1.026783] [G loss: 1.000290]\n",
      "1546 [D loss: 1.025195] [G loss: 1.000179]\n",
      "1547 [D loss: 1.025702] [G loss: 1.000234]\n",
      "1548 [D loss: 1.022859] [G loss: 1.000252]\n",
      "1549 [D loss: 1.027838] [G loss: 1.000116]\n",
      "1550 [D loss: 1.026083] [G loss: 1.000189]\n",
      "1551 [D loss: 1.027294] [G loss: 1.000145]\n",
      "1552 [D loss: 1.026279] [G loss: 1.000195]\n",
      "1553 [D loss: 1.027120] [G loss: 1.000185]\n",
      "1554 [D loss: 1.026915] [G loss: 1.000172]\n",
      "1555 [D loss: 1.028333] [G loss: 1.000219]\n",
      "1556 [D loss: 1.025246] [G loss: 1.000325]\n",
      "1557 [D loss: 1.021745] [G loss: 1.000089]\n",
      "1558 [D loss: 1.023678] [G loss: 1.000217]\n",
      "1559 [D loss: 1.026061] [G loss: 1.000196]\n",
      "1560 [D loss: 1.022465] [G loss: 1.000205]\n",
      "1561 [D loss: 1.022605] [G loss: 1.000120]\n",
      "1562 [D loss: 1.024423] [G loss: 1.000307]\n",
      "1563 [D loss: 1.022927] [G loss: 1.000221]\n",
      "1564 [D loss: 1.024002] [G loss: 1.000142]\n",
      "1565 [D loss: 1.025500] [G loss: 1.000309]\n",
      "1566 [D loss: 1.022721] [G loss: 1.000465]\n",
      "1567 [D loss: 1.024091] [G loss: 1.000200]\n",
      "1568 [D loss: 1.026213] [G loss: 1.000214]\n",
      "1569 [D loss: 1.025460] [G loss: 1.000188]\n",
      "1570 [D loss: 1.027326] [G loss: 1.000261]\n",
      "1571 [D loss: 1.025897] [G loss: 1.000301]\n",
      "1572 [D loss: 1.026018] [G loss: 1.000293]\n",
      "1573 [D loss: 1.026904] [G loss: 1.000214]\n",
      "1574 [D loss: 1.027015] [G loss: 1.000243]\n",
      "1575 [D loss: 1.026439] [G loss: 1.000179]\n",
      "1576 [D loss: 1.027778] [G loss: 1.000271]\n",
      "1577 [D loss: 1.024786] [G loss: 1.000281]\n",
      "1578 [D loss: 1.024237] [G loss: 1.000237]\n",
      "1579 [D loss: 1.027547] [G loss: 1.000257]\n",
      "1580 [D loss: 1.027197] [G loss: 1.000200]\n",
      "1581 [D loss: 1.026637] [G loss: 1.000225]\n",
      "1582 [D loss: 1.026998] [G loss: 1.000168]\n",
      "1583 [D loss: 1.025795] [G loss: 1.000234]\n",
      "1584 [D loss: 1.023663] [G loss: 1.000246]\n",
      "1585 [D loss: 1.026709] [G loss: 1.000310]\n",
      "1586 [D loss: 1.027141] [G loss: 1.000223]\n",
      "1587 [D loss: 1.025185] [G loss: 1.000191]\n",
      "1588 [D loss: 1.027547] [G loss: 1.000255]\n",
      "1589 [D loss: 1.021823] [G loss: 1.000241]\n",
      "1590 [D loss: 1.024161] [G loss: 1.000248]\n",
      "1591 [D loss: 1.026081] [G loss: 1.000158]\n",
      "1592 [D loss: 1.026495] [G loss: 1.000192]\n",
      "1593 [D loss: 1.023554] [G loss: 1.000268]\n",
      "1594 [D loss: 1.025059] [G loss: 1.000225]\n",
      "1595 [D loss: 1.028821] [G loss: 1.000236]\n",
      "1596 [D loss: 1.025083] [G loss: 1.000223]\n",
      "1597 [D loss: 1.026498] [G loss: 1.000261]\n",
      "1598 [D loss: 1.026009] [G loss: 1.000185]\n",
      "1599 [D loss: 1.026926] [G loss: 1.000185]\n",
      "1600 [D loss: 1.026807] [G loss: 1.000234]\n",
      "1601 [D loss: 1.024576] [G loss: 1.000242]\n",
      "1602 [D loss: 1.025554] [G loss: 1.000209]\n",
      "1603 [D loss: 1.026816] [G loss: 1.000240]\n",
      "1604 [D loss: 1.021926] [G loss: 1.000256]\n",
      "1605 [D loss: 1.025893] [G loss: 1.000319]\n",
      "1606 [D loss: 1.025728] [G loss: 1.000202]\n",
      "1607 [D loss: 1.025664] [G loss: 1.000266]\n",
      "1608 [D loss: 1.024994] [G loss: 1.000212]\n",
      "1609 [D loss: 1.026372] [G loss: 1.000240]\n",
      "1610 [D loss: 1.024487] [G loss: 1.000244]\n",
      "1611 [D loss: 1.024909] [G loss: 1.000177]\n",
      "1612 [D loss: 1.024541] [G loss: 1.000332]\n",
      "1613 [D loss: 1.025101] [G loss: 1.000179]\n",
      "1614 [D loss: 1.028282] [G loss: 1.000232]\n",
      "1615 [D loss: 1.022986] [G loss: 1.000231]\n",
      "1616 [D loss: 1.027906] [G loss: 1.000220]\n",
      "1617 [D loss: 1.023598] [G loss: 1.000185]\n",
      "1618 [D loss: 1.025234] [G loss: 1.000130]\n",
      "1619 [D loss: 1.025499] [G loss: 1.000157]\n",
      "1620 [D loss: 1.026748] [G loss: 1.000195]\n",
      "1621 [D loss: 1.027167] [G loss: 1.000284]\n",
      "1622 [D loss: 1.025563] [G loss: 1.000290]\n",
      "1623 [D loss: 1.026763] [G loss: 1.000233]\n",
      "1624 [D loss: 1.024552] [G loss: 1.000160]\n",
      "1625 [D loss: 1.026581] [G loss: 1.000193]\n",
      "1626 [D loss: 1.027573] [G loss: 1.000283]\n",
      "1627 [D loss: 1.027538] [G loss: 1.000264]\n",
      "1628 [D loss: 1.028299] [G loss: 1.000153]\n",
      "1629 [D loss: 1.026037] [G loss: 1.000192]\n",
      "1630 [D loss: 1.028663] [G loss: 1.000216]\n",
      "1631 [D loss: 1.024443] [G loss: 1.000188]\n",
      "1632 [D loss: 1.024745] [G loss: 1.000142]\n",
      "1633 [D loss: 1.024519] [G loss: 1.000276]\n",
      "1634 [D loss: 1.026262] [G loss: 1.000269]\n",
      "1635 [D loss: 1.025634] [G loss: 1.000273]\n",
      "1636 [D loss: 1.026171] [G loss: 1.000215]\n",
      "1637 [D loss: 1.028375] [G loss: 1.000214]\n",
      "1638 [D loss: 1.024586] [G loss: 1.000245]\n",
      "1639 [D loss: 1.024701] [G loss: 1.000174]\n",
      "1640 [D loss: 1.027726] [G loss: 1.000213]\n",
      "1641 [D loss: 1.028454] [G loss: 1.000197]\n",
      "1642 [D loss: 1.028171] [G loss: 1.000147]\n",
      "1643 [D loss: 1.023489] [G loss: 1.000176]\n",
      "1644 [D loss: 1.027116] [G loss: 1.000181]\n",
      "1645 [D loss: 1.026700] [G loss: 1.000218]\n",
      "1646 [D loss: 1.026774] [G loss: 1.000238]\n",
      "1647 [D loss: 1.023949] [G loss: 1.000362]\n",
      "1648 [D loss: 1.026821] [G loss: 1.000189]\n",
      "1649 [D loss: 1.027538] [G loss: 1.000254]\n",
      "1650 [D loss: 1.028822] [G loss: 1.000292]\n",
      "1651 [D loss: 1.027319] [G loss: 1.000244]\n",
      "1652 [D loss: 1.026891] [G loss: 1.000277]\n",
      "1653 [D loss: 1.026764] [G loss: 1.000223]\n",
      "1654 [D loss: 1.025862] [G loss: 1.000192]\n",
      "1655 [D loss: 1.026756] [G loss: 1.000152]\n",
      "1656 [D loss: 1.027494] [G loss: 1.000231]\n",
      "1657 [D loss: 1.026356] [G loss: 1.000212]\n",
      "1658 [D loss: 1.026877] [G loss: 1.000194]\n",
      "1659 [D loss: 1.024475] [G loss: 1.000230]\n",
      "1660 [D loss: 1.025366] [G loss: 1.000293]\n",
      "1661 [D loss: 1.028824] [G loss: 1.000227]\n",
      "1662 [D loss: 1.026035] [G loss: 1.000312]\n",
      "1663 [D loss: 1.026719] [G loss: 1.000132]\n",
      "1664 [D loss: 1.023438] [G loss: 1.000284]\n",
      "1665 [D loss: 1.024904] [G loss: 1.000149]\n",
      "1666 [D loss: 1.023734] [G loss: 1.000301]\n",
      "1667 [D loss: 1.024455] [G loss: 1.000312]\n",
      "1668 [D loss: 1.027996] [G loss: 1.000300]\n",
      "1669 [D loss: 1.025558] [G loss: 1.000259]\n",
      "1670 [D loss: 1.026521] [G loss: 1.000211]\n",
      "1671 [D loss: 1.027247] [G loss: 1.000300]\n",
      "1672 [D loss: 1.028201] [G loss: 1.000307]\n",
      "1673 [D loss: 1.027496] [G loss: 1.000276]\n",
      "1674 [D loss: 1.026008] [G loss: 1.000218]\n",
      "1675 [D loss: 1.027410] [G loss: 1.000199]\n",
      "1676 [D loss: 1.023643] [G loss: 1.000151]\n",
      "1677 [D loss: 1.027285] [G loss: 1.000337]\n",
      "1678 [D loss: 1.025898] [G loss: 1.000226]\n",
      "1679 [D loss: 1.023677] [G loss: 1.000132]\n",
      "1680 [D loss: 1.023882] [G loss: 1.000255]\n",
      "1681 [D loss: 1.024503] [G loss: 1.000184]\n",
      "1682 [D loss: 1.026837] [G loss: 1.000292]\n",
      "1683 [D loss: 1.027830] [G loss: 1.000248]\n",
      "1684 [D loss: 1.026879] [G loss: 1.000281]\n",
      "1685 [D loss: 1.028433] [G loss: 1.000286]\n",
      "1686 [D loss: 1.026342] [G loss: 1.000170]\n",
      "1687 [D loss: 1.027869] [G loss: 1.000168]\n",
      "1688 [D loss: 1.025846] [G loss: 1.000198]\n",
      "1689 [D loss: 1.025501] [G loss: 1.000260]\n",
      "1690 [D loss: 1.026811] [G loss: 1.000226]\n",
      "1691 [D loss: 1.026485] [G loss: 1.000249]\n",
      "1692 [D loss: 1.025502] [G loss: 1.000363]\n",
      "1693 [D loss: 1.024542] [G loss: 1.000301]\n",
      "1694 [D loss: 1.023525] [G loss: 1.000196]\n",
      "1695 [D loss: 1.025298] [G loss: 1.000293]\n",
      "1696 [D loss: 1.024671] [G loss: 1.000265]\n",
      "1697 [D loss: 1.026055] [G loss: 1.000329]\n",
      "1698 [D loss: 1.025142] [G loss: 1.000342]\n",
      "1699 [D loss: 1.026660] [G loss: 1.000191]\n",
      "1700 [D loss: 1.026333] [G loss: 1.000135]\n",
      "1701 [D loss: 1.024980] [G loss: 1.000177]\n",
      "1702 [D loss: 1.025351] [G loss: 1.000230]\n",
      "1703 [D loss: 1.027342] [G loss: 1.000261]\n",
      "1704 [D loss: 1.026692] [G loss: 1.000286]\n",
      "1705 [D loss: 1.025225] [G loss: 1.000259]\n",
      "1706 [D loss: 1.025160] [G loss: 1.000269]\n",
      "1707 [D loss: 1.025326] [G loss: 1.000173]\n",
      "1708 [D loss: 1.022013] [G loss: 1.000192]\n",
      "1709 [D loss: 1.026375] [G loss: 1.000193]\n",
      "1710 [D loss: 1.025138] [G loss: 1.000252]\n",
      "1711 [D loss: 1.026647] [G loss: 1.000138]\n",
      "1712 [D loss: 1.027706] [G loss: 1.000267]\n",
      "1713 [D loss: 1.024000] [G loss: 1.000181]\n",
      "1714 [D loss: 1.025587] [G loss: 1.000288]\n",
      "1715 [D loss: 1.025162] [G loss: 1.000280]\n",
      "1716 [D loss: 1.027021] [G loss: 1.000363]\n",
      "1717 [D loss: 1.025595] [G loss: 1.000307]\n",
      "1718 [D loss: 1.025971] [G loss: 1.000330]\n",
      "1719 [D loss: 1.022885] [G loss: 1.000208]\n",
      "1720 [D loss: 1.027123] [G loss: 1.000137]\n",
      "1721 [D loss: 1.025932] [G loss: 1.000163]\n",
      "1722 [D loss: 1.026899] [G loss: 1.000268]\n",
      "1723 [D loss: 1.023504] [G loss: 1.000243]\n",
      "1724 [D loss: 1.027887] [G loss: 1.000337]\n",
      "1725 [D loss: 1.026338] [G loss: 1.000252]\n",
      "1726 [D loss: 1.026448] [G loss: 1.000303]\n",
      "1727 [D loss: 1.027833] [G loss: 1.000341]\n",
      "1728 [D loss: 1.025990] [G loss: 1.000160]\n",
      "1729 [D loss: 1.023248] [G loss: 1.000263]\n",
      "1730 [D loss: 1.027059] [G loss: 1.000186]\n",
      "1731 [D loss: 1.025887] [G loss: 1.000215]\n",
      "1732 [D loss: 1.024164] [G loss: 1.000239]\n",
      "1733 [D loss: 1.029532] [G loss: 1.000342]\n",
      "1734 [D loss: 1.024649] [G loss: 1.000207]\n",
      "1735 [D loss: 1.025747] [G loss: 1.000183]\n",
      "1736 [D loss: 1.026563] [G loss: 1.000201]\n",
      "1737 [D loss: 1.027878] [G loss: 1.000194]\n",
      "1738 [D loss: 1.020302] [G loss: 1.000333]\n",
      "1739 [D loss: 1.027863] [G loss: 1.000202]\n",
      "1740 [D loss: 1.023725] [G loss: 1.000245]\n",
      "1741 [D loss: 1.023945] [G loss: 1.000274]\n",
      "1742 [D loss: 1.025688] [G loss: 1.000228]\n",
      "1743 [D loss: 1.025677] [G loss: 1.000223]\n",
      "1744 [D loss: 1.025714] [G loss: 1.000365]\n",
      "1745 [D loss: 1.027803] [G loss: 1.000126]\n",
      "1746 [D loss: 1.025574] [G loss: 1.000302]\n",
      "1747 [D loss: 1.026036] [G loss: 1.000178]\n",
      "1748 [D loss: 1.023445] [G loss: 1.000184]\n",
      "1749 [D loss: 1.026272] [G loss: 1.000076]\n",
      "1750 [D loss: 1.028361] [G loss: 1.000164]\n",
      "1751 [D loss: 1.026156] [G loss: 1.000126]\n",
      "1752 [D loss: 1.024462] [G loss: 1.000217]\n",
      "1753 [D loss: 1.027252] [G loss: 1.000208]\n",
      "1754 [D loss: 1.024707] [G loss: 1.000107]\n",
      "1755 [D loss: 1.025746] [G loss: 1.000154]\n",
      "1756 [D loss: 1.025636] [G loss: 1.000190]\n",
      "1757 [D loss: 1.023333] [G loss: 1.000196]\n",
      "1758 [D loss: 1.026623] [G loss: 1.000175]\n",
      "1759 [D loss: 1.026520] [G loss: 1.000231]\n",
      "1760 [D loss: 1.028286] [G loss: 1.000344]\n",
      "1761 [D loss: 1.027980] [G loss: 1.000223]\n",
      "1762 [D loss: 1.026245] [G loss: 1.000304]\n",
      "1763 [D loss: 1.027898] [G loss: 1.000144]\n",
      "1764 [D loss: 1.026820] [G loss: 1.000185]\n",
      "1765 [D loss: 1.025973] [G loss: 1.000253]\n",
      "1766 [D loss: 1.018870] [G loss: 1.000216]\n",
      "1767 [D loss: 1.027308] [G loss: 1.000204]\n",
      "1768 [D loss: 1.029248] [G loss: 1.000283]\n",
      "1769 [D loss: 1.026829] [G loss: 1.000092]\n",
      "1770 [D loss: 1.026017] [G loss: 1.000210]\n",
      "1771 [D loss: 1.024902] [G loss: 1.000145]\n",
      "1772 [D loss: 1.027673] [G loss: 1.000078]\n",
      "1773 [D loss: 1.026435] [G loss: 1.000262]\n",
      "1774 [D loss: 1.026824] [G loss: 1.000111]\n",
      "1775 [D loss: 1.024275] [G loss: 1.000250]\n",
      "1776 [D loss: 1.026546] [G loss: 1.000288]\n",
      "1777 [D loss: 1.025790] [G loss: 1.000190]\n",
      "1778 [D loss: 1.026927] [G loss: 1.000112]\n",
      "1779 [D loss: 1.027936] [G loss: 1.000301]\n",
      "1780 [D loss: 1.024924] [G loss: 1.000239]\n",
      "1781 [D loss: 1.027411] [G loss: 1.000293]\n",
      "1782 [D loss: 1.027635] [G loss: 1.000091]\n",
      "1783 [D loss: 1.027504] [G loss: 1.000179]\n",
      "1784 [D loss: 1.026441] [G loss: 1.000245]\n",
      "1785 [D loss: 1.026752] [G loss: 1.000252]\n",
      "1786 [D loss: 1.029426] [G loss: 1.000294]\n",
      "1787 [D loss: 1.028178] [G loss: 1.000201]\n",
      "1788 [D loss: 1.024874] [G loss: 1.000263]\n",
      "1789 [D loss: 1.026658] [G loss: 1.000167]\n",
      "1790 [D loss: 1.026477] [G loss: 1.000343]\n",
      "1791 [D loss: 1.026766] [G loss: 1.000137]\n",
      "1792 [D loss: 1.029274] [G loss: 1.000179]\n",
      "1793 [D loss: 1.026034] [G loss: 1.000239]\n",
      "1794 [D loss: 1.026747] [G loss: 1.000330]\n",
      "1795 [D loss: 1.028147] [G loss: 1.000248]\n",
      "1796 [D loss: 1.023775] [G loss: 1.000272]\n",
      "1797 [D loss: 1.028209] [G loss: 1.000173]\n",
      "1798 [D loss: 1.024667] [G loss: 1.000304]\n",
      "1799 [D loss: 1.027121] [G loss: 1.000207]\n",
      "1800 [D loss: 1.025726] [G loss: 1.000220]\n",
      "1801 [D loss: 1.024122] [G loss: 1.000267]\n",
      "1802 [D loss: 1.026479] [G loss: 1.000216]\n",
      "1803 [D loss: 1.024751] [G loss: 1.000165]\n",
      "1804 [D loss: 1.028710] [G loss: 1.000193]\n",
      "1805 [D loss: 1.026120] [G loss: 1.000223]\n",
      "1806 [D loss: 1.024221] [G loss: 1.000098]\n",
      "1807 [D loss: 1.025328] [G loss: 1.000201]\n",
      "1808 [D loss: 1.024369] [G loss: 1.000183]\n",
      "1809 [D loss: 1.027946] [G loss: 1.000192]\n",
      "1810 [D loss: 1.026214] [G loss: 1.000094]\n",
      "1811 [D loss: 1.027992] [G loss: 1.000160]\n",
      "1812 [D loss: 1.027106] [G loss: 1.000213]\n",
      "1813 [D loss: 1.026740] [G loss: 1.000161]\n",
      "1814 [D loss: 1.024410] [G loss: 1.000133]\n",
      "1815 [D loss: 1.027430] [G loss: 1.000199]\n",
      "1816 [D loss: 1.026552] [G loss: 1.000147]\n",
      "1817 [D loss: 1.022521] [G loss: 1.000251]\n",
      "1818 [D loss: 1.028521] [G loss: 1.000135]\n",
      "1819 [D loss: 1.025023] [G loss: 1.000191]\n",
      "1820 [D loss: 1.024777] [G loss: 1.000218]\n",
      "1821 [D loss: 1.024613] [G loss: 1.000126]\n",
      "1822 [D loss: 1.024638] [G loss: 1.000199]\n",
      "1823 [D loss: 1.027377] [G loss: 1.000114]\n",
      "1824 [D loss: 1.028419] [G loss: 1.000187]\n",
      "1825 [D loss: 1.026372] [G loss: 1.000285]\n",
      "1826 [D loss: 1.025469] [G loss: 1.000244]\n",
      "1827 [D loss: 1.025097] [G loss: 1.000223]\n",
      "1828 [D loss: 1.028321] [G loss: 1.000189]\n",
      "1829 [D loss: 1.025055] [G loss: 1.000088]\n",
      "1830 [D loss: 1.025570] [G loss: 1.000192]\n",
      "1831 [D loss: 1.027070] [G loss: 1.000249]\n",
      "1832 [D loss: 1.024748] [G loss: 1.000147]\n",
      "1833 [D loss: 1.026215] [G loss: 1.000261]\n",
      "1834 [D loss: 1.025396] [G loss: 1.000199]\n",
      "1835 [D loss: 1.024977] [G loss: 1.000221]\n",
      "1836 [D loss: 1.024210] [G loss: 1.000197]\n",
      "1837 [D loss: 1.023622] [G loss: 1.000212]\n",
      "1838 [D loss: 1.027005] [G loss: 1.000163]\n",
      "1839 [D loss: 1.027871] [G loss: 1.000245]\n",
      "1840 [D loss: 1.028099] [G loss: 1.000195]\n",
      "1841 [D loss: 1.024262] [G loss: 1.000218]\n",
      "1842 [D loss: 1.025957] [G loss: 1.000240]\n",
      "1843 [D loss: 1.025811] [G loss: 1.000335]\n",
      "1844 [D loss: 1.026822] [G loss: 1.000266]\n",
      "1845 [D loss: 1.027777] [G loss: 1.000198]\n",
      "1846 [D loss: 1.027264] [G loss: 1.000205]\n",
      "1847 [D loss: 1.027046] [G loss: 1.000245]\n",
      "1848 [D loss: 1.026905] [G loss: 1.000268]\n",
      "1849 [D loss: 1.024143] [G loss: 1.000199]\n",
      "1850 [D loss: 1.025698] [G loss: 1.000167]\n",
      "1851 [D loss: 1.027356] [G loss: 1.000156]\n",
      "1852 [D loss: 1.026209] [G loss: 1.000236]\n",
      "1853 [D loss: 1.026850] [G loss: 1.000289]\n",
      "1854 [D loss: 1.026099] [G loss: 1.000223]\n",
      "1855 [D loss: 1.022851] [G loss: 1.000129]\n",
      "1856 [D loss: 1.028926] [G loss: 1.000146]\n",
      "1857 [D loss: 1.026145] [G loss: 1.000183]\n",
      "1858 [D loss: 1.026914] [G loss: 1.000260]\n",
      "1859 [D loss: 1.025375] [G loss: 1.000225]\n",
      "1860 [D loss: 1.026227] [G loss: 1.000260]\n",
      "1861 [D loss: 1.026421] [G loss: 1.000138]\n",
      "1862 [D loss: 1.030924] [G loss: 1.000179]\n",
      "1863 [D loss: 1.027897] [G loss: 1.000179]\n",
      "1864 [D loss: 1.027612] [G loss: 1.000268]\n",
      "1865 [D loss: 1.025678] [G loss: 1.000263]\n",
      "1866 [D loss: 1.025621] [G loss: 1.000175]\n",
      "1867 [D loss: 1.025054] [G loss: 1.000204]\n",
      "1868 [D loss: 1.026331] [G loss: 1.000240]\n",
      "1869 [D loss: 1.028269] [G loss: 1.000309]\n",
      "1870 [D loss: 1.024420] [G loss: 1.000235]\n",
      "1871 [D loss: 1.027338] [G loss: 1.000150]\n",
      "1872 [D loss: 1.027166] [G loss: 1.000249]\n",
      "1873 [D loss: 1.025761] [G loss: 1.000234]\n",
      "1874 [D loss: 1.027529] [G loss: 1.000239]\n",
      "1875 [D loss: 1.024854] [G loss: 1.000100]\n",
      "1876 [D loss: 1.028682] [G loss: 1.000280]\n",
      "1877 [D loss: 1.026379] [G loss: 1.000195]\n",
      "1878 [D loss: 1.027132] [G loss: 1.000244]\n",
      "1879 [D loss: 1.025180] [G loss: 1.000211]\n",
      "1880 [D loss: 1.023476] [G loss: 1.000338]\n",
      "1881 [D loss: 1.027423] [G loss: 1.000264]\n",
      "1882 [D loss: 1.026973] [G loss: 1.000172]\n",
      "1883 [D loss: 1.028204] [G loss: 1.000231]\n",
      "1884 [D loss: 1.023895] [G loss: 1.000162]\n",
      "1885 [D loss: 1.026997] [G loss: 1.000249]\n",
      "1886 [D loss: 1.028099] [G loss: 1.000160]\n",
      "1887 [D loss: 1.027103] [G loss: 1.000245]\n",
      "1888 [D loss: 1.026356] [G loss: 1.000158]\n",
      "1889 [D loss: 1.026860] [G loss: 1.000306]\n",
      "1890 [D loss: 1.028956] [G loss: 1.000208]\n",
      "1891 [D loss: 1.027876] [G loss: 1.000422]\n",
      "1892 [D loss: 1.025155] [G loss: 1.000277]\n",
      "1893 [D loss: 1.025363] [G loss: 1.000205]\n",
      "1894 [D loss: 1.025893] [G loss: 1.000260]\n",
      "1895 [D loss: 1.026142] [G loss: 1.000238]\n",
      "1896 [D loss: 1.028063] [G loss: 1.000250]\n",
      "1897 [D loss: 1.026929] [G loss: 1.000275]\n",
      "1898 [D loss: 1.026223] [G loss: 1.000284]\n",
      "1899 [D loss: 1.027545] [G loss: 1.000253]\n",
      "1900 [D loss: 1.026881] [G loss: 1.000278]\n",
      "1901 [D loss: 1.026361] [G loss: 1.000355]\n",
      "1902 [D loss: 1.027467] [G loss: 1.000184]\n",
      "1903 [D loss: 1.025649] [G loss: 1.000272]\n",
      "1904 [D loss: 1.028520] [G loss: 1.000245]\n",
      "1905 [D loss: 1.022921] [G loss: 1.000203]\n",
      "1906 [D loss: 1.024693] [G loss: 1.000308]\n",
      "1907 [D loss: 1.024534] [G loss: 1.000341]\n",
      "1908 [D loss: 1.026669] [G loss: 1.000237]\n",
      "1909 [D loss: 1.025673] [G loss: 1.000258]\n",
      "1910 [D loss: 1.024237] [G loss: 1.000300]\n",
      "1911 [D loss: 1.024062] [G loss: 1.000246]\n",
      "1912 [D loss: 1.029632] [G loss: 1.000303]\n",
      "1913 [D loss: 1.026439] [G loss: 1.000364]\n",
      "1914 [D loss: 1.024122] [G loss: 1.000171]\n",
      "1915 [D loss: 1.022612] [G loss: 1.000179]\n",
      "1916 [D loss: 1.026356] [G loss: 1.000243]\n",
      "1917 [D loss: 1.026667] [G loss: 1.000202]\n",
      "1918 [D loss: 1.026540] [G loss: 1.000283]\n",
      "1919 [D loss: 1.027921] [G loss: 1.000136]\n",
      "1920 [D loss: 1.024085] [G loss: 1.000237]\n",
      "1921 [D loss: 1.024548] [G loss: 1.000220]\n",
      "1922 [D loss: 1.026710] [G loss: 1.000229]\n",
      "1923 [D loss: 1.024430] [G loss: 1.000184]\n",
      "1924 [D loss: 1.023100] [G loss: 1.000291]\n",
      "1925 [D loss: 1.028638] [G loss: 1.000183]\n",
      "1926 [D loss: 1.022799] [G loss: 1.000322]\n",
      "1927 [D loss: 1.028213] [G loss: 1.000305]\n",
      "1928 [D loss: 1.024842] [G loss: 1.000159]\n",
      "1929 [D loss: 1.025063] [G loss: 1.000235]\n",
      "1930 [D loss: 1.025883] [G loss: 1.000235]\n",
      "1931 [D loss: 1.026882] [G loss: 1.000128]\n",
      "1932 [D loss: 1.025407] [G loss: 1.000205]\n",
      "1933 [D loss: 1.024762] [G loss: 1.000288]\n",
      "1934 [D loss: 1.024567] [G loss: 1.000121]\n",
      "1935 [D loss: 1.027243] [G loss: 1.000163]\n",
      "1936 [D loss: 1.025301] [G loss: 1.000185]\n",
      "1937 [D loss: 1.025491] [G loss: 1.000288]\n",
      "1938 [D loss: 1.025330] [G loss: 1.000254]\n",
      "1939 [D loss: 1.030711] [G loss: 1.000248]\n",
      "1940 [D loss: 1.027967] [G loss: 1.000280]\n",
      "1941 [D loss: 1.026054] [G loss: 1.000312]\n",
      "1942 [D loss: 1.026727] [G loss: 1.000245]\n",
      "1943 [D loss: 1.025599] [G loss: 1.000231]\n",
      "1944 [D loss: 1.023162] [G loss: 1.000264]\n",
      "1945 [D loss: 1.025371] [G loss: 1.000329]\n",
      "1946 [D loss: 1.029230] [G loss: 1.000324]\n",
      "1947 [D loss: 1.025565] [G loss: 1.000219]\n",
      "1948 [D loss: 1.027336] [G loss: 1.000203]\n",
      "1949 [D loss: 1.024573] [G loss: 1.000267]\n",
      "1950 [D loss: 1.024595] [G loss: 1.000205]\n",
      "1951 [D loss: 1.026481] [G loss: 1.000190]\n",
      "1952 [D loss: 1.026674] [G loss: 1.000164]\n",
      "1953 [D loss: 1.024978] [G loss: 1.000234]\n",
      "1954 [D loss: 1.027216] [G loss: 1.000253]\n",
      "1955 [D loss: 1.024919] [G loss: 1.000268]\n",
      "1956 [D loss: 1.027525] [G loss: 1.000315]\n",
      "1957 [D loss: 1.023699] [G loss: 1.000174]\n",
      "1958 [D loss: 1.026895] [G loss: 1.000169]\n",
      "1959 [D loss: 1.024721] [G loss: 1.000233]\n",
      "1960 [D loss: 1.026702] [G loss: 1.000358]\n",
      "1961 [D loss: 1.027488] [G loss: 1.000203]\n",
      "1962 [D loss: 1.021820] [G loss: 1.000261]\n",
      "1963 [D loss: 1.024650] [G loss: 1.000345]\n",
      "1964 [D loss: 1.023211] [G loss: 1.000294]\n",
      "1965 [D loss: 1.024232] [G loss: 1.000303]\n",
      "1966 [D loss: 1.026626] [G loss: 1.000191]\n",
      "1967 [D loss: 1.027988] [G loss: 1.000165]\n",
      "1968 [D loss: 1.026595] [G loss: 1.000185]\n",
      "1969 [D loss: 1.027125] [G loss: 1.000276]\n",
      "1970 [D loss: 1.025530] [G loss: 1.000228]\n",
      "1971 [D loss: 1.022772] [G loss: 1.000332]\n",
      "1972 [D loss: 1.027294] [G loss: 1.000211]\n",
      "1973 [D loss: 1.026639] [G loss: 1.000162]\n",
      "1974 [D loss: 1.027359] [G loss: 1.000277]\n",
      "1975 [D loss: 1.025515] [G loss: 1.000236]\n",
      "1976 [D loss: 1.026958] [G loss: 1.000244]\n",
      "1977 [D loss: 1.026254] [G loss: 1.000277]\n",
      "1978 [D loss: 1.025749] [G loss: 1.000288]\n",
      "1979 [D loss: 1.026123] [G loss: 1.000169]\n",
      "1980 [D loss: 1.025943] [G loss: 1.000236]\n",
      "1981 [D loss: 1.027634] [G loss: 1.000198]\n",
      "1982 [D loss: 1.023001] [G loss: 1.000220]\n",
      "1983 [D loss: 1.026036] [G loss: 1.000216]\n",
      "1984 [D loss: 1.026733] [G loss: 1.000323]\n",
      "1985 [D loss: 1.023061] [G loss: 1.000287]\n",
      "1986 [D loss: 1.025978] [G loss: 1.000306]\n",
      "1987 [D loss: 1.027023] [G loss: 1.000282]\n",
      "1988 [D loss: 1.027613] [G loss: 1.000257]\n",
      "1989 [D loss: 1.028042] [G loss: 1.000147]\n",
      "1990 [D loss: 1.025577] [G loss: 1.000274]\n",
      "1991 [D loss: 1.026610] [G loss: 1.000205]\n",
      "1992 [D loss: 1.026260] [G loss: 1.000234]\n",
      "1993 [D loss: 1.023608] [G loss: 1.000179]\n",
      "1994 [D loss: 1.025892] [G loss: 1.000200]\n",
      "1995 [D loss: 1.026114] [G loss: 1.000226]\n",
      "1996 [D loss: 1.028249] [G loss: 1.000270]\n",
      "1997 [D loss: 1.027723] [G loss: 1.000243]\n",
      "1998 [D loss: 1.025561] [G loss: 1.000287]\n",
      "1999 [D loss: 1.026345] [G loss: 1.000216]\n",
      "2000 [D loss: 1.027914] [G loss: 1.000166]\n",
      "2001 [D loss: 1.026908] [G loss: 1.000219]\n",
      "2002 [D loss: 1.025053] [G loss: 1.000327]\n",
      "2003 [D loss: 1.024391] [G loss: 1.000224]\n",
      "2004 [D loss: 1.023623] [G loss: 1.000243]\n",
      "2005 [D loss: 1.024387] [G loss: 1.000199]\n",
      "2006 [D loss: 1.027372] [G loss: 1.000251]\n",
      "2007 [D loss: 1.026235] [G loss: 1.000300]\n",
      "2008 [D loss: 1.026500] [G loss: 1.000207]\n",
      "2009 [D loss: 1.027421] [G loss: 1.000234]\n",
      "2010 [D loss: 1.027500] [G loss: 1.000298]\n",
      "2011 [D loss: 1.027052] [G loss: 1.000257]\n",
      "2012 [D loss: 1.024023] [G loss: 1.000231]\n",
      "2013 [D loss: 1.028071] [G loss: 1.000204]\n",
      "2014 [D loss: 1.025232] [G loss: 1.000281]\n",
      "2015 [D loss: 1.026043] [G loss: 1.000234]\n",
      "2016 [D loss: 1.029177] [G loss: 1.000277]\n",
      "2017 [D loss: 1.026664] [G loss: 1.000118]\n",
      "2018 [D loss: 1.023770] [G loss: 1.000255]\n",
      "2019 [D loss: 1.027902] [G loss: 1.000248]\n",
      "2020 [D loss: 1.024300] [G loss: 1.000272]\n",
      "2021 [D loss: 1.025008] [G loss: 1.000242]\n",
      "2022 [D loss: 1.027917] [G loss: 1.000238]\n",
      "2023 [D loss: 1.026504] [G loss: 1.000218]\n",
      "2024 [D loss: 1.026905] [G loss: 1.000174]\n",
      "2025 [D loss: 1.027844] [G loss: 1.000151]\n",
      "2026 [D loss: 1.028161] [G loss: 1.000235]\n",
      "2027 [D loss: 1.026935] [G loss: 1.000242]\n",
      "2028 [D loss: 1.027200] [G loss: 1.000212]\n",
      "2029 [D loss: 1.025345] [G loss: 1.000221]\n",
      "2030 [D loss: 1.027908] [G loss: 1.000279]\n",
      "2031 [D loss: 1.026740] [G loss: 1.000093]\n",
      "2032 [D loss: 1.023451] [G loss: 1.000281]\n",
      "2033 [D loss: 1.026698] [G loss: 1.000291]\n",
      "2034 [D loss: 1.027054] [G loss: 1.000279]\n",
      "2035 [D loss: 1.024837] [G loss: 1.000297]\n",
      "2036 [D loss: 1.025518] [G loss: 1.000190]\n",
      "2037 [D loss: 1.028289] [G loss: 1.000166]\n",
      "2038 [D loss: 1.026147] [G loss: 1.000262]\n",
      "2039 [D loss: 1.026964] [G loss: 1.000296]\n",
      "2040 [D loss: 1.027635] [G loss: 1.000236]\n",
      "2041 [D loss: 1.028337] [G loss: 1.000346]\n",
      "2042 [D loss: 1.025974] [G loss: 1.000224]\n",
      "2043 [D loss: 1.023283] [G loss: 1.000295]\n",
      "2044 [D loss: 1.025446] [G loss: 1.000118]\n",
      "2045 [D loss: 1.023989] [G loss: 1.000251]\n",
      "2046 [D loss: 1.024197] [G loss: 1.000196]\n",
      "2047 [D loss: 1.027429] [G loss: 1.000159]\n",
      "2048 [D loss: 1.026212] [G loss: 1.000246]\n",
      "2049 [D loss: 1.024732] [G loss: 1.000206]\n",
      "2050 [D loss: 1.027995] [G loss: 1.000319]\n",
      "2051 [D loss: 1.024377] [G loss: 1.000244]\n",
      "2052 [D loss: 1.025935] [G loss: 1.000303]\n",
      "2053 [D loss: 1.025940] [G loss: 1.000259]\n",
      "2054 [D loss: 1.024154] [G loss: 1.000165]\n",
      "2055 [D loss: 1.025951] [G loss: 1.000282]\n",
      "2056 [D loss: 1.025965] [G loss: 1.000219]\n",
      "2057 [D loss: 1.028276] [G loss: 1.000214]\n",
      "2058 [D loss: 1.025001] [G loss: 1.000218]\n",
      "2059 [D loss: 1.030002] [G loss: 1.000276]\n",
      "2060 [D loss: 1.026504] [G loss: 1.000241]\n",
      "2061 [D loss: 1.025575] [G loss: 1.000274]\n",
      "2062 [D loss: 1.025155] [G loss: 1.000291]\n",
      "2063 [D loss: 1.027627] [G loss: 1.000279]\n",
      "2064 [D loss: 1.028351] [G loss: 1.000206]\n",
      "2065 [D loss: 1.026561] [G loss: 1.000253]\n",
      "2066 [D loss: 1.026534] [G loss: 1.000134]\n",
      "2067 [D loss: 1.025543] [G loss: 1.000195]\n",
      "2068 [D loss: 1.025782] [G loss: 1.000238]\n",
      "2069 [D loss: 1.026209] [G loss: 1.000297]\n",
      "2070 [D loss: 1.026048] [G loss: 1.000235]\n",
      "2071 [D loss: 1.029552] [G loss: 1.000237]\n",
      "2072 [D loss: 1.024064] [G loss: 1.000314]\n",
      "2073 [D loss: 1.024014] [G loss: 1.000269]\n",
      "2074 [D loss: 1.026687] [G loss: 1.000260]\n",
      "2075 [D loss: 1.028278] [G loss: 1.000266]\n",
      "2076 [D loss: 1.028007] [G loss: 1.000127]\n",
      "2077 [D loss: 1.026530] [G loss: 1.000334]\n",
      "2078 [D loss: 1.025766] [G loss: 1.000283]\n",
      "2079 [D loss: 1.026332] [G loss: 1.000308]\n",
      "2080 [D loss: 1.026179] [G loss: 1.000187]\n",
      "2081 [D loss: 1.026016] [G loss: 1.000198]\n",
      "2082 [D loss: 1.026011] [G loss: 1.000245]\n",
      "2083 [D loss: 1.024055] [G loss: 1.000378]\n",
      "2084 [D loss: 1.028229] [G loss: 1.000241]\n",
      "2085 [D loss: 1.025113] [G loss: 1.000330]\n",
      "2086 [D loss: 1.028223] [G loss: 1.000248]\n",
      "2087 [D loss: 1.025984] [G loss: 1.000240]\n",
      "2088 [D loss: 1.026642] [G loss: 1.000213]\n",
      "2089 [D loss: 1.028365] [G loss: 1.000325]\n",
      "2090 [D loss: 1.024923] [G loss: 1.000247]\n",
      "2091 [D loss: 1.027876] [G loss: 1.000364]\n",
      "2092 [D loss: 1.027095] [G loss: 1.000212]\n",
      "2093 [D loss: 1.024786] [G loss: 1.000226]\n",
      "2094 [D loss: 1.023730] [G loss: 1.000217]\n",
      "2095 [D loss: 1.026113] [G loss: 1.000212]\n",
      "2096 [D loss: 1.024470] [G loss: 1.000278]\n",
      "2097 [D loss: 1.027798] [G loss: 1.000326]\n",
      "2098 [D loss: 1.027951] [G loss: 1.000252]\n",
      "2099 [D loss: 1.024636] [G loss: 1.000154]\n",
      "2100 [D loss: 1.024310] [G loss: 1.000241]\n",
      "2101 [D loss: 1.024237] [G loss: 1.000212]\n",
      "2102 [D loss: 1.026560] [G loss: 1.000244]\n",
      "2103 [D loss: 1.028196] [G loss: 1.000196]\n",
      "2104 [D loss: 1.026993] [G loss: 1.000269]\n",
      "2105 [D loss: 1.027032] [G loss: 1.000235]\n",
      "2106 [D loss: 1.022937] [G loss: 1.000338]\n",
      "2107 [D loss: 1.025100] [G loss: 1.000245]\n",
      "2108 [D loss: 1.027153] [G loss: 1.000239]\n",
      "2109 [D loss: 1.027402] [G loss: 1.000149]\n",
      "2110 [D loss: 1.026567] [G loss: 1.000213]\n",
      "2111 [D loss: 1.024902] [G loss: 1.000185]\n",
      "2112 [D loss: 1.027845] [G loss: 1.000301]\n",
      "2113 [D loss: 1.028191] [G loss: 1.000255]\n",
      "2114 [D loss: 1.025291] [G loss: 1.000334]\n",
      "2115 [D loss: 1.024111] [G loss: 1.000184]\n",
      "2116 [D loss: 1.025989] [G loss: 1.000248]\n",
      "2117 [D loss: 1.027208] [G loss: 1.000302]\n",
      "2118 [D loss: 1.026227] [G loss: 1.000240]\n",
      "2119 [D loss: 1.026095] [G loss: 1.000239]\n",
      "2120 [D loss: 1.028145] [G loss: 1.000258]\n",
      "2121 [D loss: 1.025080] [G loss: 1.000237]\n",
      "2122 [D loss: 1.026458] [G loss: 1.000291]\n",
      "2123 [D loss: 1.028979] [G loss: 1.000192]\n",
      "2124 [D loss: 1.027287] [G loss: 1.000223]\n",
      "2125 [D loss: 1.024820] [G loss: 1.000278]\n",
      "2126 [D loss: 1.025860] [G loss: 1.000266]\n",
      "2127 [D loss: 1.026794] [G loss: 1.000193]\n",
      "2128 [D loss: 1.028422] [G loss: 1.000266]\n",
      "2129 [D loss: 1.026460] [G loss: 1.000211]\n",
      "2130 [D loss: 1.028637] [G loss: 1.000195]\n",
      "2131 [D loss: 1.028112] [G loss: 1.000246]\n",
      "2132 [D loss: 1.026637] [G loss: 1.000163]\n",
      "2133 [D loss: 1.029656] [G loss: 1.000296]\n",
      "2134 [D loss: 1.024951] [G loss: 1.000275]\n",
      "2135 [D loss: 1.025474] [G loss: 1.000204]\n",
      "2136 [D loss: 1.026352] [G loss: 1.000394]\n",
      "2137 [D loss: 1.025837] [G loss: 1.000252]\n",
      "2138 [D loss: 1.028108] [G loss: 1.000228]\n",
      "2139 [D loss: 1.026407] [G loss: 1.000282]\n",
      "2140 [D loss: 1.029274] [G loss: 1.000265]\n",
      "2141 [D loss: 1.025061] [G loss: 1.000292]\n",
      "2142 [D loss: 1.024888] [G loss: 1.000126]\n",
      "2143 [D loss: 1.026057] [G loss: 1.000242]\n",
      "2144 [D loss: 1.027078] [G loss: 1.000224]\n",
      "2145 [D loss: 1.023315] [G loss: 1.000268]\n",
      "2146 [D loss: 1.027882] [G loss: 1.000201]\n",
      "2147 [D loss: 1.025173] [G loss: 1.000253]\n",
      "2148 [D loss: 1.027540] [G loss: 1.000335]\n",
      "2149 [D loss: 1.026245] [G loss: 1.000257]\n",
      "2150 [D loss: 1.024502] [G loss: 1.000222]\n",
      "2151 [D loss: 1.028202] [G loss: 1.000203]\n",
      "2152 [D loss: 1.025156] [G loss: 1.000302]\n",
      "2153 [D loss: 1.027801] [G loss: 1.000268]\n",
      "2154 [D loss: 1.026134] [G loss: 1.000348]\n",
      "2155 [D loss: 1.025724] [G loss: 1.000209]\n",
      "2156 [D loss: 1.027232] [G loss: 1.000288]\n",
      "2157 [D loss: 1.025332] [G loss: 1.000317]\n",
      "2158 [D loss: 1.024407] [G loss: 1.000251]\n",
      "2159 [D loss: 1.026173] [G loss: 1.000244]\n",
      "2160 [D loss: 1.022296] [G loss: 1.000207]\n",
      "2161 [D loss: 1.025160] [G loss: 1.000258]\n",
      "2162 [D loss: 1.025022] [G loss: 1.000262]\n",
      "2163 [D loss: 1.027705] [G loss: 1.000253]\n",
      "2164 [D loss: 1.025479] [G loss: 1.000319]\n",
      "2165 [D loss: 1.026606] [G loss: 1.000210]\n",
      "2166 [D loss: 1.026709] [G loss: 1.000244]\n",
      "2167 [D loss: 1.026242] [G loss: 1.000238]\n",
      "2168 [D loss: 1.024329] [G loss: 1.000324]\n",
      "2169 [D loss: 1.025085] [G loss: 1.000194]\n",
      "2170 [D loss: 1.024830] [G loss: 1.000189]\n",
      "2171 [D loss: 1.028745] [G loss: 1.000264]\n",
      "2172 [D loss: 1.026624] [G loss: 1.000253]\n",
      "2173 [D loss: 1.026849] [G loss: 1.000261]\n",
      "2174 [D loss: 1.023880] [G loss: 1.000207]\n",
      "2175 [D loss: 1.023467] [G loss: 1.000277]\n",
      "2176 [D loss: 1.024067] [G loss: 1.000247]\n",
      "2177 [D loss: 1.023924] [G loss: 1.000250]\n",
      "2178 [D loss: 1.028025] [G loss: 1.000278]\n",
      "2179 [D loss: 1.024354] [G loss: 1.000386]\n",
      "2180 [D loss: 1.026691] [G loss: 1.000261]\n",
      "2181 [D loss: 1.025282] [G loss: 1.000299]\n",
      "2182 [D loss: 1.026825] [G loss: 1.000213]\n",
      "2183 [D loss: 1.025917] [G loss: 1.000282]\n",
      "2184 [D loss: 1.027917] [G loss: 1.000199]\n",
      "2185 [D loss: 1.028902] [G loss: 1.000269]\n",
      "2186 [D loss: 1.029967] [G loss: 1.000266]\n",
      "2187 [D loss: 1.027302] [G loss: 1.000257]\n",
      "2188 [D loss: 1.024674] [G loss: 1.000235]\n",
      "2189 [D loss: 1.024952] [G loss: 1.000221]\n",
      "2190 [D loss: 1.023433] [G loss: 1.000128]\n",
      "2191 [D loss: 1.024976] [G loss: 1.000160]\n",
      "2192 [D loss: 1.024930] [G loss: 1.000360]\n",
      "2193 [D loss: 1.028873] [G loss: 1.000246]\n",
      "2194 [D loss: 1.027907] [G loss: 1.000232]\n",
      "2195 [D loss: 1.025726] [G loss: 1.000230]\n",
      "2196 [D loss: 1.024539] [G loss: 1.000316]\n",
      "2197 [D loss: 1.024178] [G loss: 1.000367]\n",
      "2198 [D loss: 1.026520] [G loss: 1.000298]\n",
      "2199 [D loss: 1.027149] [G loss: 1.000277]\n",
      "2200 [D loss: 1.025326] [G loss: 1.000303]\n",
      "2201 [D loss: 1.028890] [G loss: 1.000304]\n",
      "2202 [D loss: 1.024815] [G loss: 1.000225]\n",
      "2203 [D loss: 1.029694] [G loss: 1.000298]\n",
      "2204 [D loss: 1.028690] [G loss: 1.000293]\n",
      "2205 [D loss: 1.024998] [G loss: 1.000317]\n",
      "2206 [D loss: 1.025918] [G loss: 1.000182]\n",
      "2207 [D loss: 1.027171] [G loss: 1.000255]\n",
      "2208 [D loss: 1.027877] [G loss: 1.000225]\n",
      "2209 [D loss: 1.026080] [G loss: 1.000225]\n",
      "2210 [D loss: 1.030203] [G loss: 1.000250]\n",
      "2211 [D loss: 1.026388] [G loss: 1.000237]\n",
      "2212 [D loss: 1.027842] [G loss: 1.000242]\n",
      "2213 [D loss: 1.027172] [G loss: 1.000257]\n",
      "2214 [D loss: 1.025661] [G loss: 1.000229]\n",
      "2215 [D loss: 1.027085] [G loss: 1.000326]\n",
      "2216 [D loss: 1.023568] [G loss: 1.000300]\n",
      "2217 [D loss: 1.028230] [G loss: 1.000209]\n",
      "2218 [D loss: 1.027120] [G loss: 1.000222]\n",
      "2219 [D loss: 1.025374] [G loss: 1.000113]\n",
      "2220 [D loss: 1.023093] [G loss: 1.000295]\n",
      "2221 [D loss: 1.025798] [G loss: 1.000214]\n",
      "2222 [D loss: 1.026583] [G loss: 1.000339]\n",
      "2223 [D loss: 1.028176] [G loss: 1.000284]\n",
      "2224 [D loss: 1.026851] [G loss: 1.000257]\n",
      "2225 [D loss: 1.026175] [G loss: 1.000293]\n",
      "2226 [D loss: 1.027861] [G loss: 1.000322]\n",
      "2227 [D loss: 1.028139] [G loss: 1.000283]\n",
      "2228 [D loss: 1.025759] [G loss: 1.000271]\n",
      "2229 [D loss: 1.027119] [G loss: 1.000264]\n",
      "2230 [D loss: 1.024223] [G loss: 1.000352]\n",
      "2231 [D loss: 1.025280] [G loss: 1.000260]\n",
      "2232 [D loss: 1.026950] [G loss: 1.000220]\n",
      "2233 [D loss: 1.026368] [G loss: 1.000278]\n",
      "2234 [D loss: 1.027935] [G loss: 1.000189]\n",
      "2235 [D loss: 1.026315] [G loss: 1.000296]\n",
      "2236 [D loss: 1.027221] [G loss: 1.000255]\n",
      "2237 [D loss: 1.026067] [G loss: 1.000344]\n",
      "2238 [D loss: 1.025119] [G loss: 1.000312]\n",
      "2239 [D loss: 1.025017] [G loss: 1.000343]\n",
      "2240 [D loss: 1.027302] [G loss: 1.000342]\n",
      "2241 [D loss: 1.027524] [G loss: 1.000146]\n",
      "2242 [D loss: 1.026108] [G loss: 1.000317]\n",
      "2243 [D loss: 1.023818] [G loss: 1.000305]\n",
      "2244 [D loss: 1.028263] [G loss: 1.000211]\n",
      "2245 [D loss: 1.026252] [G loss: 1.000227]\n",
      "2246 [D loss: 1.028770] [G loss: 1.000309]\n",
      "2247 [D loss: 1.027103] [G loss: 1.000182]\n",
      "2248 [D loss: 1.028384] [G loss: 1.000311]\n",
      "2249 [D loss: 1.023735] [G loss: 1.000259]\n",
      "2250 [D loss: 1.023893] [G loss: 1.000339]\n",
      "2251 [D loss: 1.024391] [G loss: 1.000205]\n",
      "2252 [D loss: 1.025488] [G loss: 1.000294]\n",
      "2253 [D loss: 1.027192] [G loss: 1.000173]\n",
      "2254 [D loss: 1.028085] [G loss: 1.000332]\n",
      "2255 [D loss: 1.024881] [G loss: 1.000229]\n",
      "2256 [D loss: 1.027376] [G loss: 1.000246]\n",
      "2257 [D loss: 1.029299] [G loss: 1.000226]\n",
      "2258 [D loss: 1.027324] [G loss: 1.000152]\n",
      "2259 [D loss: 1.024036] [G loss: 1.000222]\n",
      "2260 [D loss: 1.027387] [G loss: 1.000204]\n",
      "2261 [D loss: 1.025062] [G loss: 1.000134]\n",
      "2262 [D loss: 1.023887] [G loss: 1.000280]\n",
      "2263 [D loss: 1.021780] [G loss: 1.000361]\n",
      "2264 [D loss: 1.025539] [G loss: 1.000325]\n",
      "2265 [D loss: 1.022825] [G loss: 1.000231]\n",
      "2266 [D loss: 1.026818] [G loss: 1.000151]\n",
      "2267 [D loss: 1.026514] [G loss: 1.000274]\n",
      "2268 [D loss: 1.026540] [G loss: 1.000263]\n",
      "2269 [D loss: 1.025543] [G loss: 1.000237]\n",
      "2270 [D loss: 1.026817] [G loss: 1.000220]\n",
      "2271 [D loss: 1.025468] [G loss: 1.000234]\n",
      "2272 [D loss: 1.027605] [G loss: 1.000275]\n",
      "2273 [D loss: 1.028654] [G loss: 1.000247]\n",
      "2274 [D loss: 1.025811] [G loss: 1.000198]\n",
      "2275 [D loss: 1.023241] [G loss: 1.000243]\n",
      "2276 [D loss: 1.025961] [G loss: 1.000304]\n",
      "2277 [D loss: 1.028477] [G loss: 1.000330]\n",
      "2278 [D loss: 1.024817] [G loss: 1.000240]\n",
      "2279 [D loss: 1.027653] [G loss: 1.000208]\n",
      "2280 [D loss: 1.026773] [G loss: 1.000282]\n",
      "2281 [D loss: 1.025389] [G loss: 1.000284]\n",
      "2282 [D loss: 1.025250] [G loss: 1.000197]\n",
      "2283 [D loss: 1.027332] [G loss: 1.000240]\n",
      "2284 [D loss: 1.025348] [G loss: 1.000331]\n",
      "2285 [D loss: 1.026145] [G loss: 1.000289]\n",
      "2286 [D loss: 1.024478] [G loss: 1.000231]\n",
      "2287 [D loss: 1.027345] [G loss: 1.000221]\n",
      "2288 [D loss: 1.025954] [G loss: 1.000287]\n",
      "2289 [D loss: 1.025848] [G loss: 1.000295]\n",
      "2290 [D loss: 1.027159] [G loss: 1.000176]\n",
      "2291 [D loss: 1.026282] [G loss: 1.000201]\n",
      "2292 [D loss: 1.025258] [G loss: 1.000197]\n",
      "2293 [D loss: 1.028668] [G loss: 1.000288]\n",
      "2294 [D loss: 1.026370] [G loss: 1.000270]\n",
      "2295 [D loss: 1.024594] [G loss: 1.000181]\n",
      "2296 [D loss: 1.025008] [G loss: 1.000209]\n",
      "2297 [D loss: 1.028142] [G loss: 1.000155]\n",
      "2298 [D loss: 1.026800] [G loss: 1.000123]\n",
      "2299 [D loss: 1.025729] [G loss: 1.000312]\n",
      "2300 [D loss: 1.025630] [G loss: 1.000355]\n",
      "2301 [D loss: 1.025366] [G loss: 1.000116]\n",
      "2302 [D loss: 1.028337] [G loss: 1.000304]\n",
      "2303 [D loss: 1.018347] [G loss: 1.000182]\n",
      "2304 [D loss: 1.026273] [G loss: 1.000301]\n",
      "2305 [D loss: 1.024840] [G loss: 1.000262]\n",
      "2306 [D loss: 1.025686] [G loss: 1.000306]\n",
      "2307 [D loss: 1.025034] [G loss: 1.000130]\n",
      "2308 [D loss: 1.023570] [G loss: 1.000143]\n",
      "2309 [D loss: 1.025881] [G loss: 1.000178]\n",
      "2310 [D loss: 1.027913] [G loss: 1.000233]\n",
      "2311 [D loss: 1.025939] [G loss: 1.000220]\n",
      "2312 [D loss: 1.027783] [G loss: 1.000332]\n",
      "2313 [D loss: 1.025058] [G loss: 1.000338]\n",
      "2314 [D loss: 1.024622] [G loss: 1.000207]\n",
      "2315 [D loss: 1.027591] [G loss: 1.000300]\n",
      "2316 [D loss: 1.025936] [G loss: 1.000394]\n",
      "2317 [D loss: 1.024522] [G loss: 1.000246]\n",
      "2318 [D loss: 1.026159] [G loss: 1.000341]\n",
      "2319 [D loss: 1.029594] [G loss: 1.000271]\n",
      "2320 [D loss: 1.026838] [G loss: 1.000229]\n",
      "2321 [D loss: 1.025712] [G loss: 1.000273]\n",
      "2322 [D loss: 1.026085] [G loss: 1.000246]\n",
      "2323 [D loss: 1.024493] [G loss: 1.000343]\n",
      "2324 [D loss: 1.028016] [G loss: 1.000234]\n",
      "2325 [D loss: 1.026890] [G loss: 1.000150]\n",
      "2326 [D loss: 1.025495] [G loss: 1.000295]\n",
      "2327 [D loss: 1.025167] [G loss: 1.000227]\n",
      "2328 [D loss: 1.025872] [G loss: 1.000208]\n",
      "2329 [D loss: 1.024663] [G loss: 1.000270]\n",
      "2330 [D loss: 1.024334] [G loss: 1.000370]\n",
      "2331 [D loss: 1.026190] [G loss: 1.000275]\n",
      "2332 [D loss: 1.026790] [G loss: 1.000217]\n",
      "2333 [D loss: 1.024432] [G loss: 1.000084]\n",
      "2334 [D loss: 1.026085] [G loss: 1.000284]\n",
      "2335 [D loss: 1.025290] [G loss: 1.000282]\n",
      "2336 [D loss: 1.025663] [G loss: 1.000306]\n",
      "2337 [D loss: 1.025735] [G loss: 1.000196]\n",
      "2338 [D loss: 1.025242] [G loss: 1.000231]\n",
      "2339 [D loss: 1.028052] [G loss: 1.000246]\n",
      "2340 [D loss: 1.026076] [G loss: 1.000181]\n",
      "2341 [D loss: 1.025385] [G loss: 1.000097]\n",
      "2342 [D loss: 1.027913] [G loss: 1.000243]\n",
      "2343 [D loss: 1.026773] [G loss: 1.000212]\n",
      "2344 [D loss: 1.025912] [G loss: 1.000239]\n",
      "2345 [D loss: 1.026934] [G loss: 1.000196]\n",
      "2346 [D loss: 1.025555] [G loss: 1.000263]\n",
      "2347 [D loss: 1.028131] [G loss: 1.000247]\n",
      "2348 [D loss: 1.026318] [G loss: 1.000219]\n",
      "2349 [D loss: 1.025581] [G loss: 1.000347]\n",
      "2350 [D loss: 1.025408] [G loss: 1.000211]\n",
      "2351 [D loss: 1.025390] [G loss: 1.000313]\n",
      "2352 [D loss: 1.024787] [G loss: 1.000257]\n",
      "2353 [D loss: 1.024552] [G loss: 1.000223]\n",
      "2354 [D loss: 1.025009] [G loss: 1.000081]\n",
      "2355 [D loss: 1.025177] [G loss: 1.000277]\n",
      "2356 [D loss: 1.025078] [G loss: 1.000179]\n",
      "2357 [D loss: 1.024090] [G loss: 1.000206]\n",
      "2358 [D loss: 1.027567] [G loss: 1.000326]\n",
      "2359 [D loss: 1.025873] [G loss: 1.000154]\n",
      "2360 [D loss: 1.027747] [G loss: 1.000253]\n",
      "2361 [D loss: 1.028329] [G loss: 1.000175]\n",
      "2362 [D loss: 1.025352] [G loss: 1.000196]\n",
      "2363 [D loss: 1.026810] [G loss: 1.000180]\n",
      "2364 [D loss: 1.024838] [G loss: 1.000207]\n",
      "2365 [D loss: 1.025950] [G loss: 1.000254]\n",
      "2366 [D loss: 1.025592] [G loss: 1.000295]\n",
      "2367 [D loss: 1.028006] [G loss: 1.000189]\n",
      "2368 [D loss: 1.028250] [G loss: 1.000296]\n",
      "2369 [D loss: 1.027810] [G loss: 1.000096]\n",
      "2370 [D loss: 1.025660] [G loss: 1.000310]\n",
      "2371 [D loss: 1.026889] [G loss: 1.000327]\n",
      "2372 [D loss: 1.028068] [G loss: 1.000176]\n",
      "2373 [D loss: 1.025317] [G loss: 1.000178]\n",
      "2374 [D loss: 1.024578] [G loss: 1.000316]\n",
      "2375 [D loss: 1.023777] [G loss: 1.000188]\n",
      "2376 [D loss: 1.026770] [G loss: 1.000401]\n",
      "2377 [D loss: 1.027478] [G loss: 1.000201]\n",
      "2378 [D loss: 1.026339] [G loss: 1.000350]\n",
      "2379 [D loss: 1.027695] [G loss: 1.000247]\n",
      "2380 [D loss: 1.024093] [G loss: 1.000222]\n",
      "2381 [D loss: 1.023118] [G loss: 1.000325]\n",
      "2382 [D loss: 1.025145] [G loss: 1.000225]\n",
      "2383 [D loss: 1.027226] [G loss: 1.000254]\n",
      "2384 [D loss: 1.024368] [G loss: 1.000140]\n",
      "2385 [D loss: 1.022542] [G loss: 1.000182]\n",
      "2386 [D loss: 1.025477] [G loss: 1.000285]\n",
      "2387 [D loss: 1.025954] [G loss: 1.000169]\n",
      "2388 [D loss: 1.027508] [G loss: 1.000276]\n",
      "2389 [D loss: 1.028706] [G loss: 1.000166]\n",
      "2390 [D loss: 1.024755] [G loss: 1.000207]\n",
      "2391 [D loss: 1.026931] [G loss: 1.000209]\n",
      "2392 [D loss: 1.027302] [G loss: 1.000199]\n",
      "2393 [D loss: 1.024377] [G loss: 1.000260]\n",
      "2394 [D loss: 1.026446] [G loss: 1.000166]\n",
      "2395 [D loss: 1.028207] [G loss: 1.000226]\n",
      "2396 [D loss: 1.024990] [G loss: 1.000346]\n",
      "2397 [D loss: 1.027053] [G loss: 1.000352]\n",
      "2398 [D loss: 1.026848] [G loss: 1.000275]\n",
      "2399 [D loss: 1.029663] [G loss: 1.000265]\n",
      "2400 [D loss: 1.026920] [G loss: 1.000320]\n",
      "2401 [D loss: 1.026401] [G loss: 1.000264]\n",
      "2402 [D loss: 1.024905] [G loss: 1.000275]\n",
      "2403 [D loss: 1.027270] [G loss: 1.000221]\n",
      "2404 [D loss: 1.025629] [G loss: 1.000200]\n",
      "2405 [D loss: 1.025588] [G loss: 1.000199]\n",
      "2406 [D loss: 1.030053] [G loss: 1.000304]\n",
      "2407 [D loss: 1.025447] [G loss: 1.000220]\n",
      "2408 [D loss: 1.024370] [G loss: 1.000262]\n",
      "2409 [D loss: 1.027062] [G loss: 1.000190]\n",
      "2410 [D loss: 1.024796] [G loss: 1.000215]\n",
      "2411 [D loss: 1.026270] [G loss: 1.000239]\n",
      "2412 [D loss: 1.025660] [G loss: 1.000259]\n",
      "2413 [D loss: 1.025264] [G loss: 1.000357]\n",
      "2414 [D loss: 1.027934] [G loss: 1.000271]\n",
      "2415 [D loss: 1.025128] [G loss: 1.000250]\n",
      "2416 [D loss: 1.025981] [G loss: 1.000246]\n",
      "2417 [D loss: 1.024337] [G loss: 1.000270]\n",
      "2418 [D loss: 1.026217] [G loss: 1.000290]\n",
      "2419 [D loss: 1.025600] [G loss: 1.000205]\n",
      "2420 [D loss: 1.022243] [G loss: 1.000243]\n",
      "2421 [D loss: 1.025100] [G loss: 1.000247]\n",
      "2422 [D loss: 1.026792] [G loss: 1.000311]\n",
      "2423 [D loss: 1.027411] [G loss: 1.000211]\n",
      "2424 [D loss: 1.026652] [G loss: 1.000186]\n",
      "2425 [D loss: 1.026918] [G loss: 1.000277]\n",
      "2426 [D loss: 1.027924] [G loss: 1.000171]\n",
      "2427 [D loss: 1.028616] [G loss: 1.000214]\n",
      "2428 [D loss: 1.026249] [G loss: 1.000259]\n",
      "2429 [D loss: 1.026927] [G loss: 1.000204]\n",
      "2430 [D loss: 1.024459] [G loss: 1.000332]\n",
      "2431 [D loss: 1.025623] [G loss: 1.000196]\n",
      "2432 [D loss: 1.026707] [G loss: 1.000186]\n",
      "2433 [D loss: 1.026361] [G loss: 1.000300]\n",
      "2434 [D loss: 1.028100] [G loss: 1.000207]\n",
      "2435 [D loss: 1.025033] [G loss: 1.000227]\n",
      "2436 [D loss: 1.025360] [G loss: 1.000312]\n",
      "2437 [D loss: 1.027409] [G loss: 1.000193]\n",
      "2438 [D loss: 1.026222] [G loss: 1.000307]\n",
      "2439 [D loss: 1.026323] [G loss: 1.000121]\n",
      "2440 [D loss: 1.027087] [G loss: 1.000295]\n",
      "2441 [D loss: 1.027389] [G loss: 1.000298]\n",
      "2442 [D loss: 1.027347] [G loss: 1.000142]\n",
      "2443 [D loss: 1.027545] [G loss: 1.000310]\n",
      "2444 [D loss: 1.028144] [G loss: 1.000177]\n",
      "2445 [D loss: 1.027391] [G loss: 1.000250]\n",
      "2446 [D loss: 1.026202] [G loss: 1.000257]\n",
      "2447 [D loss: 1.028071] [G loss: 1.000225]\n",
      "2448 [D loss: 1.023638] [G loss: 1.000194]\n",
      "2449 [D loss: 1.024593] [G loss: 1.000233]\n",
      "2450 [D loss: 1.024766] [G loss: 1.000345]\n",
      "2451 [D loss: 1.028023] [G loss: 1.000212]\n",
      "2452 [D loss: 1.028827] [G loss: 1.000252]\n",
      "2453 [D loss: 1.027503] [G loss: 1.000230]\n",
      "2454 [D loss: 1.028078] [G loss: 1.000298]\n",
      "2455 [D loss: 1.027546] [G loss: 1.000255]\n",
      "2456 [D loss: 1.026399] [G loss: 1.000221]\n",
      "2457 [D loss: 1.024731] [G loss: 1.000292]\n",
      "2458 [D loss: 1.025571] [G loss: 1.000224]\n",
      "2459 [D loss: 1.024416] [G loss: 1.000125]\n",
      "2460 [D loss: 1.028737] [G loss: 1.000152]\n",
      "2461 [D loss: 1.029388] [G loss: 1.000266]\n",
      "2462 [D loss: 1.023474] [G loss: 1.000276]\n",
      "2463 [D loss: 1.025485] [G loss: 1.000308]\n",
      "2464 [D loss: 1.026506] [G loss: 1.000282]\n",
      "2465 [D loss: 1.026064] [G loss: 1.000170]\n",
      "2466 [D loss: 1.025329] [G loss: 1.000197]\n",
      "2467 [D loss: 1.023900] [G loss: 1.000267]\n",
      "2468 [D loss: 1.028067] [G loss: 1.000233]\n",
      "2469 [D loss: 1.025670] [G loss: 1.000124]\n",
      "2470 [D loss: 1.026945] [G loss: 1.000314]\n",
      "2471 [D loss: 1.023472] [G loss: 1.000317]\n",
      "2472 [D loss: 1.025734] [G loss: 1.000223]\n",
      "2473 [D loss: 1.026606] [G loss: 1.000196]\n",
      "2474 [D loss: 1.028131] [G loss: 1.000292]\n",
      "2475 [D loss: 1.026132] [G loss: 1.000326]\n",
      "2476 [D loss: 1.024991] [G loss: 1.000325]\n",
      "2477 [D loss: 1.026306] [G loss: 1.000190]\n",
      "2478 [D loss: 1.025070] [G loss: 1.000376]\n",
      "2479 [D loss: 1.025264] [G loss: 1.000383]\n",
      "2480 [D loss: 1.026925] [G loss: 1.000235]\n",
      "2481 [D loss: 1.027160] [G loss: 1.000301]\n",
      "2482 [D loss: 1.025042] [G loss: 1.000210]\n",
      "2483 [D loss: 1.026464] [G loss: 1.000328]\n",
      "2484 [D loss: 1.027398] [G loss: 1.000241]\n",
      "2485 [D loss: 1.026157] [G loss: 1.000164]\n",
      "2486 [D loss: 1.025738] [G loss: 1.000225]\n",
      "2487 [D loss: 1.026624] [G loss: 1.000283]\n",
      "2488 [D loss: 1.027111] [G loss: 1.000244]\n",
      "2489 [D loss: 1.026537] [G loss: 1.000213]\n",
      "2490 [D loss: 1.025832] [G loss: 1.000313]\n",
      "2491 [D loss: 1.026629] [G loss: 1.000359]\n",
      "2492 [D loss: 1.026572] [G loss: 1.000300]\n",
      "2493 [D loss: 1.026925] [G loss: 1.000328]\n",
      "2494 [D loss: 1.021712] [G loss: 1.000293]\n",
      "2495 [D loss: 1.024431] [G loss: 1.000353]\n",
      "2496 [D loss: 1.026508] [G loss: 1.000171]\n",
      "2497 [D loss: 1.028249] [G loss: 1.000336]\n",
      "2498 [D loss: 1.026768] [G loss: 1.000325]\n",
      "2499 [D loss: 1.025101] [G loss: 1.000220]\n",
      "2500 [D loss: 1.025300] [G loss: 1.000305]\n",
      "2501 [D loss: 1.026817] [G loss: 1.000361]\n",
      "2502 [D loss: 1.025417] [G loss: 1.000315]\n",
      "2503 [D loss: 1.027198] [G loss: 1.000274]\n",
      "2504 [D loss: 1.027522] [G loss: 1.000204]\n",
      "2505 [D loss: 1.027028] [G loss: 1.000250]\n",
      "2506 [D loss: 1.024111] [G loss: 1.000241]\n",
      "2507 [D loss: 1.027130] [G loss: 1.000297]\n",
      "2508 [D loss: 1.026912] [G loss: 1.000305]\n",
      "2509 [D loss: 1.024609] [G loss: 1.000352]\n",
      "2510 [D loss: 1.025523] [G loss: 1.000193]\n",
      "2511 [D loss: 1.026515] [G loss: 1.000233]\n",
      "2512 [D loss: 1.030214] [G loss: 1.000289]\n",
      "2513 [D loss: 1.027286] [G loss: 1.000250]\n",
      "2514 [D loss: 1.024104] [G loss: 1.000247]\n",
      "2515 [D loss: 1.025993] [G loss: 1.000304]\n",
      "2516 [D loss: 1.026635] [G loss: 1.000230]\n",
      "2517 [D loss: 1.024493] [G loss: 1.000207]\n",
      "2518 [D loss: 1.024541] [G loss: 1.000159]\n",
      "2519 [D loss: 1.024194] [G loss: 1.000241]\n",
      "2520 [D loss: 1.026492] [G loss: 1.000220]\n",
      "2521 [D loss: 1.025151] [G loss: 1.000225]\n",
      "2522 [D loss: 1.027681] [G loss: 1.000308]\n",
      "2523 [D loss: 1.028292] [G loss: 1.000257]\n",
      "2524 [D loss: 1.027334] [G loss: 1.000278]\n",
      "2525 [D loss: 1.027375] [G loss: 1.000223]\n",
      "2526 [D loss: 1.026504] [G loss: 1.000366]\n",
      "2527 [D loss: 1.024381] [G loss: 1.000265]\n",
      "2528 [D loss: 1.024464] [G loss: 1.000190]\n",
      "2529 [D loss: 1.027549] [G loss: 1.000223]\n",
      "2530 [D loss: 1.026945] [G loss: 1.000186]\n",
      "2531 [D loss: 1.026684] [G loss: 1.000263]\n",
      "2532 [D loss: 1.027134] [G loss: 1.000215]\n",
      "2533 [D loss: 1.029112] [G loss: 1.000178]\n",
      "2534 [D loss: 1.027378] [G loss: 1.000228]\n",
      "2535 [D loss: 1.024240] [G loss: 1.000204]\n",
      "2536 [D loss: 1.025356] [G loss: 1.000248]\n",
      "2537 [D loss: 1.026529] [G loss: 1.000153]\n",
      "2538 [D loss: 1.026455] [G loss: 1.000225]\n",
      "2539 [D loss: 1.026803] [G loss: 1.000099]\n",
      "2540 [D loss: 1.027104] [G loss: 1.000229]\n",
      "2541 [D loss: 1.025697] [G loss: 1.000219]\n",
      "2542 [D loss: 1.027056] [G loss: 1.000223]\n",
      "2543 [D loss: 1.026157] [G loss: 1.000319]\n",
      "2544 [D loss: 1.026831] [G loss: 1.000273]\n",
      "2545 [D loss: 1.025790] [G loss: 1.000358]\n",
      "2546 [D loss: 1.027183] [G loss: 1.000275]\n",
      "2547 [D loss: 1.026499] [G loss: 1.000164]\n",
      "2548 [D loss: 1.025978] [G loss: 1.000260]\n",
      "2549 [D loss: 1.027291] [G loss: 1.000273]\n",
      "2550 [D loss: 1.025334] [G loss: 1.000137]\n",
      "2551 [D loss: 1.025470] [G loss: 1.000232]\n",
      "2552 [D loss: 1.026760] [G loss: 1.000147]\n",
      "2553 [D loss: 1.027916] [G loss: 1.000305]\n",
      "2554 [D loss: 1.026824] [G loss: 1.000273]\n",
      "2555 [D loss: 1.028383] [G loss: 1.000304]\n",
      "2556 [D loss: 1.026725] [G loss: 1.000282]\n",
      "2557 [D loss: 1.027175] [G loss: 1.000334]\n",
      "2558 [D loss: 1.025103] [G loss: 1.000263]\n",
      "2559 [D loss: 1.024950] [G loss: 1.000317]\n",
      "2560 [D loss: 1.027401] [G loss: 1.000247]\n",
      "2561 [D loss: 1.025826] [G loss: 1.000172]\n",
      "2562 [D loss: 1.025189] [G loss: 1.000275]\n",
      "2563 [D loss: 1.027797] [G loss: 1.000233]\n",
      "2564 [D loss: 1.023001] [G loss: 1.000215]\n",
      "2565 [D loss: 1.029260] [G loss: 1.000294]\n",
      "2566 [D loss: 1.026045] [G loss: 1.000261]\n",
      "2567 [D loss: 1.024158] [G loss: 1.000246]\n",
      "2568 [D loss: 1.026611] [G loss: 1.000222]\n",
      "2569 [D loss: 1.025797] [G loss: 1.000144]\n",
      "2570 [D loss: 1.027232] [G loss: 1.000210]\n",
      "2571 [D loss: 1.025967] [G loss: 1.000192]\n",
      "2572 [D loss: 1.026949] [G loss: 1.000307]\n",
      "2573 [D loss: 1.028328] [G loss: 1.000244]\n",
      "2574 [D loss: 1.026819] [G loss: 1.000135]\n",
      "2575 [D loss: 1.026613] [G loss: 1.000194]\n",
      "2576 [D loss: 1.025663] [G loss: 1.000176]\n",
      "2577 [D loss: 1.027806] [G loss: 1.000150]\n",
      "2578 [D loss: 1.027354] [G loss: 1.000291]\n",
      "2579 [D loss: 1.024022] [G loss: 1.000137]\n",
      "2580 [D loss: 1.026092] [G loss: 1.000184]\n",
      "2581 [D loss: 1.025277] [G loss: 1.000132]\n",
      "2582 [D loss: 1.025736] [G loss: 1.000201]\n",
      "2583 [D loss: 1.029663] [G loss: 1.000174]\n",
      "2584 [D loss: 1.027528] [G loss: 1.000293]\n",
      "2585 [D loss: 1.024343] [G loss: 1.000167]\n",
      "2586 [D loss: 1.026995] [G loss: 1.000185]\n",
      "2587 [D loss: 1.024706] [G loss: 1.000276]\n",
      "2588 [D loss: 1.028269] [G loss: 1.000207]\n",
      "2589 [D loss: 1.024149] [G loss: 1.000265]\n",
      "2590 [D loss: 1.027050] [G loss: 1.000381]\n",
      "2591 [D loss: 1.028468] [G loss: 1.000320]\n",
      "2592 [D loss: 1.023328] [G loss: 1.000345]\n",
      "2593 [D loss: 1.027749] [G loss: 1.000164]\n",
      "2594 [D loss: 1.026380] [G loss: 1.000233]\n",
      "2595 [D loss: 1.028250] [G loss: 1.000215]\n",
      "2596 [D loss: 1.026577] [G loss: 1.000107]\n",
      "2597 [D loss: 1.027706] [G loss: 1.000204]\n",
      "2598 [D loss: 1.026433] [G loss: 1.000130]\n",
      "2599 [D loss: 1.025482] [G loss: 1.000255]\n",
      "2600 [D loss: 1.024996] [G loss: 1.000257]\n",
      "2601 [D loss: 1.025437] [G loss: 1.000217]\n",
      "2602 [D loss: 1.026617] [G loss: 1.000352]\n",
      "2603 [D loss: 1.025695] [G loss: 1.000291]\n",
      "2604 [D loss: 1.026951] [G loss: 1.000176]\n",
      "2605 [D loss: 1.024058] [G loss: 1.000166]\n",
      "2606 [D loss: 1.024303] [G loss: 1.000258]\n",
      "2607 [D loss: 1.025083] [G loss: 1.000213]\n",
      "2608 [D loss: 1.023641] [G loss: 1.000290]\n",
      "2609 [D loss: 1.027111] [G loss: 1.000217]\n",
      "2610 [D loss: 1.023871] [G loss: 1.000195]\n",
      "2611 [D loss: 1.029612] [G loss: 1.000277]\n",
      "2612 [D loss: 1.025817] [G loss: 1.000232]\n",
      "2613 [D loss: 1.026217] [G loss: 1.000268]\n",
      "2614 [D loss: 1.026586] [G loss: 1.000198]\n",
      "2615 [D loss: 1.028491] [G loss: 1.000252]\n",
      "2616 [D loss: 1.023679] [G loss: 1.000230]\n",
      "2617 [D loss: 1.025840] [G loss: 1.000184]\n",
      "2618 [D loss: 1.029770] [G loss: 1.000223]\n",
      "2619 [D loss: 1.028097] [G loss: 1.000305]\n",
      "2620 [D loss: 1.026166] [G loss: 1.000231]\n",
      "2621 [D loss: 1.024817] [G loss: 1.000292]\n",
      "2622 [D loss: 1.029372] [G loss: 1.000162]\n",
      "2623 [D loss: 1.025386] [G loss: 1.000275]\n",
      "2624 [D loss: 1.026445] [G loss: 1.000190]\n",
      "2625 [D loss: 1.026315] [G loss: 1.000288]\n",
      "2626 [D loss: 1.027940] [G loss: 1.000247]\n",
      "2627 [D loss: 1.024636] [G loss: 1.000296]\n",
      "2628 [D loss: 1.025890] [G loss: 1.000293]\n",
      "2629 [D loss: 1.023465] [G loss: 1.000288]\n",
      "2630 [D loss: 1.022201] [G loss: 1.000204]\n",
      "2631 [D loss: 1.024612] [G loss: 1.000268]\n",
      "2632 [D loss: 1.025497] [G loss: 1.000228]\n",
      "2633 [D loss: 1.027683] [G loss: 1.000218]\n",
      "2634 [D loss: 1.027139] [G loss: 1.000256]\n",
      "2635 [D loss: 1.027070] [G loss: 1.000121]\n",
      "2636 [D loss: 1.023120] [G loss: 1.000262]\n",
      "2637 [D loss: 1.027552] [G loss: 1.000313]\n",
      "2638 [D loss: 1.022461] [G loss: 1.000378]\n",
      "2639 [D loss: 1.024349] [G loss: 1.000247]\n",
      "2640 [D loss: 1.028148] [G loss: 1.000234]\n",
      "2641 [D loss: 1.024310] [G loss: 1.000067]\n",
      "2642 [D loss: 1.024182] [G loss: 1.000144]\n",
      "2643 [D loss: 1.026912] [G loss: 1.000306]\n",
      "2644 [D loss: 1.029950] [G loss: 1.000263]\n",
      "2645 [D loss: 1.026406] [G loss: 1.000229]\n",
      "2646 [D loss: 1.026383] [G loss: 1.000266]\n",
      "2647 [D loss: 1.025573] [G loss: 1.000143]\n",
      "2648 [D loss: 1.029441] [G loss: 1.000186]\n",
      "2649 [D loss: 1.024430] [G loss: 1.000064]\n",
      "2650 [D loss: 1.026105] [G loss: 1.000323]\n",
      "2651 [D loss: 1.026523] [G loss: 1.000339]\n",
      "2652 [D loss: 1.028877] [G loss: 1.000244]\n",
      "2653 [D loss: 1.026355] [G loss: 1.000223]\n",
      "2654 [D loss: 1.027541] [G loss: 1.000260]\n",
      "2655 [D loss: 1.027813] [G loss: 1.000170]\n",
      "2656 [D loss: 1.025473] [G loss: 1.000227]\n",
      "2657 [D loss: 1.028579] [G loss: 1.000324]\n",
      "2658 [D loss: 1.027294] [G loss: 1.000118]\n",
      "2659 [D loss: 1.025817] [G loss: 1.000260]\n",
      "2660 [D loss: 1.023428] [G loss: 1.000147]\n",
      "2661 [D loss: 1.026552] [G loss: 1.000207]\n",
      "2662 [D loss: 1.028406] [G loss: 1.000173]\n",
      "2663 [D loss: 1.027608] [G loss: 1.000224]\n",
      "2664 [D loss: 1.025044] [G loss: 1.000283]\n",
      "2665 [D loss: 1.024715] [G loss: 1.000179]\n",
      "2666 [D loss: 1.026838] [G loss: 1.000330]\n",
      "2667 [D loss: 1.024414] [G loss: 1.000212]\n",
      "2668 [D loss: 1.028939] [G loss: 1.000311]\n",
      "2669 [D loss: 1.026361] [G loss: 1.000335]\n",
      "2670 [D loss: 1.026752] [G loss: 1.000154]\n",
      "2671 [D loss: 1.023579] [G loss: 1.000229]\n",
      "2672 [D loss: 1.028272] [G loss: 1.000293]\n",
      "2673 [D loss: 1.026264] [G loss: 1.000191]\n",
      "2674 [D loss: 1.023460] [G loss: 1.000204]\n",
      "2675 [D loss: 1.026191] [G loss: 1.000324]\n",
      "2676 [D loss: 1.027131] [G loss: 1.000227]\n",
      "2677 [D loss: 1.026155] [G loss: 1.000194]\n",
      "2678 [D loss: 1.024111] [G loss: 1.000179]\n",
      "2679 [D loss: 1.025027] [G loss: 1.000169]\n",
      "2680 [D loss: 1.030300] [G loss: 1.000283]\n",
      "2681 [D loss: 1.022662] [G loss: 1.000219]\n",
      "2682 [D loss: 1.029307] [G loss: 1.000376]\n",
      "2683 [D loss: 1.028369] [G loss: 1.000180]\n",
      "2684 [D loss: 1.027806] [G loss: 1.000168]\n",
      "2685 [D loss: 1.025716] [G loss: 1.000261]\n",
      "2686 [D loss: 1.026671] [G loss: 1.000283]\n",
      "2687 [D loss: 1.028877] [G loss: 1.000277]\n",
      "2688 [D loss: 1.027577] [G loss: 1.000285]\n",
      "2689 [D loss: 1.025692] [G loss: 1.000206]\n",
      "2690 [D loss: 1.024952] [G loss: 1.000196]\n",
      "2691 [D loss: 1.027584] [G loss: 1.000156]\n",
      "2692 [D loss: 1.025612] [G loss: 1.000298]\n",
      "2693 [D loss: 1.023692] [G loss: 1.000338]\n",
      "2694 [D loss: 1.026680] [G loss: 1.000232]\n",
      "2695 [D loss: 1.026191] [G loss: 1.000292]\n",
      "2696 [D loss: 1.026065] [G loss: 1.000137]\n",
      "2697 [D loss: 1.028346] [G loss: 1.000211]\n",
      "2698 [D loss: 1.026012] [G loss: 1.000176]\n",
      "2699 [D loss: 1.026346] [G loss: 1.000247]\n",
      "2700 [D loss: 1.029679] [G loss: 1.000250]\n",
      "2701 [D loss: 1.028034] [G loss: 1.000228]\n",
      "2702 [D loss: 1.027272] [G loss: 1.000236]\n",
      "2703 [D loss: 1.026788] [G loss: 1.000273]\n",
      "2704 [D loss: 1.024779] [G loss: 1.000191]\n",
      "2705 [D loss: 1.025360] [G loss: 1.000305]\n",
      "2706 [D loss: 1.027625] [G loss: 1.000163]\n",
      "2707 [D loss: 1.027880] [G loss: 1.000250]\n",
      "2708 [D loss: 1.027222] [G loss: 1.000235]\n",
      "2709 [D loss: 1.028957] [G loss: 1.000321]\n",
      "2710 [D loss: 1.029662] [G loss: 1.000273]\n",
      "2711 [D loss: 1.027206] [G loss: 1.000294]\n",
      "2712 [D loss: 1.024111] [G loss: 1.000316]\n",
      "2713 [D loss: 1.023830] [G loss: 1.000307]\n",
      "2714 [D loss: 1.028427] [G loss: 1.000359]\n",
      "2715 [D loss: 1.026177] [G loss: 1.000349]\n",
      "2716 [D loss: 1.027908] [G loss: 1.000237]\n",
      "2717 [D loss: 1.026589] [G loss: 1.000308]\n",
      "2718 [D loss: 1.026128] [G loss: 1.000229]\n",
      "2719 [D loss: 1.027695] [G loss: 1.000231]\n",
      "2720 [D loss: 1.024923] [G loss: 1.000301]\n",
      "2721 [D loss: 1.026808] [G loss: 1.000275]\n",
      "2722 [D loss: 1.028357] [G loss: 1.000269]\n",
      "2723 [D loss: 1.025062] [G loss: 1.000175]\n",
      "2724 [D loss: 1.028945] [G loss: 1.000231]\n",
      "2725 [D loss: 1.026316] [G loss: 1.000229]\n",
      "2726 [D loss: 1.026100] [G loss: 1.000287]\n",
      "2727 [D loss: 1.025616] [G loss: 1.000159]\n",
      "2728 [D loss: 1.022855] [G loss: 1.000239]\n",
      "2729 [D loss: 1.026381] [G loss: 1.000228]\n",
      "2730 [D loss: 1.025599] [G loss: 1.000270]\n",
      "2731 [D loss: 1.026823] [G loss: 1.000202]\n",
      "2732 [D loss: 1.029686] [G loss: 1.000257]\n",
      "2733 [D loss: 1.026482] [G loss: 1.000186]\n",
      "2734 [D loss: 1.026327] [G loss: 1.000251]\n",
      "2735 [D loss: 1.028112] [G loss: 1.000337]\n",
      "2736 [D loss: 1.026919] [G loss: 1.000153]\n",
      "2737 [D loss: 1.028025] [G loss: 1.000288]\n",
      "2738 [D loss: 1.026110] [G loss: 1.000240]\n",
      "2739 [D loss: 1.024355] [G loss: 1.000317]\n",
      "2740 [D loss: 1.028037] [G loss: 1.000247]\n",
      "2741 [D loss: 1.028418] [G loss: 1.000348]\n",
      "2742 [D loss: 1.024507] [G loss: 1.000171]\n",
      "2743 [D loss: 1.026076] [G loss: 1.000115]\n",
      "2744 [D loss: 1.025455] [G loss: 1.000293]\n",
      "2745 [D loss: 1.025010] [G loss: 1.000364]\n",
      "2746 [D loss: 1.025961] [G loss: 1.000272]\n",
      "2747 [D loss: 1.027083] [G loss: 1.000160]\n",
      "2748 [D loss: 1.026583] [G loss: 1.000269]\n",
      "2749 [D loss: 1.027508] [G loss: 1.000192]\n",
      "2750 [D loss: 1.026144] [G loss: 1.000347]\n",
      "2751 [D loss: 1.024999] [G loss: 1.000287]\n",
      "2752 [D loss: 1.027168] [G loss: 1.000210]\n",
      "2753 [D loss: 1.024946] [G loss: 1.000239]\n",
      "2754 [D loss: 1.026880] [G loss: 1.000225]\n",
      "2755 [D loss: 1.027158] [G loss: 1.000255]\n",
      "2756 [D loss: 1.025170] [G loss: 1.000219]\n",
      "2757 [D loss: 1.025536] [G loss: 1.000277]\n",
      "2758 [D loss: 1.024038] [G loss: 1.000241]\n",
      "2759 [D loss: 1.026789] [G loss: 1.000201]\n",
      "2760 [D loss: 1.026297] [G loss: 1.000307]\n",
      "2761 [D loss: 1.027150] [G loss: 1.000253]\n",
      "2762 [D loss: 1.024965] [G loss: 1.000243]\n",
      "2763 [D loss: 1.027027] [G loss: 1.000259]\n",
      "2764 [D loss: 1.026330] [G loss: 1.000320]\n",
      "2765 [D loss: 1.022289] [G loss: 1.000311]\n",
      "2766 [D loss: 1.028379] [G loss: 1.000286]\n",
      "2767 [D loss: 1.027120] [G loss: 1.000227]\n",
      "2768 [D loss: 1.024144] [G loss: 1.000300]\n",
      "2769 [D loss: 1.026253] [G loss: 1.000199]\n",
      "2770 [D loss: 1.026456] [G loss: 1.000218]\n",
      "2771 [D loss: 1.024996] [G loss: 1.000251]\n",
      "2772 [D loss: 1.026810] [G loss: 1.000287]\n",
      "2773 [D loss: 1.029010] [G loss: 1.000196]\n",
      "2774 [D loss: 1.024240] [G loss: 1.000242]\n",
      "2775 [D loss: 1.027572] [G loss: 1.000194]\n",
      "2776 [D loss: 1.025097] [G loss: 1.000315]\n",
      "2777 [D loss: 1.028686] [G loss: 1.000274]\n",
      "2778 [D loss: 1.026287] [G loss: 1.000226]\n",
      "2779 [D loss: 1.027743] [G loss: 1.000211]\n",
      "2780 [D loss: 1.025685] [G loss: 1.000322]\n",
      "2781 [D loss: 1.027205] [G loss: 1.000208]\n",
      "2782 [D loss: 1.027197] [G loss: 1.000233]\n",
      "2783 [D loss: 1.025409] [G loss: 1.000264]\n",
      "2784 [D loss: 1.025779] [G loss: 1.000191]\n",
      "2785 [D loss: 1.024119] [G loss: 1.000252]\n",
      "2786 [D loss: 1.026623] [G loss: 1.000295]\n",
      "2787 [D loss: 1.027584] [G loss: 1.000222]\n",
      "2788 [D loss: 1.024876] [G loss: 1.000188]\n",
      "2789 [D loss: 1.026889] [G loss: 1.000202]\n",
      "2790 [D loss: 1.023200] [G loss: 1.000231]\n",
      "2791 [D loss: 1.025841] [G loss: 1.000259]\n",
      "2792 [D loss: 1.024355] [G loss: 1.000224]\n",
      "2793 [D loss: 1.025321] [G loss: 1.000250]\n",
      "2794 [D loss: 1.029623] [G loss: 1.000286]\n",
      "2795 [D loss: 1.023942] [G loss: 1.000259]\n",
      "2796 [D loss: 1.027531] [G loss: 1.000284]\n",
      "2797 [D loss: 1.027218] [G loss: 1.000379]\n",
      "2798 [D loss: 1.025455] [G loss: 1.000226]\n",
      "2799 [D loss: 1.025949] [G loss: 1.000131]\n",
      "2800 [D loss: 1.026684] [G loss: 1.000204]\n",
      "2801 [D loss: 1.026617] [G loss: 1.000274]\n",
      "2802 [D loss: 1.026933] [G loss: 1.000235]\n",
      "2803 [D loss: 1.025262] [G loss: 1.000281]\n",
      "2804 [D loss: 1.025949] [G loss: 1.000247]\n",
      "2805 [D loss: 1.026998] [G loss: 1.000207]\n",
      "2806 [D loss: 1.026725] [G loss: 1.000294]\n",
      "2807 [D loss: 1.025413] [G loss: 1.000282]\n",
      "2808 [D loss: 1.025586] [G loss: 1.000292]\n",
      "2809 [D loss: 1.027160] [G loss: 1.000229]\n",
      "2810 [D loss: 1.028374] [G loss: 1.000217]\n",
      "2811 [D loss: 1.023248] [G loss: 1.000293]\n",
      "2812 [D loss: 1.026410] [G loss: 1.000267]\n",
      "2813 [D loss: 1.026625] [G loss: 1.000208]\n",
      "2814 [D loss: 1.026231] [G loss: 1.000202]\n",
      "2815 [D loss: 1.027155] [G loss: 1.000072]\n",
      "2816 [D loss: 1.027991] [G loss: 1.000297]\n",
      "2817 [D loss: 1.025591] [G loss: 1.000258]\n",
      "2818 [D loss: 1.024640] [G loss: 1.000303]\n",
      "2819 [D loss: 1.026639] [G loss: 1.000259]\n",
      "2820 [D loss: 1.027115] [G loss: 1.000272]\n",
      "2821 [D loss: 1.024054] [G loss: 1.000354]\n",
      "2822 [D loss: 1.026340] [G loss: 1.000229]\n",
      "2823 [D loss: 1.025038] [G loss: 1.000260]\n",
      "2824 [D loss: 1.024776] [G loss: 1.000191]\n",
      "2825 [D loss: 1.026661] [G loss: 1.000245]\n",
      "2826 [D loss: 1.023092] [G loss: 1.000281]\n",
      "2827 [D loss: 1.024657] [G loss: 1.000273]\n",
      "2828 [D loss: 1.026402] [G loss: 1.000283]\n",
      "2829 [D loss: 1.025511] [G loss: 1.000268]\n",
      "2830 [D loss: 1.022098] [G loss: 1.000257]\n",
      "2831 [D loss: 1.027902] [G loss: 1.000270]\n",
      "2832 [D loss: 1.028470] [G loss: 1.000270]\n",
      "2833 [D loss: 1.026454] [G loss: 1.000206]\n",
      "2834 [D loss: 1.026340] [G loss: 1.000244]\n",
      "2835 [D loss: 1.026364] [G loss: 1.000181]\n",
      "2836 [D loss: 1.026588] [G loss: 1.000288]\n",
      "2837 [D loss: 1.024642] [G loss: 1.000233]\n",
      "2838 [D loss: 1.025759] [G loss: 1.000331]\n",
      "2839 [D loss: 1.024616] [G loss: 1.000294]\n",
      "2840 [D loss: 1.026224] [G loss: 1.000303]\n",
      "2841 [D loss: 1.027724] [G loss: 1.000252]\n",
      "2842 [D loss: 1.026045] [G loss: 1.000174]\n",
      "2843 [D loss: 1.025014] [G loss: 1.000158]\n",
      "2844 [D loss: 1.024803] [G loss: 1.000242]\n",
      "2845 [D loss: 1.026519] [G loss: 1.000319]\n",
      "2846 [D loss: 1.024626] [G loss: 1.000285]\n",
      "2847 [D loss: 1.022836] [G loss: 1.000313]\n",
      "2848 [D loss: 1.028538] [G loss: 1.000265]\n",
      "2849 [D loss: 1.025189] [G loss: 1.000308]\n",
      "2850 [D loss: 1.026417] [G loss: 1.000273]\n",
      "2851 [D loss: 1.025939] [G loss: 1.000166]\n",
      "2852 [D loss: 1.025637] [G loss: 1.000127]\n",
      "2853 [D loss: 1.024399] [G loss: 1.000232]\n",
      "2854 [D loss: 1.027795] [G loss: 1.000241]\n",
      "2855 [D loss: 1.029282] [G loss: 1.000251]\n",
      "2856 [D loss: 1.026480] [G loss: 1.000255]\n",
      "2857 [D loss: 1.025627] [G loss: 1.000211]\n",
      "2858 [D loss: 1.024235] [G loss: 1.000241]\n",
      "2859 [D loss: 1.027385] [G loss: 1.000204]\n",
      "2860 [D loss: 1.024308] [G loss: 1.000189]\n",
      "2861 [D loss: 1.027928] [G loss: 1.000287]\n",
      "2862 [D loss: 1.025334] [G loss: 1.000233]\n",
      "2863 [D loss: 1.025915] [G loss: 1.000324]\n",
      "2864 [D loss: 1.023600] [G loss: 1.000169]\n",
      "2865 [D loss: 1.027504] [G loss: 1.000320]\n",
      "2866 [D loss: 1.026032] [G loss: 1.000281]\n",
      "2867 [D loss: 1.025961] [G loss: 1.000248]\n",
      "2868 [D loss: 1.025778] [G loss: 1.000251]\n",
      "2869 [D loss: 1.025351] [G loss: 1.000221]\n",
      "2870 [D loss: 1.027771] [G loss: 1.000254]\n",
      "2871 [D loss: 1.027143] [G loss: 1.000340]\n",
      "2872 [D loss: 1.025332] [G loss: 1.000251]\n",
      "2873 [D loss: 1.026026] [G loss: 1.000100]\n",
      "2874 [D loss: 1.025979] [G loss: 1.000315]\n",
      "2875 [D loss: 1.025932] [G loss: 1.000235]\n",
      "2876 [D loss: 1.028136] [G loss: 1.000269]\n",
      "2877 [D loss: 1.024916] [G loss: 1.000233]\n",
      "2878 [D loss: 1.024646] [G loss: 1.000265]\n",
      "2879 [D loss: 1.025403] [G loss: 1.000197]\n",
      "2880 [D loss: 1.025910] [G loss: 1.000222]\n",
      "2881 [D loss: 1.026400] [G loss: 1.000284]\n",
      "2882 [D loss: 1.026331] [G loss: 1.000254]\n",
      "2883 [D loss: 1.026422] [G loss: 1.000296]\n",
      "2884 [D loss: 1.024265] [G loss: 1.000393]\n",
      "2885 [D loss: 1.027711] [G loss: 1.000277]\n",
      "2886 [D loss: 1.025312] [G loss: 1.000308]\n",
      "2887 [D loss: 1.025437] [G loss: 1.000258]\n",
      "2888 [D loss: 1.026980] [G loss: 1.000252]\n",
      "2889 [D loss: 1.023773] [G loss: 1.000218]\n",
      "2890 [D loss: 1.026377] [G loss: 1.000287]\n",
      "2891 [D loss: 1.025877] [G loss: 1.000205]\n",
      "2892 [D loss: 1.024901] [G loss: 1.000329]\n",
      "2893 [D loss: 1.026703] [G loss: 1.000252]\n",
      "2894 [D loss: 1.024952] [G loss: 1.000221]\n",
      "2895 [D loss: 1.024215] [G loss: 1.000274]\n",
      "2896 [D loss: 1.023744] [G loss: 1.000213]\n",
      "2897 [D loss: 1.029445] [G loss: 1.000202]\n",
      "2898 [D loss: 1.025229] [G loss: 1.000226]\n",
      "2899 [D loss: 1.024989] [G loss: 1.000162]\n",
      "2900 [D loss: 1.028942] [G loss: 1.000358]\n",
      "2901 [D loss: 1.025692] [G loss: 1.000320]\n",
      "2902 [D loss: 1.028687] [G loss: 1.000280]\n",
      "2903 [D loss: 1.024196] [G loss: 1.000183]\n",
      "2904 [D loss: 1.025690] [G loss: 1.000241]\n",
      "2905 [D loss: 1.026554] [G loss: 1.000210]\n",
      "2906 [D loss: 1.026257] [G loss: 1.000295]\n",
      "2907 [D loss: 1.027161] [G loss: 1.000192]\n",
      "2908 [D loss: 1.027011] [G loss: 1.000281]\n",
      "2909 [D loss: 1.025361] [G loss: 1.000236]\n",
      "2910 [D loss: 1.026567] [G loss: 1.000218]\n",
      "2911 [D loss: 1.025659] [G loss: 1.000196]\n",
      "2912 [D loss: 1.024950] [G loss: 1.000339]\n",
      "2913 [D loss: 1.026500] [G loss: 1.000267]\n",
      "2914 [D loss: 1.024800] [G loss: 1.000239]\n",
      "2915 [D loss: 1.028897] [G loss: 1.000260]\n",
      "2916 [D loss: 1.025900] [G loss: 1.000362]\n",
      "2917 [D loss: 1.029336] [G loss: 1.000249]\n",
      "2918 [D loss: 1.026325] [G loss: 1.000312]\n",
      "2919 [D loss: 1.026839] [G loss: 1.000221]\n",
      "2920 [D loss: 1.025638] [G loss: 1.000355]\n",
      "2921 [D loss: 1.026986] [G loss: 1.000263]\n",
      "2922 [D loss: 1.027541] [G loss: 1.000256]\n",
      "2923 [D loss: 1.024090] [G loss: 1.000334]\n",
      "2924 [D loss: 1.027785] [G loss: 1.000253]\n",
      "2925 [D loss: 1.026152] [G loss: 1.000206]\n",
      "2926 [D loss: 1.026398] [G loss: 1.000198]\n",
      "2927 [D loss: 1.026929] [G loss: 1.000247]\n",
      "2928 [D loss: 1.028074] [G loss: 1.000233]\n",
      "2929 [D loss: 1.026427] [G loss: 1.000221]\n",
      "2930 [D loss: 1.024025] [G loss: 1.000330]\n",
      "2931 [D loss: 1.027837] [G loss: 1.000290]\n",
      "2932 [D loss: 1.028129] [G loss: 1.000300]\n",
      "2933 [D loss: 1.026597] [G loss: 1.000275]\n",
      "2934 [D loss: 1.027829] [G loss: 1.000235]\n",
      "2935 [D loss: 1.025703] [G loss: 1.000290]\n",
      "2936 [D loss: 1.025242] [G loss: 1.000269]\n",
      "2937 [D loss: 1.027367] [G loss: 1.000271]\n",
      "2938 [D loss: 1.026534] [G loss: 1.000248]\n",
      "2939 [D loss: 1.026582] [G loss: 1.000216]\n",
      "2940 [D loss: 1.026083] [G loss: 1.000343]\n",
      "2941 [D loss: 1.024354] [G loss: 1.000365]\n",
      "2942 [D loss: 1.025958] [G loss: 1.000301]\n",
      "2943 [D loss: 1.022946] [G loss: 1.000366]\n",
      "2944 [D loss: 1.025784] [G loss: 1.000278]\n",
      "2945 [D loss: 1.025918] [G loss: 1.000271]\n",
      "2946 [D loss: 1.022810] [G loss: 1.000343]\n",
      "2947 [D loss: 1.027763] [G loss: 1.000259]\n",
      "2948 [D loss: 1.025781] [G loss: 1.000304]\n",
      "2949 [D loss: 1.021812] [G loss: 1.000221]\n",
      "2950 [D loss: 1.023812] [G loss: 1.000173]\n",
      "2951 [D loss: 1.026819] [G loss: 1.000263]\n",
      "2952 [D loss: 1.026984] [G loss: 1.000329]\n",
      "2953 [D loss: 1.029306] [G loss: 1.000373]\n",
      "2954 [D loss: 1.028876] [G loss: 1.000349]\n",
      "2955 [D loss: 1.023738] [G loss: 1.000320]\n",
      "2956 [D loss: 1.023352] [G loss: 1.000321]\n",
      "2957 [D loss: 1.023516] [G loss: 1.000213]\n",
      "2958 [D loss: 1.024427] [G loss: 1.000318]\n",
      "2959 [D loss: 1.024674] [G loss: 1.000274]\n",
      "2960 [D loss: 1.026102] [G loss: 1.000343]\n",
      "2961 [D loss: 1.025957] [G loss: 1.000257]\n",
      "2962 [D loss: 1.024870] [G loss: 1.000252]\n",
      "2963 [D loss: 1.028288] [G loss: 1.000306]\n",
      "2964 [D loss: 1.025887] [G loss: 1.000257]\n",
      "2965 [D loss: 1.028173] [G loss: 1.000234]\n",
      "2966 [D loss: 1.025862] [G loss: 1.000307]\n",
      "2967 [D loss: 1.025778] [G loss: 1.000323]\n",
      "2968 [D loss: 1.024907] [G loss: 1.000203]\n",
      "2969 [D loss: 1.026214] [G loss: 1.000343]\n",
      "2970 [D loss: 1.026668] [G loss: 1.000256]\n",
      "2971 [D loss: 1.026308] [G loss: 1.000283]\n",
      "2972 [D loss: 1.028607] [G loss: 1.000273]\n",
      "2973 [D loss: 1.023858] [G loss: 1.000310]\n",
      "2974 [D loss: 1.028191] [G loss: 1.000246]\n",
      "2975 [D loss: 1.025620] [G loss: 1.000209]\n",
      "2976 [D loss: 1.024389] [G loss: 1.000360]\n",
      "2977 [D loss: 1.025196] [G loss: 1.000271]\n",
      "2978 [D loss: 1.025729] [G loss: 1.000272]\n",
      "2979 [D loss: 1.026223] [G loss: 1.000291]\n",
      "2980 [D loss: 1.027528] [G loss: 1.000245]\n",
      "2981 [D loss: 1.027652] [G loss: 1.000296]\n",
      "2982 [D loss: 1.027284] [G loss: 1.000341]\n",
      "2983 [D loss: 1.028178] [G loss: 1.000268]\n",
      "2984 [D loss: 1.026810] [G loss: 1.000205]\n",
      "2985 [D loss: 1.028148] [G loss: 1.000270]\n",
      "2986 [D loss: 1.024756] [G loss: 1.000275]\n",
      "2987 [D loss: 1.028484] [G loss: 1.000297]\n",
      "2988 [D loss: 1.027451] [G loss: 1.000351]\n",
      "2989 [D loss: 1.021330] [G loss: 1.000327]\n",
      "2990 [D loss: 1.024403] [G loss: 1.000304]\n",
      "2991 [D loss: 1.024501] [G loss: 1.000204]\n",
      "2992 [D loss: 1.026721] [G loss: 1.000312]\n",
      "2993 [D loss: 1.027732] [G loss: 1.000258]\n",
      "2994 [D loss: 1.026208] [G loss: 1.000346]\n",
      "2995 [D loss: 1.026150] [G loss: 1.000327]\n",
      "2996 [D loss: 1.026447] [G loss: 1.000334]\n",
      "2997 [D loss: 1.026673] [G loss: 1.000291]\n",
      "2998 [D loss: 1.023764] [G loss: 1.000126]\n",
      "2999 [D loss: 1.025857] [G loss: 1.000371]\n",
      "3000 [D loss: 1.028306] [G loss: 1.000398]\n",
      "3001 [D loss: 1.028691] [G loss: 1.000341]\n",
      "3002 [D loss: 1.027499] [G loss: 1.000252]\n",
      "3003 [D loss: 1.027989] [G loss: 1.000333]\n",
      "3004 [D loss: 1.028240] [G loss: 1.000349]\n",
      "3005 [D loss: 1.028371] [G loss: 1.000333]\n",
      "3006 [D loss: 1.025769] [G loss: 1.000304]\n",
      "3007 [D loss: 1.026531] [G loss: 1.000163]\n",
      "3008 [D loss: 1.027167] [G loss: 1.000248]\n",
      "3009 [D loss: 1.025253] [G loss: 1.000207]\n",
      "3010 [D loss: 1.025485] [G loss: 1.000291]\n",
      "3011 [D loss: 1.024621] [G loss: 1.000286]\n",
      "3012 [D loss: 1.025835] [G loss: 1.000200]\n",
      "3013 [D loss: 1.028867] [G loss: 1.000258]\n",
      "3014 [D loss: 1.025837] [G loss: 1.000292]\n",
      "3015 [D loss: 1.026322] [G loss: 1.000348]\n",
      "3016 [D loss: 1.026825] [G loss: 1.000274]\n",
      "3017 [D loss: 1.026408] [G loss: 1.000253]\n",
      "3018 [D loss: 1.027335] [G loss: 1.000242]\n",
      "3019 [D loss: 1.027495] [G loss: 1.000274]\n",
      "3020 [D loss: 1.026027] [G loss: 1.000270]\n",
      "3021 [D loss: 1.027641] [G loss: 1.000339]\n",
      "3022 [D loss: 1.026840] [G loss: 1.000256]\n",
      "3023 [D loss: 1.026420] [G loss: 1.000378]\n",
      "3024 [D loss: 1.025436] [G loss: 1.000378]\n",
      "3025 [D loss: 1.028331] [G loss: 1.000228]\n",
      "3026 [D loss: 1.027715] [G loss: 1.000391]\n",
      "3027 [D loss: 1.021863] [G loss: 1.000245]\n",
      "3028 [D loss: 1.025001] [G loss: 1.000219]\n",
      "3029 [D loss: 1.028367] [G loss: 1.000309]\n",
      "3030 [D loss: 1.027381] [G loss: 1.000305]\n",
      "3031 [D loss: 1.026037] [G loss: 1.000344]\n",
      "3032 [D loss: 1.025878] [G loss: 1.000249]\n",
      "3033 [D loss: 1.025899] [G loss: 1.000448]\n",
      "3034 [D loss: 1.026698] [G loss: 1.000329]\n",
      "3035 [D loss: 1.024481] [G loss: 1.000246]\n",
      "3036 [D loss: 1.023713] [G loss: 1.000252]\n",
      "3037 [D loss: 1.026491] [G loss: 1.000336]\n",
      "3038 [D loss: 1.026411] [G loss: 1.000286]\n",
      "3039 [D loss: 1.026656] [G loss: 1.000315]\n",
      "3040 [D loss: 1.027558] [G loss: 1.000365]\n",
      "3041 [D loss: 1.027601] [G loss: 1.000363]\n",
      "3042 [D loss: 1.026163] [G loss: 1.000451]\n",
      "3043 [D loss: 1.025598] [G loss: 1.000333]\n",
      "3044 [D loss: 1.026295] [G loss: 1.000264]\n",
      "3045 [D loss: 1.027881] [G loss: 1.000327]\n",
      "3046 [D loss: 1.025708] [G loss: 1.000237]\n",
      "3047 [D loss: 1.026771] [G loss: 1.000229]\n",
      "3048 [D loss: 1.025445] [G loss: 1.000302]\n",
      "3049 [D loss: 1.028138] [G loss: 1.000327]\n",
      "3050 [D loss: 1.025831] [G loss: 1.000315]\n",
      "3051 [D loss: 1.026028] [G loss: 1.000328]\n",
      "3052 [D loss: 1.028541] [G loss: 1.000310]\n",
      "3053 [D loss: 1.025249] [G loss: 1.000247]\n",
      "3054 [D loss: 1.024125] [G loss: 1.000143]\n",
      "3055 [D loss: 1.028297] [G loss: 1.000247]\n",
      "3056 [D loss: 1.028969] [G loss: 1.000452]\n",
      "3057 [D loss: 1.025874] [G loss: 1.000226]\n",
      "3058 [D loss: 1.026515] [G loss: 1.000333]\n",
      "3059 [D loss: 1.027269] [G loss: 1.000282]\n",
      "3060 [D loss: 1.027508] [G loss: 1.000198]\n",
      "3061 [D loss: 1.027247] [G loss: 1.000275]\n",
      "3062 [D loss: 1.023976] [G loss: 1.000314]\n",
      "3063 [D loss: 1.024355] [G loss: 1.000300]\n",
      "3064 [D loss: 1.023168] [G loss: 1.000353]\n",
      "3065 [D loss: 1.026429] [G loss: 1.000309]\n",
      "3066 [D loss: 1.023937] [G loss: 1.000190]\n",
      "3067 [D loss: 1.024263] [G loss: 1.000347]\n",
      "3068 [D loss: 1.027429] [G loss: 1.000332]\n",
      "3069 [D loss: 1.024431] [G loss: 1.000246]\n",
      "3070 [D loss: 1.026205] [G loss: 1.000250]\n",
      "3071 [D loss: 1.026889] [G loss: 1.000329]\n",
      "3072 [D loss: 1.026936] [G loss: 1.000334]\n",
      "3073 [D loss: 1.028115] [G loss: 1.000328]\n",
      "3074 [D loss: 1.026886] [G loss: 1.000317]\n",
      "3075 [D loss: 1.026975] [G loss: 1.000319]\n",
      "3076 [D loss: 1.025609] [G loss: 1.000227]\n",
      "3077 [D loss: 1.023793] [G loss: 1.000349]\n",
      "3078 [D loss: 1.025426] [G loss: 1.000378]\n",
      "3079 [D loss: 1.027407] [G loss: 1.000246]\n",
      "3080 [D loss: 1.025371] [G loss: 1.000320]\n",
      "3081 [D loss: 1.028485] [G loss: 1.000217]\n",
      "3082 [D loss: 1.025377] [G loss: 1.000351]\n",
      "3083 [D loss: 1.025866] [G loss: 1.000191]\n",
      "3084 [D loss: 1.026413] [G loss: 1.000241]\n",
      "3085 [D loss: 1.026434] [G loss: 1.000343]\n",
      "3086 [D loss: 1.025873] [G loss: 1.000280]\n",
      "3087 [D loss: 1.026719] [G loss: 1.000286]\n",
      "3088 [D loss: 1.027582] [G loss: 1.000449]\n",
      "3089 [D loss: 1.024469] [G loss: 1.000373]\n",
      "3090 [D loss: 1.028285] [G loss: 1.000272]\n",
      "3091 [D loss: 1.027784] [G loss: 1.000371]\n",
      "3092 [D loss: 1.028847] [G loss: 1.000328]\n",
      "3093 [D loss: 1.025296] [G loss: 1.000208]\n",
      "3094 [D loss: 1.024466] [G loss: 1.000345]\n",
      "3095 [D loss: 1.027414] [G loss: 1.000308]\n",
      "3096 [D loss: 1.024769] [G loss: 1.000237]\n",
      "3097 [D loss: 1.026544] [G loss: 1.000352]\n",
      "3098 [D loss: 1.025369] [G loss: 1.000203]\n",
      "3099 [D loss: 1.027725] [G loss: 1.000348]\n",
      "3100 [D loss: 1.024956] [G loss: 1.000436]\n",
      "3101 [D loss: 1.028248] [G loss: 1.000261]\n",
      "3102 [D loss: 1.025439] [G loss: 1.000331]\n",
      "3103 [D loss: 1.026533] [G loss: 1.000314]\n",
      "3104 [D loss: 1.024922] [G loss: 1.000243]\n",
      "3105 [D loss: 1.024311] [G loss: 1.000264]\n",
      "3106 [D loss: 1.026387] [G loss: 1.000281]\n",
      "3107 [D loss: 1.025443] [G loss: 1.000274]\n",
      "3108 [D loss: 1.025396] [G loss: 1.000308]\n",
      "3109 [D loss: 1.026425] [G loss: 1.000380]\n",
      "3110 [D loss: 1.029259] [G loss: 1.000280]\n",
      "3111 [D loss: 1.027467] [G loss: 1.000334]\n",
      "3112 [D loss: 1.026102] [G loss: 1.000301]\n",
      "3113 [D loss: 1.021893] [G loss: 1.000285]\n",
      "3114 [D loss: 1.025972] [G loss: 1.000311]\n",
      "3115 [D loss: 1.027769] [G loss: 1.000323]\n",
      "3116 [D loss: 1.026824] [G loss: 1.000345]\n",
      "3117 [D loss: 1.027559] [G loss: 1.000201]\n",
      "3118 [D loss: 1.026107] [G loss: 1.000331]\n",
      "3119 [D loss: 1.024219] [G loss: 1.000327]\n",
      "3120 [D loss: 1.026162] [G loss: 1.000302]\n",
      "3121 [D loss: 1.025313] [G loss: 1.000230]\n",
      "3122 [D loss: 1.027025] [G loss: 1.000299]\n",
      "3123 [D loss: 1.024385] [G loss: 1.000287]\n",
      "3124 [D loss: 1.024939] [G loss: 1.000188]\n",
      "3125 [D loss: 1.027358] [G loss: 1.000345]\n",
      "3126 [D loss: 1.023792] [G loss: 1.000245]\n",
      "3127 [D loss: 1.026933] [G loss: 1.000220]\n",
      "3128 [D loss: 1.025796] [G loss: 1.000293]\n",
      "3129 [D loss: 1.027085] [G loss: 1.000290]\n",
      "3130 [D loss: 1.025845] [G loss: 1.000457]\n",
      "3131 [D loss: 1.022375] [G loss: 1.000277]\n",
      "3132 [D loss: 1.025750] [G loss: 1.000349]\n",
      "3133 [D loss: 1.028186] [G loss: 1.000215]\n",
      "3134 [D loss: 1.026006] [G loss: 1.000243]\n",
      "3135 [D loss: 1.024601] [G loss: 1.000291]\n",
      "3136 [D loss: 1.026345] [G loss: 1.000234]\n",
      "3137 [D loss: 1.025589] [G loss: 1.000361]\n",
      "3138 [D loss: 1.026994] [G loss: 1.000346]\n",
      "3139 [D loss: 1.024285] [G loss: 1.000336]\n",
      "3140 [D loss: 1.029258] [G loss: 1.000344]\n",
      "3141 [D loss: 1.025372] [G loss: 1.000368]\n",
      "3142 [D loss: 1.027103] [G loss: 1.000320]\n",
      "3143 [D loss: 1.029587] [G loss: 1.000163]\n",
      "3144 [D loss: 1.027847] [G loss: 1.000300]\n",
      "3145 [D loss: 1.026471] [G loss: 1.000265]\n",
      "3146 [D loss: 1.026195] [G loss: 1.000323]\n",
      "3147 [D loss: 1.025083] [G loss: 1.000238]\n",
      "3148 [D loss: 1.025400] [G loss: 1.000329]\n",
      "3149 [D loss: 1.029099] [G loss: 1.000200]\n",
      "3150 [D loss: 1.025709] [G loss: 1.000184]\n",
      "3151 [D loss: 1.026053] [G loss: 1.000289]\n",
      "3152 [D loss: 1.026339] [G loss: 1.000314]\n",
      "3153 [D loss: 1.027848] [G loss: 1.000275]\n",
      "3154 [D loss: 1.027184] [G loss: 1.000278]\n",
      "3155 [D loss: 1.025478] [G loss: 1.000288]\n",
      "3156 [D loss: 1.027334] [G loss: 1.000328]\n",
      "3157 [D loss: 1.023838] [G loss: 1.000370]\n",
      "3158 [D loss: 1.024478] [G loss: 1.000373]\n",
      "3159 [D loss: 1.027864] [G loss: 1.000401]\n",
      "3160 [D loss: 1.029034] [G loss: 1.000193]\n",
      "3161 [D loss: 1.026215] [G loss: 1.000274]\n",
      "3162 [D loss: 1.025309] [G loss: 1.000317]\n",
      "3163 [D loss: 1.027165] [G loss: 1.000281]\n",
      "3164 [D loss: 1.026289] [G loss: 1.000329]\n",
      "3165 [D loss: 1.025259] [G loss: 1.000248]\n",
      "3166 [D loss: 1.026720] [G loss: 1.000405]\n",
      "3167 [D loss: 1.028461] [G loss: 1.000336]\n",
      "3168 [D loss: 1.026249] [G loss: 1.000282]\n",
      "3169 [D loss: 1.028889] [G loss: 1.000306]\n",
      "3170 [D loss: 1.021732] [G loss: 1.000272]\n",
      "3171 [D loss: 1.025519] [G loss: 1.000208]\n",
      "3172 [D loss: 1.026568] [G loss: 1.000271]\n",
      "3173 [D loss: 1.023026] [G loss: 1.000286]\n",
      "3174 [D loss: 1.024506] [G loss: 1.000356]\n",
      "3175 [D loss: 1.026599] [G loss: 1.000164]\n",
      "3176 [D loss: 1.027022] [G loss: 1.000292]\n",
      "3177 [D loss: 1.027789] [G loss: 1.000220]\n",
      "3178 [D loss: 1.025420] [G loss: 1.000233]\n",
      "3179 [D loss: 1.027188] [G loss: 1.000306]\n",
      "3180 [D loss: 1.026294] [G loss: 1.000254]\n",
      "3181 [D loss: 1.025540] [G loss: 1.000368]\n",
      "3182 [D loss: 1.025610] [G loss: 1.000229]\n",
      "3183 [D loss: 1.025504] [G loss: 1.000285]\n",
      "3184 [D loss: 1.026486] [G loss: 1.000242]\n",
      "3185 [D loss: 1.025363] [G loss: 1.000344]\n",
      "3186 [D loss: 1.025187] [G loss: 1.000231]\n",
      "3187 [D loss: 1.026977] [G loss: 1.000314]\n",
      "3188 [D loss: 1.026509] [G loss: 1.000296]\n",
      "3189 [D loss: 1.025724] [G loss: 1.000311]\n",
      "3190 [D loss: 1.028052] [G loss: 1.000293]\n",
      "3191 [D loss: 1.028419] [G loss: 1.000331]\n",
      "3192 [D loss: 1.025821] [G loss: 1.000373]\n",
      "3193 [D loss: 1.027005] [G loss: 1.000287]\n",
      "3194 [D loss: 1.027515] [G loss: 1.000266]\n",
      "3195 [D loss: 1.027637] [G loss: 1.000227]\n",
      "3196 [D loss: 1.025130] [G loss: 1.000264]\n",
      "3197 [D loss: 1.025951] [G loss: 1.000300]\n",
      "3198 [D loss: 1.025004] [G loss: 1.000286]\n",
      "3199 [D loss: 1.026481] [G loss: 1.000294]\n",
      "3200 [D loss: 1.025413] [G loss: 1.000290]\n",
      "3201 [D loss: 1.029204] [G loss: 1.000302]\n",
      "3202 [D loss: 1.021293] [G loss: 1.000305]\n",
      "3203 [D loss: 1.026520] [G loss: 1.000244]\n",
      "3204 [D loss: 1.026675] [G loss: 1.000354]\n",
      "3205 [D loss: 1.022366] [G loss: 1.000296]\n",
      "3206 [D loss: 1.025567] [G loss: 1.000237]\n",
      "3207 [D loss: 1.026787] [G loss: 1.000299]\n",
      "3208 [D loss: 1.025743] [G loss: 1.000310]\n",
      "3209 [D loss: 1.026406] [G loss: 1.000211]\n",
      "3210 [D loss: 1.024601] [G loss: 1.000277]\n",
      "3211 [D loss: 1.024033] [G loss: 1.000327]\n",
      "3212 [D loss: 1.027102] [G loss: 1.000255]\n",
      "3213 [D loss: 1.028258] [G loss: 1.000273]\n",
      "3214 [D loss: 1.025549] [G loss: 1.000307]\n",
      "3215 [D loss: 1.026030] [G loss: 1.000293]\n",
      "3216 [D loss: 1.024475] [G loss: 1.000340]\n",
      "3217 [D loss: 1.026678] [G loss: 1.000393]\n",
      "3218 [D loss: 1.022657] [G loss: 1.000337]\n",
      "3219 [D loss: 1.025368] [G loss: 1.000288]\n",
      "3220 [D loss: 1.029503] [G loss: 1.000280]\n",
      "3221 [D loss: 1.027531] [G loss: 1.000290]\n",
      "3222 [D loss: 1.025152] [G loss: 1.000365]\n",
      "3223 [D loss: 1.024141] [G loss: 1.000260]\n",
      "3224 [D loss: 1.025938] [G loss: 1.000301]\n",
      "3225 [D loss: 1.028041] [G loss: 1.000314]\n",
      "3226 [D loss: 1.028992] [G loss: 1.000211]\n",
      "3227 [D loss: 1.026569] [G loss: 1.000215]\n",
      "3228 [D loss: 1.027329] [G loss: 1.000263]\n",
      "3229 [D loss: 1.028462] [G loss: 1.000339]\n",
      "3230 [D loss: 1.027835] [G loss: 1.000303]\n",
      "3231 [D loss: 1.027452] [G loss: 1.000271]\n",
      "3232 [D loss: 1.023691] [G loss: 1.000261]\n",
      "3233 [D loss: 1.028910] [G loss: 1.000261]\n",
      "3234 [D loss: 1.029533] [G loss: 1.000380]\n",
      "3235 [D loss: 1.026759] [G loss: 1.000297]\n",
      "3236 [D loss: 1.024776] [G loss: 1.000320]\n",
      "3237 [D loss: 1.027562] [G loss: 1.000280]\n",
      "3238 [D loss: 1.027284] [G loss: 1.000258]\n",
      "3239 [D loss: 1.027483] [G loss: 1.000270]\n",
      "3240 [D loss: 1.026039] [G loss: 1.000374]\n",
      "3241 [D loss: 1.025180] [G loss: 1.000277]\n",
      "3242 [D loss: 1.025188] [G loss: 1.000215]\n",
      "3243 [D loss: 1.025475] [G loss: 1.000394]\n",
      "3244 [D loss: 1.026846] [G loss: 1.000283]\n",
      "3245 [D loss: 1.026070] [G loss: 1.000293]\n",
      "3246 [D loss: 1.025973] [G loss: 1.000368]\n",
      "3247 [D loss: 1.026384] [G loss: 1.000332]\n",
      "3248 [D loss: 1.028348] [G loss: 1.000206]\n",
      "3249 [D loss: 1.029129] [G loss: 1.000313]\n",
      "3250 [D loss: 1.026301] [G loss: 1.000309]\n",
      "3251 [D loss: 1.025352] [G loss: 1.000185]\n",
      "3252 [D loss: 1.027812] [G loss: 1.000293]\n",
      "3253 [D loss: 1.027073] [G loss: 1.000303]\n",
      "3254 [D loss: 1.026939] [G loss: 1.000281]\n",
      "3255 [D loss: 1.024730] [G loss: 1.000248]\n",
      "3256 [D loss: 1.025884] [G loss: 1.000286]\n",
      "3257 [D loss: 1.024509] [G loss: 1.000244]\n",
      "3258 [D loss: 1.025354] [G loss: 1.000364]\n",
      "3259 [D loss: 1.027484] [G loss: 1.000215]\n",
      "3260 [D loss: 1.028467] [G loss: 1.000289]\n",
      "3261 [D loss: 1.025223] [G loss: 1.000249]\n",
      "3262 [D loss: 1.027503] [G loss: 1.000235]\n",
      "3263 [D loss: 1.026800] [G loss: 1.000454]\n",
      "3264 [D loss: 1.026464] [G loss: 1.000230]\n",
      "3265 [D loss: 1.023022] [G loss: 1.000329]\n",
      "3266 [D loss: 1.028664] [G loss: 1.000341]\n",
      "3267 [D loss: 1.026644] [G loss: 1.000319]\n",
      "3268 [D loss: 1.025406] [G loss: 1.000321]\n",
      "3269 [D loss: 1.028985] [G loss: 1.000393]\n",
      "3270 [D loss: 1.026546] [G loss: 1.000403]\n",
      "3271 [D loss: 1.023769] [G loss: 1.000230]\n",
      "3272 [D loss: 1.026207] [G loss: 1.000269]\n",
      "3273 [D loss: 1.026430] [G loss: 1.000274]\n",
      "3274 [D loss: 1.026860] [G loss: 1.000358]\n",
      "3275 [D loss: 1.025829] [G loss: 1.000275]\n",
      "3276 [D loss: 1.027265] [G loss: 1.000273]\n",
      "3277 [D loss: 1.025943] [G loss: 1.000340]\n",
      "3278 [D loss: 1.021834] [G loss: 1.000259]\n",
      "3279 [D loss: 1.028235] [G loss: 1.000279]\n",
      "3280 [D loss: 1.025864] [G loss: 1.000267]\n",
      "3281 [D loss: 1.023513] [G loss: 1.000279]\n",
      "3282 [D loss: 1.027248] [G loss: 1.000360]\n",
      "3283 [D loss: 1.026535] [G loss: 1.000335]\n",
      "3284 [D loss: 1.025708] [G loss: 1.000299]\n",
      "3285 [D loss: 1.027642] [G loss: 1.000259]\n",
      "3286 [D loss: 1.026411] [G loss: 1.000315]\n",
      "3287 [D loss: 1.025478] [G loss: 1.000261]\n",
      "3288 [D loss: 1.027272] [G loss: 1.000421]\n",
      "3289 [D loss: 1.024504] [G loss: 1.000302]\n",
      "3290 [D loss: 1.025952] [G loss: 1.000217]\n",
      "3291 [D loss: 1.028172] [G loss: 1.000303]\n",
      "3292 [D loss: 1.028406] [G loss: 1.000310]\n",
      "3293 [D loss: 1.026586] [G loss: 1.000324]\n",
      "3294 [D loss: 1.027475] [G loss: 1.000273]\n",
      "3295 [D loss: 1.025929] [G loss: 1.000231]\n",
      "3296 [D loss: 1.029848] [G loss: 1.000340]\n",
      "3297 [D loss: 1.028160] [G loss: 1.000248]\n",
      "3298 [D loss: 1.025060] [G loss: 1.000209]\n",
      "3299 [D loss: 1.025787] [G loss: 1.000339]\n",
      "3300 [D loss: 1.028510] [G loss: 1.000351]\n",
      "3301 [D loss: 1.026447] [G loss: 1.000338]\n",
      "3302 [D loss: 1.024899] [G loss: 1.000327]\n",
      "3303 [D loss: 1.026052] [G loss: 1.000269]\n",
      "3304 [D loss: 1.024979] [G loss: 1.000157]\n",
      "3305 [D loss: 1.025477] [G loss: 1.000226]\n",
      "3306 [D loss: 1.029927] [G loss: 1.000348]\n",
      "3307 [D loss: 1.026849] [G loss: 1.000291]\n",
      "3308 [D loss: 1.026704] [G loss: 1.000255]\n",
      "3309 [D loss: 1.025085] [G loss: 1.000257]\n",
      "3310 [D loss: 1.023370] [G loss: 1.000387]\n",
      "3311 [D loss: 1.025743] [G loss: 1.000291]\n",
      "3312 [D loss: 1.027050] [G loss: 1.000412]\n",
      "3313 [D loss: 1.030465] [G loss: 1.000262]\n",
      "3314 [D loss: 1.026037] [G loss: 1.000372]\n",
      "3315 [D loss: 1.023936] [G loss: 1.000278]\n",
      "3316 [D loss: 1.023424] [G loss: 1.000321]\n",
      "3317 [D loss: 1.028365] [G loss: 1.000172]\n",
      "3318 [D loss: 1.027595] [G loss: 1.000307]\n",
      "3319 [D loss: 1.028033] [G loss: 1.000360]\n",
      "3320 [D loss: 1.027888] [G loss: 1.000251]\n",
      "3321 [D loss: 1.024574] [G loss: 1.000418]\n",
      "3322 [D loss: 1.024185] [G loss: 1.000277]\n",
      "3323 [D loss: 1.025577] [G loss: 1.000254]\n",
      "3324 [D loss: 1.027788] [G loss: 1.000287]\n",
      "3325 [D loss: 1.026993] [G loss: 1.000231]\n",
      "3326 [D loss: 1.028393] [G loss: 1.000182]\n",
      "3327 [D loss: 1.025855] [G loss: 1.000309]\n",
      "3328 [D loss: 1.025942] [G loss: 1.000317]\n",
      "3329 [D loss: 1.027994] [G loss: 1.000292]\n",
      "3330 [D loss: 1.022463] [G loss: 1.000345]\n",
      "3331 [D loss: 1.026541] [G loss: 1.000355]\n",
      "3332 [D loss: 1.025036] [G loss: 1.000321]\n",
      "3333 [D loss: 1.024836] [G loss: 1.000277]\n",
      "3334 [D loss: 1.028655] [G loss: 1.000279]\n",
      "3335 [D loss: 1.028871] [G loss: 1.000193]\n",
      "3336 [D loss: 1.023698] [G loss: 1.000199]\n",
      "3337 [D loss: 1.024323] [G loss: 1.000347]\n",
      "3338 [D loss: 1.026844] [G loss: 1.000233]\n",
      "3339 [D loss: 1.025497] [G loss: 1.000333]\n",
      "3340 [D loss: 1.025751] [G loss: 1.000243]\n",
      "3341 [D loss: 1.026233] [G loss: 1.000236]\n",
      "3342 [D loss: 1.024278] [G loss: 1.000205]\n",
      "3343 [D loss: 1.026032] [G loss: 1.000353]\n",
      "3344 [D loss: 1.025223] [G loss: 1.000354]\n",
      "3345 [D loss: 1.027823] [G loss: 1.000273]\n",
      "3346 [D loss: 1.022963] [G loss: 1.000184]\n",
      "3347 [D loss: 1.026806] [G loss: 1.000325]\n",
      "3348 [D loss: 1.024979] [G loss: 1.000210]\n",
      "3349 [D loss: 1.026983] [G loss: 1.000382]\n",
      "3350 [D loss: 1.022904] [G loss: 1.000352]\n",
      "3351 [D loss: 1.026425] [G loss: 1.000358]\n",
      "3352 [D loss: 1.027572] [G loss: 1.000209]\n",
      "3353 [D loss: 1.025087] [G loss: 1.000332]\n",
      "3354 [D loss: 1.025775] [G loss: 1.000301]\n",
      "3355 [D loss: 1.028768] [G loss: 1.000352]\n",
      "3356 [D loss: 1.025638] [G loss: 1.000242]\n",
      "3357 [D loss: 1.027893] [G loss: 1.000441]\n",
      "3358 [D loss: 1.028005] [G loss: 1.000335]\n",
      "3359 [D loss: 1.026808] [G loss: 1.000272]\n",
      "3360 [D loss: 1.026157] [G loss: 1.000298]\n",
      "3361 [D loss: 1.024043] [G loss: 1.000340]\n",
      "3362 [D loss: 1.027025] [G loss: 1.000336]\n",
      "3363 [D loss: 1.026228] [G loss: 1.000285]\n",
      "3364 [D loss: 1.023644] [G loss: 1.000309]\n",
      "3365 [D loss: 1.029546] [G loss: 1.000297]\n",
      "3366 [D loss: 1.023257] [G loss: 1.000257]\n",
      "3367 [D loss: 1.027461] [G loss: 1.000226]\n",
      "3368 [D loss: 1.028751] [G loss: 1.000224]\n",
      "3369 [D loss: 1.025393] [G loss: 1.000238]\n",
      "3370 [D loss: 1.026717] [G loss: 1.000275]\n",
      "3371 [D loss: 1.025799] [G loss: 1.000209]\n",
      "3372 [D loss: 1.024530] [G loss: 1.000361]\n",
      "3373 [D loss: 1.024140] [G loss: 1.000227]\n",
      "3374 [D loss: 1.023002] [G loss: 1.000305]\n",
      "3375 [D loss: 1.027756] [G loss: 1.000361]\n",
      "3376 [D loss: 1.024662] [G loss: 1.000201]\n",
      "3377 [D loss: 1.024893] [G loss: 1.000298]\n",
      "3378 [D loss: 1.025035] [G loss: 1.000276]\n",
      "3379 [D loss: 1.028856] [G loss: 1.000264]\n",
      "3380 [D loss: 1.025539] [G loss: 1.000312]\n",
      "3381 [D loss: 1.026606] [G loss: 1.000345]\n",
      "3382 [D loss: 1.025957] [G loss: 1.000348]\n",
      "3383 [D loss: 1.028898] [G loss: 1.000287]\n",
      "3384 [D loss: 1.025784] [G loss: 1.000295]\n",
      "3385 [D loss: 1.027329] [G loss: 1.000156]\n",
      "3386 [D loss: 1.027589] [G loss: 1.000291]\n",
      "3387 [D loss: 1.027522] [G loss: 1.000272]\n",
      "3388 [D loss: 1.028013] [G loss: 1.000214]\n",
      "3389 [D loss: 1.026359] [G loss: 1.000308]\n",
      "3390 [D loss: 1.024672] [G loss: 1.000215]\n",
      "3391 [D loss: 1.028157] [G loss: 1.000245]\n",
      "3392 [D loss: 1.028505] [G loss: 1.000217]\n",
      "3393 [D loss: 1.029172] [G loss: 1.000235]\n",
      "3394 [D loss: 1.025112] [G loss: 1.000277]\n",
      "3395 [D loss: 1.026201] [G loss: 1.000226]\n",
      "3396 [D loss: 1.028055] [G loss: 1.000304]\n",
      "3397 [D loss: 1.027158] [G loss: 1.000306]\n",
      "3398 [D loss: 1.024294] [G loss: 1.000320]\n",
      "3399 [D loss: 1.025657] [G loss: 1.000339]\n",
      "3400 [D loss: 1.025781] [G loss: 1.000394]\n",
      "3401 [D loss: 1.027555] [G loss: 1.000290]\n",
      "3402 [D loss: 1.028075] [G loss: 1.000280]\n",
      "3403 [D loss: 1.026580] [G loss: 1.000284]\n",
      "3404 [D loss: 1.028414] [G loss: 1.000321]\n",
      "3405 [D loss: 1.024350] [G loss: 1.000403]\n",
      "3406 [D loss: 1.027159] [G loss: 1.000257]\n",
      "3407 [D loss: 1.024051] [G loss: 1.000261]\n",
      "3408 [D loss: 1.030211] [G loss: 1.000301]\n",
      "3409 [D loss: 1.026171] [G loss: 1.000192]\n",
      "3410 [D loss: 1.026577] [G loss: 1.000244]\n",
      "3411 [D loss: 1.023623] [G loss: 1.000367]\n",
      "3412 [D loss: 1.028888] [G loss: 1.000268]\n",
      "3413 [D loss: 1.024597] [G loss: 1.000168]\n",
      "3414 [D loss: 1.028230] [G loss: 1.000239]\n",
      "3415 [D loss: 1.026108] [G loss: 1.000236]\n",
      "3416 [D loss: 1.024152] [G loss: 1.000362]\n",
      "3417 [D loss: 1.023913] [G loss: 1.000224]\n",
      "3418 [D loss: 1.026543] [G loss: 1.000372]\n",
      "3419 [D loss: 1.027128] [G loss: 1.000261]\n",
      "3420 [D loss: 1.023333] [G loss: 1.000219]\n",
      "3421 [D loss: 1.026991] [G loss: 1.000332]\n",
      "3422 [D loss: 1.025699] [G loss: 1.000261]\n",
      "3423 [D loss: 1.027109] [G loss: 1.000287]\n",
      "3424 [D loss: 1.026373] [G loss: 1.000279]\n",
      "3425 [D loss: 1.024948] [G loss: 1.000238]\n",
      "3426 [D loss: 1.022717] [G loss: 1.000278]\n",
      "3427 [D loss: 1.026335] [G loss: 1.000299]\n",
      "3428 [D loss: 1.025297] [G loss: 1.000274]\n",
      "3429 [D loss: 1.025764] [G loss: 1.000377]\n",
      "3430 [D loss: 1.026617] [G loss: 1.000190]\n",
      "3431 [D loss: 1.027831] [G loss: 1.000357]\n",
      "3432 [D loss: 1.026550] [G loss: 1.000300]\n",
      "3433 [D loss: 1.026939] [G loss: 1.000263]\n",
      "3434 [D loss: 1.024955] [G loss: 1.000307]\n",
      "3435 [D loss: 1.027059] [G loss: 1.000170]\n",
      "3436 [D loss: 1.022869] [G loss: 1.000283]\n",
      "3437 [D loss: 1.026381] [G loss: 1.000221]\n",
      "3438 [D loss: 1.024323] [G loss: 1.000312]\n",
      "3439 [D loss: 1.026153] [G loss: 1.000299]\n",
      "3440 [D loss: 1.026321] [G loss: 1.000280]\n",
      "3441 [D loss: 1.023653] [G loss: 1.000405]\n",
      "3442 [D loss: 1.021327] [G loss: 1.000376]\n",
      "3443 [D loss: 1.025696] [G loss: 1.000230]\n",
      "3444 [D loss: 1.025105] [G loss: 1.000317]\n",
      "3445 [D loss: 1.028782] [G loss: 1.000312]\n",
      "3446 [D loss: 1.026716] [G loss: 1.000229]\n",
      "3447 [D loss: 1.027513] [G loss: 1.000270]\n",
      "3448 [D loss: 1.027004] [G loss: 1.000228]\n",
      "3449 [D loss: 1.027167] [G loss: 1.000351]\n",
      "3450 [D loss: 1.026440] [G loss: 1.000216]\n",
      "3451 [D loss: 1.027398] [G loss: 1.000314]\n",
      "3452 [D loss: 1.026269] [G loss: 1.000262]\n",
      "3453 [D loss: 1.025884] [G loss: 1.000261]\n",
      "3454 [D loss: 1.027178] [G loss: 1.000311]\n",
      "3455 [D loss: 1.026163] [G loss: 1.000282]\n",
      "3456 [D loss: 1.027796] [G loss: 1.000269]\n",
      "3457 [D loss: 1.025203] [G loss: 1.000322]\n",
      "3458 [D loss: 1.025384] [G loss: 1.000284]\n",
      "3459 [D loss: 1.023223] [G loss: 1.000318]\n",
      "3460 [D loss: 1.028240] [G loss: 1.000313]\n",
      "3461 [D loss: 1.026726] [G loss: 1.000287]\n",
      "3462 [D loss: 1.025841] [G loss: 1.000278]\n",
      "3463 [D loss: 1.025689] [G loss: 1.000192]\n",
      "3464 [D loss: 1.024262] [G loss: 1.000348]\n",
      "3465 [D loss: 1.028902] [G loss: 1.000322]\n",
      "3466 [D loss: 1.025547] [G loss: 1.000206]\n",
      "3467 [D loss: 1.025066] [G loss: 1.000308]\n",
      "3468 [D loss: 1.026583] [G loss: 1.000194]\n",
      "3469 [D loss: 1.024863] [G loss: 1.000347]\n",
      "3470 [D loss: 1.024814] [G loss: 1.000157]\n",
      "3471 [D loss: 1.025346] [G loss: 1.000117]\n",
      "3472 [D loss: 1.025817] [G loss: 1.000297]\n",
      "3473 [D loss: 1.029001] [G loss: 1.000305]\n",
      "3474 [D loss: 1.027735] [G loss: 1.000384]\n",
      "3475 [D loss: 1.030170] [G loss: 1.000281]\n",
      "3476 [D loss: 1.027169] [G loss: 1.000208]\n",
      "3477 [D loss: 1.028016] [G loss: 1.000216]\n",
      "3478 [D loss: 1.026260] [G loss: 1.000292]\n",
      "3479 [D loss: 1.027114] [G loss: 1.000213]\n",
      "3480 [D loss: 1.025480] [G loss: 1.000262]\n",
      "3481 [D loss: 1.022668] [G loss: 1.000250]\n",
      "3482 [D loss: 1.023546] [G loss: 1.000332]\n",
      "3483 [D loss: 1.027444] [G loss: 1.000371]\n",
      "3484 [D loss: 1.027797] [G loss: 1.000289]\n",
      "3485 [D loss: 1.025818] [G loss: 1.000282]\n",
      "3486 [D loss: 1.026691] [G loss: 1.000204]\n",
      "3487 [D loss: 1.026898] [G loss: 1.000232]\n",
      "3488 [D loss: 1.026259] [G loss: 1.000244]\n",
      "3489 [D loss: 1.026752] [G loss: 1.000314]\n",
      "3490 [D loss: 1.026216] [G loss: 1.000305]\n",
      "3491 [D loss: 1.029320] [G loss: 1.000331]\n",
      "3492 [D loss: 1.024963] [G loss: 1.000308]\n",
      "3493 [D loss: 1.026187] [G loss: 1.000339]\n",
      "3494 [D loss: 1.025274] [G loss: 1.000228]\n",
      "3495 [D loss: 1.027588] [G loss: 1.000244]\n",
      "3496 [D loss: 1.025505] [G loss: 1.000195]\n",
      "3497 [D loss: 1.028230] [G loss: 1.000283]\n",
      "3498 [D loss: 1.025963] [G loss: 1.000238]\n",
      "3499 [D loss: 1.029198] [G loss: 1.000208]\n",
      "3500 [D loss: 1.026172] [G loss: 1.000301]\n",
      "3501 [D loss: 1.027088] [G loss: 1.000281]\n",
      "3502 [D loss: 1.026835] [G loss: 1.000312]\n",
      "3503 [D loss: 1.024735] [G loss: 1.000297]\n",
      "3504 [D loss: 1.028236] [G loss: 1.000314]\n",
      "3505 [D loss: 1.025506] [G loss: 1.000246]\n",
      "3506 [D loss: 1.025561] [G loss: 1.000171]\n",
      "3507 [D loss: 1.027936] [G loss: 1.000229]\n",
      "3508 [D loss: 1.028272] [G loss: 1.000239]\n",
      "3509 [D loss: 1.026074] [G loss: 1.000388]\n",
      "3510 [D loss: 1.024313] [G loss: 1.000277]\n",
      "3511 [D loss: 1.026896] [G loss: 1.000252]\n",
      "3512 [D loss: 1.026219] [G loss: 1.000238]\n",
      "3513 [D loss: 1.025770] [G loss: 1.000321]\n",
      "3514 [D loss: 1.026611] [G loss: 1.000263]\n",
      "3515 [D loss: 1.025651] [G loss: 1.000432]\n",
      "3516 [D loss: 1.028752] [G loss: 1.000308]\n",
      "3517 [D loss: 1.024388] [G loss: 1.000258]\n",
      "3518 [D loss: 1.026451] [G loss: 1.000400]\n",
      "3519 [D loss: 1.027111] [G loss: 1.000294]\n",
      "3520 [D loss: 1.023845] [G loss: 1.000321]\n",
      "3521 [D loss: 1.024166] [G loss: 1.000231]\n",
      "3522 [D loss: 1.026817] [G loss: 1.000330]\n",
      "3523 [D loss: 1.026891] [G loss: 1.000194]\n",
      "3524 [D loss: 1.025700] [G loss: 1.000394]\n",
      "3525 [D loss: 1.026752] [G loss: 1.000275]\n",
      "3526 [D loss: 1.028873] [G loss: 1.000172]\n",
      "3527 [D loss: 1.023198] [G loss: 1.000269]\n",
      "3528 [D loss: 1.025838] [G loss: 1.000437]\n",
      "3529 [D loss: 1.025343] [G loss: 1.000309]\n",
      "3530 [D loss: 1.027759] [G loss: 1.000426]\n",
      "3531 [D loss: 1.026687] [G loss: 1.000233]\n",
      "3532 [D loss: 1.026978] [G loss: 1.000287]\n",
      "3533 [D loss: 1.029270] [G loss: 1.000249]\n",
      "3534 [D loss: 1.024825] [G loss: 1.000344]\n",
      "3535 [D loss: 1.029391] [G loss: 1.000366]\n",
      "3536 [D loss: 1.026368] [G loss: 1.000303]\n",
      "3537 [D loss: 1.028568] [G loss: 1.000344]\n",
      "3538 [D loss: 1.027685] [G loss: 1.000217]\n",
      "3539 [D loss: 1.027876] [G loss: 1.000263]\n",
      "3540 [D loss: 1.026130] [G loss: 1.000351]\n",
      "3541 [D loss: 1.026358] [G loss: 1.000173]\n",
      "3542 [D loss: 1.025597] [G loss: 1.000326]\n",
      "3543 [D loss: 1.028121] [G loss: 1.000307]\n",
      "3544 [D loss: 1.025487] [G loss: 1.000276]\n",
      "3545 [D loss: 1.026206] [G loss: 1.000315]\n",
      "3546 [D loss: 1.027436] [G loss: 1.000313]\n",
      "3547 [D loss: 1.029781] [G loss: 1.000306]\n",
      "3548 [D loss: 1.024576] [G loss: 1.000194]\n",
      "3549 [D loss: 1.027106] [G loss: 1.000263]\n",
      "3550 [D loss: 1.027551] [G loss: 1.000302]\n",
      "3551 [D loss: 1.027196] [G loss: 1.000288]\n",
      "3552 [D loss: 1.024066] [G loss: 1.000327]\n",
      "3553 [D loss: 1.028066] [G loss: 1.000319]\n",
      "3554 [D loss: 1.028964] [G loss: 1.000300]\n",
      "3555 [D loss: 1.024303] [G loss: 1.000298]\n",
      "3556 [D loss: 1.026660] [G loss: 1.000194]\n",
      "3557 [D loss: 1.027079] [G loss: 1.000251]\n",
      "3558 [D loss: 1.025447] [G loss: 1.000303]\n",
      "3559 [D loss: 1.026081] [G loss: 1.000263]\n",
      "3560 [D loss: 1.027003] [G loss: 1.000361]\n",
      "3561 [D loss: 1.025994] [G loss: 1.000281]\n",
      "3562 [D loss: 1.029006] [G loss: 1.000378]\n",
      "3563 [D loss: 1.026596] [G loss: 1.000279]\n",
      "3564 [D loss: 1.026551] [G loss: 1.000228]\n",
      "3565 [D loss: 1.025407] [G loss: 1.000349]\n",
      "3566 [D loss: 1.026179] [G loss: 1.000348]\n",
      "3567 [D loss: 1.025510] [G loss: 1.000260]\n",
      "3568 [D loss: 1.025444] [G loss: 1.000170]\n",
      "3569 [D loss: 1.025083] [G loss: 1.000260]\n",
      "3570 [D loss: 1.024639] [G loss: 1.000337]\n",
      "3571 [D loss: 1.028277] [G loss: 1.000278]\n",
      "3572 [D loss: 1.026516] [G loss: 1.000477]\n",
      "3573 [D loss: 1.022068] [G loss: 1.000219]\n",
      "3574 [D loss: 1.027541] [G loss: 1.000279]\n",
      "3575 [D loss: 1.029973] [G loss: 1.000353]\n",
      "3576 [D loss: 1.025909] [G loss: 1.000368]\n",
      "3577 [D loss: 1.026287] [G loss: 1.000237]\n",
      "3578 [D loss: 1.023802] [G loss: 1.000314]\n",
      "3579 [D loss: 1.030330] [G loss: 1.000290]\n",
      "3580 [D loss: 1.028448] [G loss: 1.000303]\n",
      "3581 [D loss: 1.024021] [G loss: 1.000260]\n",
      "3582 [D loss: 1.024936] [G loss: 1.000220]\n",
      "3583 [D loss: 1.026073] [G loss: 1.000414]\n",
      "3584 [D loss: 1.027270] [G loss: 1.000244]\n",
      "3585 [D loss: 1.024828] [G loss: 1.000276]\n",
      "3586 [D loss: 1.027145] [G loss: 1.000287]\n",
      "3587 [D loss: 1.026952] [G loss: 1.000248]\n",
      "3588 [D loss: 1.028208] [G loss: 1.000335]\n",
      "3589 [D loss: 1.027397] [G loss: 1.000326]\n",
      "3590 [D loss: 1.027002] [G loss: 1.000415]\n",
      "3591 [D loss: 1.025244] [G loss: 1.000339]\n",
      "3592 [D loss: 1.025033] [G loss: 1.000210]\n",
      "3593 [D loss: 1.024280] [G loss: 1.000265]\n",
      "3594 [D loss: 1.024532] [G loss: 1.000342]\n",
      "3595 [D loss: 1.026231] [G loss: 1.000249]\n",
      "3596 [D loss: 1.026244] [G loss: 1.000303]\n",
      "3597 [D loss: 1.027267] [G loss: 1.000290]\n",
      "3598 [D loss: 1.026469] [G loss: 1.000291]\n",
      "3599 [D loss: 1.025862] [G loss: 1.000290]\n",
      "3600 [D loss: 1.028328] [G loss: 1.000253]\n",
      "3601 [D loss: 1.027025] [G loss: 1.000136]\n",
      "3602 [D loss: 1.027586] [G loss: 1.000215]\n",
      "3603 [D loss: 1.027727] [G loss: 1.000286]\n",
      "3604 [D loss: 1.028161] [G loss: 1.000427]\n",
      "3605 [D loss: 1.025581] [G loss: 1.000319]\n",
      "3606 [D loss: 1.027194] [G loss: 1.000321]\n",
      "3607 [D loss: 1.027054] [G loss: 1.000360]\n",
      "3608 [D loss: 1.026656] [G loss: 1.000327]\n",
      "3609 [D loss: 1.027337] [G loss: 1.000291]\n",
      "3610 [D loss: 1.026615] [G loss: 1.000249]\n",
      "3611 [D loss: 1.025449] [G loss: 1.000297]\n",
      "3612 [D loss: 1.025043] [G loss: 1.000240]\n",
      "3613 [D loss: 1.027199] [G loss: 1.000372]\n",
      "3614 [D loss: 1.028552] [G loss: 1.000244]\n",
      "3615 [D loss: 1.027099] [G loss: 1.000345]\n",
      "3616 [D loss: 1.025438] [G loss: 1.000277]\n",
      "3617 [D loss: 1.026449] [G loss: 1.000311]\n",
      "3618 [D loss: 1.027197] [G loss: 1.000331]\n",
      "3619 [D loss: 1.029868] [G loss: 1.000347]\n",
      "3620 [D loss: 1.025817] [G loss: 1.000330]\n",
      "3621 [D loss: 1.027719] [G loss: 1.000342]\n",
      "3622 [D loss: 1.026575] [G loss: 1.000214]\n",
      "3623 [D loss: 1.024955] [G loss: 1.000303]\n",
      "3624 [D loss: 1.026375] [G loss: 1.000229]\n",
      "3625 [D loss: 1.028717] [G loss: 1.000364]\n",
      "3626 [D loss: 1.024614] [G loss: 1.000298]\n",
      "3627 [D loss: 1.027515] [G loss: 1.000320]\n",
      "3628 [D loss: 1.025737] [G loss: 1.000317]\n",
      "3629 [D loss: 1.026572] [G loss: 1.000260]\n",
      "3630 [D loss: 1.025410] [G loss: 1.000275]\n",
      "3631 [D loss: 1.029328] [G loss: 1.000394]\n",
      "3632 [D loss: 1.025860] [G loss: 1.000255]\n",
      "3633 [D loss: 1.027497] [G loss: 1.000423]\n",
      "3634 [D loss: 1.024233] [G loss: 1.000251]\n",
      "3635 [D loss: 1.024914] [G loss: 1.000291]\n",
      "3636 [D loss: 1.027275] [G loss: 1.000293]\n",
      "3637 [D loss: 1.025581] [G loss: 1.000321]\n",
      "3638 [D loss: 1.023491] [G loss: 1.000170]\n",
      "3639 [D loss: 1.026273] [G loss: 1.000310]\n",
      "3640 [D loss: 1.024289] [G loss: 1.000296]\n",
      "3641 [D loss: 1.025328] [G loss: 1.000289]\n",
      "3642 [D loss: 1.026984] [G loss: 1.000286]\n",
      "3643 [D loss: 1.024795] [G loss: 1.000240]\n",
      "3644 [D loss: 1.024551] [G loss: 1.000354]\n",
      "3645 [D loss: 1.026440] [G loss: 1.000353]\n",
      "3646 [D loss: 1.027311] [G loss: 1.000274]\n",
      "3647 [D loss: 1.025352] [G loss: 1.000298]\n",
      "3648 [D loss: 1.025366] [G loss: 1.000369]\n",
      "3649 [D loss: 1.027678] [G loss: 1.000348]\n",
      "3650 [D loss: 1.025233] [G loss: 1.000360]\n",
      "3651 [D loss: 1.027366] [G loss: 1.000279]\n",
      "3652 [D loss: 1.029090] [G loss: 1.000426]\n",
      "3653 [D loss: 1.027112] [G loss: 1.000273]\n",
      "3654 [D loss: 1.025315] [G loss: 1.000351]\n",
      "3655 [D loss: 1.026091] [G loss: 1.000248]\n",
      "3656 [D loss: 1.024707] [G loss: 1.000267]\n",
      "3657 [D loss: 1.025285] [G loss: 1.000311]\n",
      "3658 [D loss: 1.027845] [G loss: 1.000376]\n",
      "3659 [D loss: 1.028378] [G loss: 1.000394]\n",
      "3660 [D loss: 1.025839] [G loss: 1.000312]\n",
      "3661 [D loss: 1.030008] [G loss: 1.000310]\n",
      "3662 [D loss: 1.024654] [G loss: 1.000286]\n",
      "3663 [D loss: 1.028365] [G loss: 1.000347]\n",
      "3664 [D loss: 1.025767] [G loss: 1.000348]\n",
      "3665 [D loss: 1.025702] [G loss: 1.000239]\n",
      "3666 [D loss: 1.027503] [G loss: 1.000412]\n",
      "3667 [D loss: 1.024122] [G loss: 1.000307]\n",
      "3668 [D loss: 1.026944] [G loss: 1.000290]\n",
      "3669 [D loss: 1.027672] [G loss: 1.000264]\n",
      "3670 [D loss: 1.026993] [G loss: 1.000305]\n",
      "3671 [D loss: 1.021585] [G loss: 1.000226]\n",
      "3672 [D loss: 1.025866] [G loss: 1.000409]\n",
      "3673 [D loss: 1.027532] [G loss: 1.000291]\n",
      "3674 [D loss: 1.027251] [G loss: 1.000387]\n",
      "3675 [D loss: 1.029325] [G loss: 1.000288]\n",
      "3676 [D loss: 1.024804] [G loss: 1.000357]\n",
      "3677 [D loss: 1.026534] [G loss: 1.000286]\n",
      "3678 [D loss: 1.026936] [G loss: 1.000245]\n",
      "3679 [D loss: 1.026906] [G loss: 1.000404]\n",
      "3680 [D loss: 1.025248] [G loss: 1.000323]\n",
      "3681 [D loss: 1.023391] [G loss: 1.000301]\n",
      "3682 [D loss: 1.026202] [G loss: 1.000319]\n",
      "3683 [D loss: 1.024169] [G loss: 1.000270]\n",
      "3684 [D loss: 1.026695] [G loss: 1.000299]\n",
      "3685 [D loss: 1.023583] [G loss: 1.000338]\n",
      "3686 [D loss: 1.027473] [G loss: 1.000351]\n",
      "3687 [D loss: 1.023649] [G loss: 1.000348]\n",
      "3688 [D loss: 1.025721] [G loss: 1.000263]\n",
      "3689 [D loss: 1.027485] [G loss: 1.000243]\n",
      "3690 [D loss: 1.027618] [G loss: 1.000383]\n",
      "3691 [D loss: 1.028944] [G loss: 1.000187]\n",
      "3692 [D loss: 1.025228] [G loss: 1.000272]\n",
      "3693 [D loss: 1.028884] [G loss: 1.000268]\n",
      "3694 [D loss: 1.022104] [G loss: 1.000309]\n",
      "3695 [D loss: 1.026410] [G loss: 1.000209]\n",
      "3696 [D loss: 1.024470] [G loss: 1.000282]\n",
      "3697 [D loss: 1.026268] [G loss: 1.000358]\n",
      "3698 [D loss: 1.026692] [G loss: 1.000335]\n",
      "3699 [D loss: 1.027889] [G loss: 1.000370]\n",
      "3700 [D loss: 1.024676] [G loss: 1.000287]\n",
      "3701 [D loss: 1.024011] [G loss: 1.000253]\n",
      "3702 [D loss: 1.024822] [G loss: 1.000336]\n",
      "3703 [D loss: 1.024466] [G loss: 1.000254]\n",
      "3704 [D loss: 1.027040] [G loss: 1.000270]\n",
      "3705 [D loss: 1.025440] [G loss: 1.000255]\n",
      "3706 [D loss: 1.027586] [G loss: 1.000248]\n",
      "3707 [D loss: 1.026694] [G loss: 1.000303]\n",
      "3708 [D loss: 1.027607] [G loss: 1.000243]\n",
      "3709 [D loss: 1.028300] [G loss: 1.000260]\n",
      "3710 [D loss: 1.026511] [G loss: 1.000291]\n",
      "3711 [D loss: 1.024653] [G loss: 1.000264]\n",
      "3712 [D loss: 1.025925] [G loss: 1.000240]\n",
      "3713 [D loss: 1.029749] [G loss: 1.000306]\n",
      "3714 [D loss: 1.028281] [G loss: 1.000293]\n",
      "3715 [D loss: 1.026392] [G loss: 1.000352]\n",
      "3716 [D loss: 1.025583] [G loss: 1.000288]\n",
      "3717 [D loss: 1.027445] [G loss: 1.000245]\n",
      "3718 [D loss: 1.024182] [G loss: 1.000337]\n",
      "3719 [D loss: 1.027808] [G loss: 1.000305]\n",
      "3720 [D loss: 1.025467] [G loss: 1.000304]\n",
      "3721 [D loss: 1.026690] [G loss: 1.000267]\n",
      "3722 [D loss: 1.025580] [G loss: 1.000315]\n",
      "3723 [D loss: 1.026659] [G loss: 1.000273]\n",
      "3724 [D loss: 1.026469] [G loss: 1.000363]\n",
      "3725 [D loss: 1.026829] [G loss: 1.000208]\n",
      "3726 [D loss: 1.030242] [G loss: 1.000271]\n",
      "3727 [D loss: 1.025609] [G loss: 1.000243]\n",
      "3728 [D loss: 1.027339] [G loss: 1.000218]\n",
      "3729 [D loss: 1.026800] [G loss: 1.000376]\n",
      "3730 [D loss: 1.028813] [G loss: 1.000273]\n",
      "3731 [D loss: 1.029189] [G loss: 1.000212]\n",
      "3732 [D loss: 1.026058] [G loss: 1.000228]\n",
      "3733 [D loss: 1.024740] [G loss: 1.000241]\n",
      "3734 [D loss: 1.025164] [G loss: 1.000324]\n",
      "3735 [D loss: 1.026153] [G loss: 1.000314]\n",
      "3736 [D loss: 1.031337] [G loss: 1.000288]\n",
      "3737 [D loss: 1.028451] [G loss: 1.000314]\n",
      "3738 [D loss: 1.025377] [G loss: 1.000258]\n",
      "3739 [D loss: 1.024770] [G loss: 1.000317]\n",
      "3740 [D loss: 1.022837] [G loss: 1.000367]\n",
      "3741 [D loss: 1.025075] [G loss: 1.000355]\n",
      "3742 [D loss: 1.026823] [G loss: 1.000376]\n",
      "3743 [D loss: 1.025452] [G loss: 1.000271]\n",
      "3744 [D loss: 1.028614] [G loss: 1.000325]\n",
      "3745 [D loss: 1.030145] [G loss: 1.000213]\n",
      "3746 [D loss: 1.025667] [G loss: 1.000221]\n",
      "3747 [D loss: 1.024947] [G loss: 1.000320]\n",
      "3748 [D loss: 1.025359] [G loss: 1.000193]\n",
      "3749 [D loss: 1.026425] [G loss: 1.000242]\n",
      "3750 [D loss: 1.026955] [G loss: 1.000321]\n",
      "3751 [D loss: 1.026587] [G loss: 1.000362]\n",
      "3752 [D loss: 1.026033] [G loss: 1.000159]\n",
      "3753 [D loss: 1.028457] [G loss: 1.000336]\n",
      "3754 [D loss: 1.024077] [G loss: 1.000319]\n",
      "3755 [D loss: 1.025443] [G loss: 1.000326]\n",
      "3756 [D loss: 1.024002] [G loss: 1.000325]\n",
      "3757 [D loss: 1.026116] [G loss: 1.000259]\n",
      "3758 [D loss: 1.025240] [G loss: 1.000207]\n",
      "3759 [D loss: 1.026020] [G loss: 1.000310]\n",
      "3760 [D loss: 1.025006] [G loss: 1.000382]\n",
      "3761 [D loss: 1.026422] [G loss: 1.000312]\n",
      "3762 [D loss: 1.025567] [G loss: 1.000320]\n",
      "3763 [D loss: 1.026842] [G loss: 1.000247]\n",
      "3764 [D loss: 1.027864] [G loss: 1.000325]\n",
      "3765 [D loss: 1.024822] [G loss: 1.000294]\n",
      "3766 [D loss: 1.022802] [G loss: 1.000407]\n",
      "3767 [D loss: 1.026251] [G loss: 1.000216]\n",
      "3768 [D loss: 1.026534] [G loss: 1.000272]\n",
      "3769 [D loss: 1.029166] [G loss: 1.000308]\n",
      "3770 [D loss: 1.023984] [G loss: 1.000424]\n",
      "3771 [D loss: 1.026261] [G loss: 1.000357]\n",
      "3772 [D loss: 1.025967] [G loss: 1.000273]\n",
      "3773 [D loss: 1.025014] [G loss: 1.000297]\n",
      "3774 [D loss: 1.027825] [G loss: 1.000351]\n",
      "3775 [D loss: 1.027456] [G loss: 1.000294]\n",
      "3776 [D loss: 1.027115] [G loss: 1.000230]\n",
      "3777 [D loss: 1.025173] [G loss: 1.000196]\n",
      "3778 [D loss: 1.028011] [G loss: 1.000309]\n",
      "3779 [D loss: 1.023985] [G loss: 1.000278]\n",
      "3780 [D loss: 1.024045] [G loss: 1.000244]\n",
      "3781 [D loss: 1.026153] [G loss: 1.000390]\n",
      "3782 [D loss: 1.023808] [G loss: 1.000286]\n",
      "3783 [D loss: 1.026085] [G loss: 1.000271]\n",
      "3784 [D loss: 1.027434] [G loss: 1.000320]\n",
      "3785 [D loss: 1.026801] [G loss: 1.000376]\n",
      "3786 [D loss: 1.025234] [G loss: 1.000240]\n",
      "3787 [D loss: 1.026887] [G loss: 1.000288]\n",
      "3788 [D loss: 1.026969] [G loss: 1.000287]\n",
      "3789 [D loss: 1.025391] [G loss: 1.000346]\n",
      "3790 [D loss: 1.026717] [G loss: 1.000199]\n",
      "3791 [D loss: 1.028819] [G loss: 1.000236]\n",
      "3792 [D loss: 1.026314] [G loss: 1.000341]\n",
      "3793 [D loss: 1.026015] [G loss: 1.000316]\n",
      "3794 [D loss: 1.024241] [G loss: 1.000199]\n",
      "3795 [D loss: 1.027651] [G loss: 1.000257]\n",
      "3796 [D loss: 1.028721] [G loss: 1.000413]\n",
      "3797 [D loss: 1.024193] [G loss: 1.000365]\n",
      "3798 [D loss: 1.028145] [G loss: 1.000322]\n",
      "3799 [D loss: 1.027392] [G loss: 1.000384]\n",
      "3800 [D loss: 1.026415] [G loss: 1.000261]\n",
      "3801 [D loss: 1.028296] [G loss: 1.000307]\n",
      "3802 [D loss: 1.026055] [G loss: 1.000343]\n",
      "3803 [D loss: 1.025623] [G loss: 1.000251]\n",
      "3804 [D loss: 1.025514] [G loss: 1.000318]\n",
      "3805 [D loss: 1.027771] [G loss: 1.000245]\n",
      "3806 [D loss: 1.027576] [G loss: 1.000320]\n",
      "3807 [D loss: 1.027855] [G loss: 1.000311]\n",
      "3808 [D loss: 1.023701] [G loss: 1.000343]\n",
      "3809 [D loss: 1.027497] [G loss: 1.000247]\n",
      "3810 [D loss: 1.026034] [G loss: 1.000354]\n",
      "3811 [D loss: 1.024674] [G loss: 1.000300]\n",
      "3812 [D loss: 1.028968] [G loss: 1.000348]\n",
      "3813 [D loss: 1.028606] [G loss: 1.000262]\n",
      "3814 [D loss: 1.026822] [G loss: 1.000291]\n",
      "3815 [D loss: 1.023443] [G loss: 1.000361]\n",
      "3816 [D loss: 1.027149] [G loss: 1.000399]\n",
      "3817 [D loss: 1.024411] [G loss: 1.000311]\n",
      "3818 [D loss: 1.026942] [G loss: 1.000359]\n",
      "3819 [D loss: 1.026749] [G loss: 1.000343]\n",
      "3820 [D loss: 1.029135] [G loss: 1.000312]\n",
      "3821 [D loss: 1.024057] [G loss: 1.000386]\n",
      "3822 [D loss: 1.026831] [G loss: 1.000346]\n",
      "3823 [D loss: 1.030382] [G loss: 1.000308]\n",
      "3824 [D loss: 1.026674] [G loss: 1.000431]\n",
      "3825 [D loss: 1.025450] [G loss: 1.000310]\n",
      "3826 [D loss: 1.026110] [G loss: 1.000299]\n",
      "3827 [D loss: 1.025365] [G loss: 1.000368]\n",
      "3828 [D loss: 1.026512] [G loss: 1.000320]\n",
      "3829 [D loss: 1.024805] [G loss: 1.000320]\n",
      "3830 [D loss: 1.024829] [G loss: 1.000260]\n",
      "3831 [D loss: 1.027391] [G loss: 1.000325]\n",
      "3832 [D loss: 1.026767] [G loss: 1.000299]\n",
      "3833 [D loss: 1.024743] [G loss: 1.000282]\n",
      "3834 [D loss: 1.024788] [G loss: 1.000261]\n",
      "3835 [D loss: 1.027866] [G loss: 1.000291]\n",
      "3836 [D loss: 1.026924] [G loss: 1.000309]\n",
      "3837 [D loss: 1.027578] [G loss: 1.000311]\n",
      "3838 [D loss: 1.028065] [G loss: 1.000334]\n",
      "3839 [D loss: 1.026941] [G loss: 1.000270]\n",
      "3840 [D loss: 1.027640] [G loss: 1.000270]\n",
      "3841 [D loss: 1.027211] [G loss: 1.000306]\n",
      "3842 [D loss: 1.025747] [G loss: 1.000264]\n",
      "3843 [D loss: 1.026936] [G loss: 1.000306]\n",
      "3844 [D loss: 1.026903] [G loss: 1.000271]\n",
      "3845 [D loss: 1.027196] [G loss: 1.000305]\n",
      "3846 [D loss: 1.026849] [G loss: 1.000309]\n",
      "3847 [D loss: 1.025596] [G loss: 1.000348]\n",
      "3848 [D loss: 1.023876] [G loss: 1.000312]\n",
      "3849 [D loss: 1.025360] [G loss: 1.000331]\n",
      "3850 [D loss: 1.026555] [G loss: 1.000330]\n",
      "3851 [D loss: 1.027157] [G loss: 1.000253]\n",
      "3852 [D loss: 1.026390] [G loss: 1.000254]\n",
      "3853 [D loss: 1.024027] [G loss: 1.000319]\n",
      "3854 [D loss: 1.025402] [G loss: 1.000303]\n",
      "3855 [D loss: 1.024472] [G loss: 1.000308]\n",
      "3856 [D loss: 1.026397] [G loss: 1.000345]\n",
      "3857 [D loss: 1.029464] [G loss: 1.000355]\n",
      "3858 [D loss: 1.024867] [G loss: 1.000268]\n",
      "3859 [D loss: 1.027442] [G loss: 1.000177]\n",
      "3860 [D loss: 1.026941] [G loss: 1.000350]\n",
      "3861 [D loss: 1.027183] [G loss: 1.000209]\n",
      "3862 [D loss: 1.027032] [G loss: 1.000280]\n",
      "3863 [D loss: 1.026723] [G loss: 1.000306]\n",
      "3864 [D loss: 1.027617] [G loss: 1.000258]\n",
      "3865 [D loss: 1.028443] [G loss: 1.000329]\n",
      "3866 [D loss: 1.028635] [G loss: 1.000309]\n",
      "3867 [D loss: 1.023939] [G loss: 1.000361]\n",
      "3868 [D loss: 1.028075] [G loss: 1.000268]\n",
      "3869 [D loss: 1.028540] [G loss: 1.000345]\n",
      "3870 [D loss: 1.025074] [G loss: 1.000346]\n",
      "3871 [D loss: 1.023826] [G loss: 1.000289]\n",
      "3872 [D loss: 1.023272] [G loss: 1.000281]\n",
      "3873 [D loss: 1.022608] [G loss: 1.000172]\n",
      "3874 [D loss: 1.026683] [G loss: 1.000328]\n",
      "3875 [D loss: 1.025229] [G loss: 1.000284]\n",
      "3876 [D loss: 1.027174] [G loss: 1.000237]\n",
      "3877 [D loss: 1.027301] [G loss: 1.000222]\n",
      "3878 [D loss: 1.027112] [G loss: 1.000228]\n",
      "3879 [D loss: 1.025649] [G loss: 1.000339]\n",
      "3880 [D loss: 1.025587] [G loss: 1.000284]\n",
      "3881 [D loss: 1.026511] [G loss: 1.000203]\n",
      "3882 [D loss: 1.025017] [G loss: 1.000357]\n",
      "3883 [D loss: 1.026401] [G loss: 1.000326]\n",
      "3884 [D loss: 1.026424] [G loss: 1.000324]\n",
      "3885 [D loss: 1.026244] [G loss: 1.000279]\n",
      "3886 [D loss: 1.027331] [G loss: 1.000252]\n",
      "3887 [D loss: 1.025913] [G loss: 1.000258]\n",
      "3888 [D loss: 1.028452] [G loss: 1.000257]\n",
      "3889 [D loss: 1.027921] [G loss: 1.000358]\n",
      "3890 [D loss: 1.029630] [G loss: 1.000263]\n",
      "3891 [D loss: 1.023566] [G loss: 1.000361]\n",
      "3892 [D loss: 1.026427] [G loss: 1.000296]\n",
      "3893 [D loss: 1.027781] [G loss: 1.000319]\n",
      "3894 [D loss: 1.030331] [G loss: 1.000234]\n",
      "3895 [D loss: 1.025446] [G loss: 1.000320]\n",
      "3896 [D loss: 1.027187] [G loss: 1.000374]\n",
      "3897 [D loss: 1.029328] [G loss: 1.000269]\n",
      "3898 [D loss: 1.023667] [G loss: 1.000343]\n",
      "3899 [D loss: 1.027361] [G loss: 1.000251]\n",
      "3900 [D loss: 1.025237] [G loss: 1.000308]\n",
      "3901 [D loss: 1.025198] [G loss: 1.000327]\n",
      "3902 [D loss: 1.026111] [G loss: 1.000260]\n",
      "3903 [D loss: 1.025111] [G loss: 1.000311]\n",
      "3904 [D loss: 1.025501] [G loss: 1.000307]\n",
      "3905 [D loss: 1.027096] [G loss: 1.000224]\n",
      "3906 [D loss: 1.026376] [G loss: 1.000357]\n",
      "3907 [D loss: 1.025090] [G loss: 1.000308]\n",
      "3908 [D loss: 1.027286] [G loss: 1.000325]\n",
      "3909 [D loss: 1.025966] [G loss: 1.000318]\n",
      "3910 [D loss: 1.028653] [G loss: 1.000310]\n",
      "3911 [D loss: 1.027518] [G loss: 1.000234]\n",
      "3912 [D loss: 1.028012] [G loss: 1.000326]\n",
      "3913 [D loss: 1.024254] [G loss: 1.000284]\n",
      "3914 [D loss: 1.027255] [G loss: 1.000283]\n",
      "3915 [D loss: 1.027028] [G loss: 1.000299]\n",
      "3916 [D loss: 1.026810] [G loss: 1.000365]\n",
      "3917 [D loss: 1.027400] [G loss: 1.000289]\n",
      "3918 [D loss: 1.027915] [G loss: 1.000317]\n",
      "3919 [D loss: 1.022382] [G loss: 1.000278]\n",
      "3920 [D loss: 1.026288] [G loss: 1.000325]\n",
      "3921 [D loss: 1.026080] [G loss: 1.000226]\n",
      "3922 [D loss: 1.025874] [G loss: 1.000312]\n",
      "3923 [D loss: 1.025350] [G loss: 1.000262]\n",
      "3924 [D loss: 1.023662] [G loss: 1.000230]\n",
      "3925 [D loss: 1.027396] [G loss: 1.000361]\n",
      "3926 [D loss: 1.026410] [G loss: 1.000349]\n",
      "3927 [D loss: 1.026652] [G loss: 1.000233]\n",
      "3928 [D loss: 1.023937] [G loss: 1.000378]\n",
      "3929 [D loss: 1.027737] [G loss: 1.000251]\n",
      "3930 [D loss: 1.026756] [G loss: 1.000361]\n",
      "3931 [D loss: 1.024736] [G loss: 1.000353]\n",
      "3932 [D loss: 1.027000] [G loss: 1.000384]\n",
      "3933 [D loss: 1.026247] [G loss: 1.000246]\n",
      "3934 [D loss: 1.026964] [G loss: 1.000285]\n",
      "3935 [D loss: 1.026390] [G loss: 1.000329]\n",
      "3936 [D loss: 1.027864] [G loss: 1.000163]\n",
      "3937 [D loss: 1.025993] [G loss: 1.000236]\n",
      "3938 [D loss: 1.026343] [G loss: 1.000322]\n",
      "3939 [D loss: 1.026928] [G loss: 1.000252]\n",
      "3940 [D loss: 1.027827] [G loss: 1.000220]\n",
      "3941 [D loss: 1.022592] [G loss: 1.000278]\n",
      "3942 [D loss: 1.027780] [G loss: 1.000285]\n",
      "3943 [D loss: 1.026166] [G loss: 1.000327]\n",
      "3944 [D loss: 1.027027] [G loss: 1.000252]\n",
      "3945 [D loss: 1.028269] [G loss: 1.000299]\n",
      "3946 [D loss: 1.027932] [G loss: 1.000233]\n",
      "3947 [D loss: 1.027786] [G loss: 1.000308]\n",
      "3948 [D loss: 1.025058] [G loss: 1.000254]\n",
      "3949 [D loss: 1.026613] [G loss: 1.000302]\n",
      "3950 [D loss: 1.025410] [G loss: 1.000327]\n",
      "3951 [D loss: 1.027163] [G loss: 1.000271]\n",
      "3952 [D loss: 1.026199] [G loss: 1.000247]\n",
      "3953 [D loss: 1.028091] [G loss: 1.000363]\n",
      "3954 [D loss: 1.027116] [G loss: 1.000230]\n",
      "3955 [D loss: 1.026406] [G loss: 1.000252]\n",
      "3956 [D loss: 1.028070] [G loss: 1.000265]\n",
      "3957 [D loss: 1.026977] [G loss: 1.000324]\n",
      "3958 [D loss: 1.027668] [G loss: 1.000301]\n",
      "3959 [D loss: 1.028464] [G loss: 1.000283]\n",
      "3960 [D loss: 1.026776] [G loss: 1.000320]\n",
      "3961 [D loss: 1.025592] [G loss: 1.000247]\n",
      "3962 [D loss: 1.024800] [G loss: 1.000302]\n",
      "3963 [D loss: 1.024942] [G loss: 1.000263]\n",
      "3964 [D loss: 1.025577] [G loss: 1.000302]\n",
      "3965 [D loss: 1.028348] [G loss: 1.000290]\n",
      "3966 [D loss: 1.025330] [G loss: 1.000263]\n",
      "3967 [D loss: 1.025432] [G loss: 1.000315]\n",
      "3968 [D loss: 1.026487] [G loss: 1.000296]\n",
      "3969 [D loss: 1.025586] [G loss: 1.000298]\n",
      "3970 [D loss: 1.026272] [G loss: 1.000304]\n",
      "3971 [D loss: 1.024075] [G loss: 1.000225]\n",
      "3972 [D loss: 1.028480] [G loss: 1.000252]\n",
      "3973 [D loss: 1.026829] [G loss: 1.000287]\n",
      "3974 [D loss: 1.030927] [G loss: 1.000248]\n",
      "3975 [D loss: 1.026703] [G loss: 1.000312]\n",
      "3976 [D loss: 1.028131] [G loss: 1.000288]\n",
      "3977 [D loss: 1.025775] [G loss: 1.000235]\n",
      "3978 [D loss: 1.024368] [G loss: 1.000256]\n",
      "3979 [D loss: 1.026827] [G loss: 1.000226]\n",
      "3980 [D loss: 1.027573] [G loss: 1.000306]\n",
      "3981 [D loss: 1.026456] [G loss: 1.000195]\n",
      "3982 [D loss: 1.028368] [G loss: 1.000250]\n",
      "3983 [D loss: 1.024746] [G loss: 1.000340]\n",
      "3984 [D loss: 1.027432] [G loss: 1.000303]\n",
      "3985 [D loss: 1.025619] [G loss: 1.000319]\n",
      "3986 [D loss: 1.030390] [G loss: 1.000309]\n",
      "3987 [D loss: 1.027398] [G loss: 1.000286]\n",
      "3988 [D loss: 1.027799] [G loss: 1.000335]\n",
      "3989 [D loss: 1.024608] [G loss: 1.000301]\n",
      "3990 [D loss: 1.026200] [G loss: 1.000283]\n",
      "3991 [D loss: 1.027925] [G loss: 1.000243]\n",
      "3992 [D loss: 1.025145] [G loss: 1.000202]\n",
      "3993 [D loss: 1.025332] [G loss: 1.000242]\n",
      "3994 [D loss: 1.024904] [G loss: 1.000334]\n",
      "3995 [D loss: 1.026193] [G loss: 1.000296]\n",
      "3996 [D loss: 1.026441] [G loss: 1.000325]\n",
      "3997 [D loss: 1.025826] [G loss: 1.000365]\n",
      "3998 [D loss: 1.027510] [G loss: 1.000218]\n",
      "3999 [D loss: 1.026921] [G loss: 1.000389]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, ReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class WGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the critic\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        # The critic takes generated images as input and determines validity\n",
    "        valid = self.critic(img)\n",
    "\n",
    "        # The combined model  (stacked generator and critic)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return model #Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(ReLU(negative_slope=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(ReLU(negative_slope=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(ReLU(negative_slope=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(ReLU(negative_slope=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return model #Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                \n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, size=(batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the critic\n",
    "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "                # Clip critic weights\n",
    "                for l in self.critic.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            \n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wgan = WGAN()\n",
    "    wgan.train(epochs=4000, batch_size=32, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1e208774a37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1, size=(64, wgan.latent_dim))\n",
    "valid = -np.ones((64, 1))\n",
    "wgan.combined.train_on_batch(noise, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
